<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>new-urban-analytics | Nikhil Kaza</title>
    <link>https://nkaza.github.io/tag/new-urban-analytics/</link>
      <atom:link href="https://nkaza.github.io/tag/new-urban-analytics/index.xml" rel="self" type="application/rss+xml" />
    <description>new-urban-analytics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2018-2023 Nikhil Kaza</copyright><lastBuildDate>Fri, 03 Aug 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nkaza.github.io/media/icon_hu1ca6a6912ef6c300619228a995d3f134_46128_512x512_fill_lanczos_center_3.png</url>
      <title>new-urban-analytics</title>
      <link>https://nkaza.github.io/tag/new-urban-analytics/</link>
    </image>
    
    <item>
      <title>Scraping web for data</title>
      <link>https://nkaza.github.io/post/scraping-web-for-data/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/post/scraping-web-for-data/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#legality&#34;&gt;Legality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#an-example-using-baidu&#34;&gt;An example using Baidu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apis-for-non-point-data&#34;&gt;APIs for non-point data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unstructured-data&#34;&gt;Unstructured data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Extracting data from un/semi/structured websites is becoming increasingly common place. Since data is collected, modified and refined continuously, it is increasingly useful to both serve them via web protocols rather than flat files that are downloaded. Furthermore, much of spatial data collection has become private, which also means that firms have stronger incentives to protect their datasets and curate what is available to the others. In other instances, the user or the analyst requires only a small section of the dataset. In these instances and others, data is served by a web-based protocol. Harvesting data usually takes the form of automating the process of sending requests to the webserver and parsing the output to extract relevant data for storage and analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;legality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Legality&lt;/h1&gt;
&lt;p&gt;Different jurisdictions have different legal restrictions and permissions on web scraping. There are also end user agreements that prevent certain actions (storage, retrieval, replication etc.). Please make sure that you are aware of these before attempting to make a local copy of the data that might be privately owned.&lt;/p&gt;
&lt;p&gt;In general, scraping requires automated and repeated requests to the server. As long as these requests are not a disruptive rate, it is unlikely that you will run afoul of legality. Local data storage and replication of services is an entirely different ball game. Please consult a lawyer.&lt;/p&gt;
&lt;p&gt;Many public and government websites are also now serving up data using web protocols. It is, therefore, useful to learn how to parse the outputs of these requests. In some instances, private firms such as Google, Baidu, Instagram etc. also provide application programming interfaces (API) that serve data in a structured format. In these instances, subject to end user agreements, rate limits and daily quotas, restrictions on storage and transfer, it may be possible to access datasets that are otherwise unavailable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-using-baidu&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example using Baidu&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.baidu.com/&#34;&gt;Baidu&lt;/a&gt; is a technology service company that provides a number of services including maps and social networking. According to Wikipedia,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The name Baidu (百度) literally means “a hundred times”, or alternatively, “countless times”. It is a quote from the last line of Xin Qiji (辛弃疾)’s classical poem “Green Jade Table in The Lantern Festival” (青玉案·元夕) saying: “Having searched hundreds of times in the crowd, suddenly turning back, She is there in the dimmest candlelight.” (众里寻他千百度，蓦然回首，那人却在灯火阑珊处。)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this post, we are going to query Baidu for points of interest around &lt;a href=&#34;https://www.travelchinaguide.com/cityguides/wuhan.htm&#34;&gt;Wuhan, China&lt;/a&gt;. This is similar to Google Places.&lt;/p&gt;
&lt;div id=&#34;setting-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up&lt;/h2&gt;
&lt;p&gt;In general, all API require registration and perusal of documentation, so that queries can be structured appropriately. In the case of Baidu, there are additional steps that are required so that the IP address of the computer that you are querying from is not blacklisted for abuse. Please see the &lt;a href=&#34;http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acquiring-api-keys.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acquiring API keys.&lt;/h2&gt;
&lt;p&gt;Every request to API requires a key so the website can control the how much and who can access the information. To acquire a key we need to :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Have a Baidu account. Register at &lt;a href=&#34;https://passport.baidu.com/v2/?reg&#34; class=&#34;uri&#34;&gt;https://passport.baidu.com/v2/?reg&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;img/1.png&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Find your computer IP address. Preferably use &lt;a href=&#34;http://ifconfig.me/&#34;&gt;ifconfig&lt;/a&gt; or &lt;a href=&#34;http://ip138.com/&#34;&gt;ip138&lt;/a&gt;
&lt;img src=&#34;img/2.png&#34; alt=&#34;ifconfig.me&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Login your Baidu account and go to &lt;a href=&#34;http://lbsyun.baidu.com/apiconsole/key?application=key&#34; class=&#34;uri&#34;&gt;http://lbsyun.baidu.com/apiconsole/key?application=key&lt;/a&gt;, click “创建应用” to create a new application. Use the IP address from the previous step to “IP白名单”, then submit the application.
&lt;img src=&#34;img/3.png&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After getting back to the application management page, make a note of the api key. You will need to use it in your R code.
&lt;img src=&#34;img/4.png&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scraping&lt;/h2&gt;
&lt;p&gt;There are There are a few steps to scrape and visualize information fro web queries. In this post, we will use Baidu API as a example to scrape the resturants around Huazhong Agricultural University (HZAU). For the moment, we will deal with structured data. Parsing unstructured data is for a different time.&lt;/p&gt;
&lt;p&gt;The steps are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Intialise your R session&lt;/li&gt;
&lt;li&gt;Set the parameters of the query&lt;/li&gt;
&lt;li&gt;Send the query repreatedly to get all the results&lt;/li&gt;
&lt;li&gt;Cleaning and exporting the data&lt;/li&gt;
&lt;li&gt;Visualize the result&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The two main packages, we are going to use for scraping the web is &lt;a href=&#34;https://CRAN.R-project.org/package=rjson&#34;&gt;RCurl&lt;/a&gt; and &lt;a href=&#34;https://CRAN.R-project.org/package=rjson&#34;&gt;rjson&lt;/a&gt;. Install them, if necessary and intialise them into the library. We will also use &lt;a href=&#34;https://CRAN.R-project.org/package=rjson&#34;&gt;devtools&lt;/a&gt; package to install packages that are not on Comprehensive R Archive Network &lt;a href=&#34;https://cran.r-project.org&#34;&gt;(CRAN)&lt;/a&gt;, but on places like &lt;a href=&#34;http://github.com&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Curl is a command line tool that allows us to transfer data especially over the web. Rcurl is an interface for that tool. Because the result of the API query is formatted in JavaScript Object Notation (JSON), we use RJSON to parse it easily. JSON is lightweight data-interchange format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################### USE YOUR OWN KEY #########################
### aquire the key from: http://lbsyun.baidu.com/apiconsole/key?application=key
key = &amp;quot;_YOUR_KEY_HERE_&amp;quot;
library(rjson)
library(RCurl)
library(tidyverse) # We have seen this package before.
################################################################&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;parameters-of-the-query&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters of the query&lt;/h3&gt;
&lt;p&gt;According to the &lt;a href=&#34;http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi&#34;&gt;API documentation&lt;/a&gt;, we need to set a number of parameters to send the request to API. First, we will use the “round buffer” API to search all the restraunts within 2km distance around Huazhong Agricultural University. You can extract the coordinates from &lt;a href=&#34;http://api.map.baidu.com/lbsapi/getpoint/index.html&#34; class=&#34;uri&#34;&gt;http://api.map.baidu.com/lbsapi/getpoint/index.html&lt;/a&gt; by searching for 华中农业大学. Also Note the lat/long format, instead of the long/lat format we should be using. Pay attention to what the API requires.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;location &amp;lt;- &amp;quot;30.48178,114.363708&amp;quot;   #Latitude and Longitude as a string.
keyword &amp;lt;- &amp;quot;餐馆&amp;quot; %&amp;gt;% curlEscape() ### set the keyword to search for in this case, resturants
city &amp;lt;- &amp;quot;武汉&amp;quot; %&amp;gt;% curlEscape() ### set the city to search

radius &amp;lt;- 4000 ### set the search radius as 2000 meters
page_size &amp;lt;- 20 ### set the number of records in each page of the response

#It will be replaced by the actual value of the result once get the first response
placeIDSet = name = lat = lng = address = NULL ### set the initial value of the other parameters&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;querying-the-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Querying the API&lt;/h3&gt;
&lt;p&gt;Querying the API is simply passing the url string to the server. &lt;code&gt;paste&lt;/code&gt; and &lt;code&gt;paste0&lt;/code&gt; are quite useful for constructing these queries&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(searchURL &amp;lt;- paste(&amp;quot;http://api.map.baidu.com/place/v2/search?query=&amp;quot;,
                      keyword,
                      &amp;quot;&amp;amp;location=&amp;quot;,location,
                      &amp;quot;&amp;amp;radius=&amp;quot;,radius,
                      &amp;quot;&amp;amp;scope=&amp;quot;,1,
                      &amp;quot;&amp;amp;page_num=&amp;quot;, 1,
                      &amp;quot;&amp;amp;page_size=&amp;quot;,page_size,
                      &amp;quot;&amp;amp;region=&amp;quot;,city,
                      &amp;quot;&amp;amp;output=json&amp;quot;,
                      &amp;quot;&amp;amp;ak=&amp;quot;,key,
                      sep=&amp;quot;&amp;quot;))
# [1] &amp;quot;http://api.map.baidu.com/place/v2/search?query=%E9%A4%90%E9%A6%86&amp;amp;location=30.48178,114.363708&amp;amp;radius=4000&amp;amp;scope=1&amp;amp;page_num=1&amp;amp;page_size=20&amp;amp;region=%E6%AD%A6%E6%B1%89&amp;amp;output=json&amp;amp;ak=mSWfSeru2jtlMtAutjy9Vv28XLC568N3&amp;quot;


result &amp;lt;- getURL(url = URLencode(searchURL),ssl.verifypeer = FALSE)
x &amp;lt;- fromJSON(result)
str(x, max.level = 2) # Setting max.level so that it won&amp;#39;t overwhelm the page. Feel free to explore.
# List of 2
#  $ status : num 210
#  $ message: chr &amp;quot;APP IPæ ¡éªŒå¤±è´¥&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Query Baidu for Parks around 1 km of Huazhong Agricultural University and print raw results&lt;/li&gt;
&lt;li&gt;Query Baidu for bus and train stations around Wuhan University and print the results&lt;/li&gt;
&lt;li&gt;How many day care centers are there in this area?&lt;/li&gt;
&lt;li&gt;Intepret these results&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;repeated-queries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Repeated queries&lt;/h3&gt;
&lt;p&gt;Because many of the results are paginated, it is imperative that we query the server repeatedly to get all the results we want. In the following code, we query until all the results are retrieved. We use the while loop for this, though other loops might be more suitable for particular use cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
page_num &amp;lt;- 0 ### set the starting page 
total &amp;lt;- 20 ### the total number of records, initial value is 20, 

while(page_num &amp;lt;= ceiling(total/page_size)-1){
    ### run the query to get the result from the server
    searchURL &amp;lt;- paste(&amp;quot;http://api.map.baidu.com/place/v2/search?query=&amp;quot;,
                      keyword,
                      &amp;quot;&amp;amp;location=&amp;quot;,location,
                      &amp;quot;&amp;amp;radius=&amp;quot;,radius,
                      &amp;quot;&amp;amp;scope=&amp;quot;,1,
                      &amp;quot;&amp;amp;page_num=&amp;quot;,page_num,
                      &amp;quot;&amp;amp;page_size=&amp;quot;,page_size,
                      &amp;quot;&amp;amp;region=&amp;quot;,city,
                      &amp;quot;&amp;amp;output=json&amp;quot;,
                      &amp;quot;&amp;amp;ak=&amp;quot;,key,
                      sep=&amp;quot;&amp;quot;)
    result = getURL(url = searchURL,ssl.verifypeer = FALSE)
    ### transfer the result from json to a list format in R
    x = fromJSON(result)
    ### get the number of total records from the result
    total = x$total
    ### print the process
    cat(&amp;quot;Retrieving&amp;quot;,page_num+1,&amp;quot;from total&amp;quot;,ceiling(total/page_size),&amp;quot;pages ...\n&amp;quot;)
    page_num = page_num + 1
    ### extract the value from the result
    
    # PLACEID
    placeIDSet = c(placeIDSet,
                   unlist(lapply(X = x$results,FUN = function(x)return(x$uid))))
    # Name of the place
    name = c(name,
             unlist(lapply(X = x$results,FUN = function(x)return(x$name))))
    #latitude
    lat = c(lat,
            unlist(lapply(X = x$results,FUN = function(x)return(x$location$lat))))
    #longitude
    lng = c(lng,
            unlist(lapply(X = x$results,FUN = function(x)return(x$location$lng))))
    #address
    address = c(address,
                unlist(lapply(X = x$results,FUN = function(x)return(x$address))))
    Sys.sleep(1) # Set this so that you do not bombard the server. The process stops for 1s. Change this to suit your purpose. 
}
# Retrieving 1 from total  pages ...
# Error in while (page_num &amp;lt;= ceiling(total/page_size) - 1) {: argument is of length zero
# save the extracted information as a dataframe
dat &amp;lt;- data.frame(name,lng,lat,address,placeIDSet)
nrow(dat)
# [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The above is a very brittle code. If the server returns an error because of heavy volume, the loop fails. Use &lt;code&gt;tryCatch&lt;/code&gt; to trap errors and continue loop and keep track of which pages have returned errors. In particular use the status and message of the results to write more graceful code that will survive.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-and-exporting-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cleaning and exporting the data&lt;/h3&gt;
&lt;p&gt;Baidu has its own projection system. To visualize the data, we need to transform the coordinates from BD09 coordinate system to the commonly used WGS84 system (epsg:4326). To achieve that, we need use a customized package on github. &amp;quot;devtools“ package is required here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;waholulu/bd09towgs84&amp;quot;)
library(bdtowgs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use its build-in function “bd09towgs84” to project the coordinates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coord &amp;lt;- subset(dat, select = c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;))
# Error in `[.data.frame`(x, r, vars, drop = drop): undefined columns selected
transfer &amp;lt;- as.data.frame(t(apply(coord,1,bd09towgs84)))
# Error in apply(coord, 1, bd09towgs84): object &amp;#39;coord&amp;#39; not found
dat$lng &amp;lt;- transfer$V1
# Error in eval(expr, envir, enclos): object &amp;#39;transfer&amp;#39; not found
dat$lat &amp;lt;- transfer$V2
# Error in eval(expr, envir, enclos): object &amp;#39;transfer&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the data can be saved as a csv file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# save the information as an excel file
write_csv(dat, path= &amp;quot;search_results.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualise-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualise results&lt;/h3&gt;
&lt;p&gt;Now we can use leaflet to visuzalize the locations of the restaurants.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Error in `[.data.frame`(dat, , c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;name&amp;quot;)): undefined columns selected
# Error in structure(list(options = options), leafletData = data): object &amp;#39;locations_df&amp;#39; not found
# Error in eval(expr, envir, enclos): object &amp;#39;m&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instead of a buffer search, use a bounding box search to return the results for restaurants (“餐馆”). Use the bounds as &lt;code&gt;c(30.531539,114.357628,30.552129,114.385296)&lt;/code&gt; (SW-NE corner points). Display on a map.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What happens you search for day care centers?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;apis-for-non-point-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;APIs for non-point data&lt;/h1&gt;
&lt;p&gt;In the above, we primarily used API to query point location information. There is no reason to think that this is limited to points. For example we can get routing information or travel time isoschornes or whatever API is serving that can be read.&lt;/p&gt;
&lt;p&gt;To demonstrate this, we can plot &lt;a href=&#34;https://www.atlasobscura.com/articles/isochrone-maps-commutes-travel-times&#34;&gt;Isochrones&lt;/a&gt; of every 2 min biking, around some random points in Wuhan. For this we use the Open Source Routing Library (OSRM), though any other API works as well (e.g. Google, Mapbox etc.). For this purposes, we are going to use the demo server for OSRM, though ideally you will &lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend&#34;&gt;set one up&lt;/a&gt; for your purposes. If you set one up for yourself, you can get other directions and travel such as walking, driving etc.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;/p&gt;
&lt;p&gt;You should be careful using the OSRM demo server, it is not always very stable.&lt;/p&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(osrm)
# Ideally set these options up
 options(osrm.server = &amp;quot;http://localhost:5000/&amp;quot;)
options(osrm.profile = &amp;#39;bike&amp;#39;) #Change this for other modes. However, the demo server only returns car profile

randompoints &amp;lt;- matrix(c(114.346566,30.533282,
                           114.298273,30.381364,
                           114.347141,30.599453), ncol=2, byrow =TRUE) %&amp;gt;% data.frame()
names(randompoints) &amp;lt;- c(&amp;#39;lng&amp;#39;, &amp;#39;lat&amp;#39;)
randompoints$name &amp;lt;- c(&amp;#39;pt1&amp;#39;, &amp;#39;pt2&amp;#39;, &amp;#39;pt3&amp;#39;)

rt &amp;lt;- osrmRoute(src = randompoints[1,c(&amp;#39;name&amp;#39;, &amp;#39;lng&amp;#39;,&amp;#39;lat&amp;#39;)], 
                dst = randompoints[2,c(&amp;#39;name&amp;#39;,&amp;#39;lng&amp;#39;,&amp;#39;lat&amp;#39;)], 
                sp = TRUE) %&amp;gt;% st_as_sf()
# Error in UseMethod(&amp;quot;st_as_sf&amp;quot;): no applicable method for &amp;#39;st_as_sf&amp;#39; applied to an object of class &amp;quot;NULL&amp;quot;

rt %&amp;gt;% leaflet() %&amp;gt;%
    addProviderTiles(providers$Stamen.TonerLines, group = &amp;quot;Basemap&amp;quot;) %&amp;gt;%
  addProviderTiles(providers$Stamen.TonerLite, group = &amp;quot;Basemap&amp;quot;) %&amp;gt;%
  addMarkers(data=randompoints[1:2,], ~lng, ~lat) %&amp;gt;%
  addPolylines(weight =5, smoothFactor = .5, color=&amp;#39;red&amp;#39;)
# Error in polygonData.default(data): Don&amp;#39;t know how to get path data from object of class function&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OSRM is a convenience package that is wrapping the calls to the server and parsing the output into Spatial*. For example, the curl query in the backend looks like&lt;/p&gt;
&lt;p&gt;&lt;code&gt;http://router.project-osrm.org/route/v1/driving/114.346566,30.533282,114.298273,30.381364&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iso &amp;lt;- list()
for (i in 1:nrow(randompoints)){
iso[[i]] &amp;lt;- osrmIsochrone(loc = randompoints[i,c(&amp;#39;lng&amp;#39;,&amp;#39;lat&amp;#39;)], breaks = seq(from = 0,to = 20, by = 2)) %&amp;gt;% st_as_sf()
}
# Error in (function (classes, fdef, mtable) : unable to find an inherited method for function &amp;#39;coordinates&amp;#39; for signature &amp;#39;&amp;quot;NULL&amp;quot;&amp;#39;

iso &amp;lt;- do.call(&amp;#39;rbind&amp;#39;, iso)

 Npal &amp;lt;- colorNumeric(
   palette = &amp;quot;Reds&amp;quot;, n = 5,
   domain = iso$center
 )
 
iso %&amp;gt;% leaflet() %&amp;gt;%
  addProviderTiles(providers$Stamen.TonerLines, group = &amp;quot;Basemap&amp;quot;) %&amp;gt;%
  addProviderTiles(providers$Stamen.TonerLite, group = &amp;quot;Basemap&amp;quot;) %&amp;gt;%
  addMarkers(data=randompoints, ~lng, ~lat) %&amp;gt;%
  addPolygons(color = &amp;quot;#444444&amp;quot;, weight = 1, smoothFactor = 0.5,
    opacity = 1.0, fillOpacity = 0.5, fillColor = ~Npal(iso$center),
    group = &amp;quot;Isochrone&amp;quot;) %&amp;gt;%
  addLegend(&amp;quot;topleft&amp;quot;, pal = Npal, values = ~iso$center,
            title = &amp;quot;Biking Time (min)&amp;quot;,opacity = 1
            )
# Error in derivePolygons(data, lng, lat, missing(lng), missing(lat), &amp;quot;addPolygons&amp;quot;): Polygon data not found; please provide addPolygons with data and/or lng/lat arguments&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unstructured-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unstructured data&lt;/h1&gt;
&lt;p&gt;JSON files are well-structured. Therefore, it is relatively easy to parse them. If the files are unstructured, a lot of effort goes into figuring out different structures and searches that will yield the dataset that is necessary. In general, packages such as &lt;code&gt;xml2&lt;/code&gt; and &lt;code&gt;rvest&lt;/code&gt; will help such tasks. This is left for a different day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Reading data from server based APIs are no different from reading and parsing a local file. However, unlike local files that are well structured, and OS handling handling of low level functions of memory management and error recovery, we ought to be extra mindful of how errors might affect and break our code. Once the data is scraped, analysis proceeds in the usual fashion. However, because the data is not of specific vintage, reproducibility of research is a serious concern. You should note it and be able to provide archival of scraped data for others to use, subject to end use restrictions. In any case, currency of the data should be balanced with the archival mission of the organisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;Parts of the code in this post is written by &lt;a href=&#34;https://planning.unc.edu/student/chenyan/&#34;&gt;Yan Chen&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Techniques &amp; Politics of New Urban Analytics</title>
      <link>https://nkaza.github.io/teaching/techniques-politics-short-course/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/teaching/techniques-politics-short-course/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#course-description-objectives&#34;&gt;Course Description &amp;amp; Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbooks&#34;&gt;Textbooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#course-policies&#34;&gt;Course Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#schedule&#34;&gt;Schedule&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;course-description-objectives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Course Description &amp;amp; Objectives&lt;/h1&gt;
&lt;p&gt;This course is about different techniques used in assembling, managing, visualising, analysing and predicting using heterogeneous and messy data sets in urban environments. These include point, polygon, raster, vector, text, image and network data; data sets with high cadence and high spatial resolution; data sets that are inherently messy and incomplete. In addition to the mechanics of urban data analytics, we will also explore the issues of ethics and politics of data generation and analysis.&lt;/p&gt;
&lt;div id=&#34;prerequisites&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Much of the analytical techniques will be taught using R. A working knowledge of the R environment is useful, though the first couple of labs, we will go over the basics. However, the course moves quickly. You are advised to seek help to keep up&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;textbooks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Textbooks&lt;/h1&gt;
&lt;p&gt;We will discuss the topics from the following two books in class. Students ae expected to read through the material before Day 1.&lt;/p&gt;
&lt;p&gt;O’Neil, Cathy (2016). &lt;em&gt;Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy&lt;/em&gt;. New York: Crown.&lt;/p&gt;
&lt;p&gt;Townsend, Anthony M (2013). &lt;em&gt;Smart cities: Big data, civic hackers, and the quest for a new utopia&lt;/em&gt;. WW Norton &amp;amp; Company.&lt;/p&gt;
&lt;p&gt;The following books are recommended for reference.&lt;/p&gt;
&lt;p&gt;Bivand, Roger S, Edzer Pebesma and Virgilio Gómez-Rubio (2013). &lt;em&gt;Applied Spatial Data Analysis with R&lt;/em&gt;. 2nd ed. 2013 edition. New York Heidelberg Dordrecht London: Springer. ISBN: 978-1-4614-7617-7.&lt;/p&gt;
&lt;p&gt;Brewer, Cynthia A. (2015). &lt;em&gt;Designing Better Maps: A Guide for GIS Users&lt;/em&gt;. 2 edition. Redlands, California: Esri Press. ISBN: 978-1-58948-440-5.&lt;/p&gt;
&lt;p&gt;Few, Stephen (2004). &lt;em&gt;Show Me the Numbers: Designing Tables and Graphs to Enlighten&lt;/em&gt;. Oakland, Calif: Analytics Press. ISBN: 978-0-9706019-9-5.&lt;/p&gt;
&lt;p&gt;Grolemund, Garrett and Hadley Wickham (2017). &lt;em&gt;R for Data Science: Import, Tidy, Transform, Visualize, and Model Data&lt;/em&gt;. Sebastopol, CA: O’ Reilly media,. URL: &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt; (visited on May. 25, 2018).&lt;/p&gt;
&lt;p&gt;Tufte, E. R (2001). &lt;em&gt;The Visual display of Quantitative Information&lt;/em&gt;. Cheshire, CT: Graphics Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;course-policies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Course Policies&lt;/h1&gt;
&lt;div id=&#34;equipment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Equipment&lt;/h2&gt;
&lt;p&gt;Every student should have a working laptop that has &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt; installed. The laptops should have sufficient memory and processing capacity to deal with large data sets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;30%&lt;/strong&gt; lab reports to be submitted at the end of the lab (Individual)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;30%&lt;/strong&gt; Daily homework assignments due by 11:59 PM (Individual)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;20%&lt;/strong&gt; Final project (Group)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;10%&lt;/strong&gt; Class &amp;amp; lab participation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Grading of labs and homeworks will be through Canvas. Instructions will be provided on the first day of class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;academic-conduct&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Academic Conduct&lt;/h2&gt;
&lt;p&gt;I firmly believe in learning from your peers and from others. All homework and lab submissions could benefit from collaborations, however, the submissions are individual. This means that interpreting the data and the results, producing the visualisations, drawing appropriate conclusions from the data is necessarily individual even when the strategies can be discussed and developed with others in class or out of class. &lt;strong&gt;All&lt;/strong&gt; help, however, should be explicitly acknowledged. Severe penalties are imposed for non-attribution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;schedule&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Schedule&lt;/h1&gt;
&lt;hr /&gt;
&lt;div id=&#34;day-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 1&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-lec-lab-introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM (Lec &amp;amp; Lab): Introduction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lecture. &lt;a href=&#34;https://nkaza.github.io/slides/techniques_Introduction/Introduction_slides.html&#34;&gt;slides&lt;/a&gt; | &lt;a href=&#34;https://stats.idre.ucla.edu/r/seminars/intro/&#34;&gt;UCLA slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lab Session: Introduction to R. &lt;a href=&#34;https://nkaza.github.io/post/introduction-to-r-exploratory-data-visualisation&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm---440-pm-lab-visualising-urban-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30 PM - 4:40 PM (Lab): Visualising urban data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lab Session: Exploring large urban datasets. Vector data. Visualsing using small multiples, choropleth maps etc. &lt;a href=&#34;https://nkaza.github.io/post/geospatial-data-in-r&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../techniques_hw/hw1.pdf&#34;&gt;Homework&lt;/a&gt;: Due Day 1 11:59 PM in Canvas&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;day-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 2&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-lec-lab-raster-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM (Lec &amp;amp; Lab): Raster Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lecture. &lt;a href=&#34;https://nkaza.github.io/slides/Raster/rasterR_slides.html&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lab Session: Basic raster analysis in R, Urban landscape metrics &lt;a href=&#34;https://www.rspatial.org/analysis/rst/9-remotesensing.html#part-i&#34;&gt;Notes 1&lt;/a&gt; | &lt;a href=&#34;https://nkaza.github.io/post/urban-morphology-landscape-metrics/&#34;&gt;Notes 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../techniques_hw/hw2.pdf&#34;&gt;Homework&lt;/a&gt;: Due Day 2 11:59 PM in Canvas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm---440-pm-seminar-the-ethics-of-smart-cities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30 PM - 4:40 PM (Seminar) : The ethics of smart cities&lt;/h3&gt;
&lt;p&gt;Students are expected to read through the material and be prepared to discuss the topics in class. This is a student driven discussion. Instructor will only facilitate.&lt;/p&gt;
&lt;div id=&#34;assigned-readings&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Assigned Readings&lt;/h4&gt;
&lt;p&gt;Goodspeed, Robert (2014). “Smart cities: moving beyond urban cybernetics to tackle wicked problems”. In: &lt;em&gt;Cambridge Journal of Regions, Economy and Society&lt;/em&gt; 8.1, pp. 79-92.&lt;/p&gt;
&lt;p&gt;Hill, Dan (2008). &lt;em&gt;The street as platform&lt;/em&gt;. URL: &lt;a href=&#34;http://www.cityofsound.com/blog/2008/02/the-street-as-p.html&#34; class=&#34;uri&#34;&gt;http://www.cityofsound.com/blog/2008/02/the-street-as-p.html&lt;/a&gt; (visited on Jun. 03, 2018).&lt;/p&gt;
&lt;p&gt;Vanolo, Alberto (2014). “Smartmentality: The smart city as disciplinary strategy”. In: &lt;em&gt;Urban Studies&lt;/em&gt; 51.5, pp. 883-898.&lt;/p&gt;
&lt;p&gt;Wang, Tricia (2016). &lt;em&gt;Why Big Data Needs Thick Data&lt;/em&gt;. URL: &lt;a href=&#34;https://medium.com/ethnography-matters/why-big-data-needs-thick-data-b4b3e75e3d7&#34; class=&#34;uri&#34;&gt;https://medium.com/ethnography-matters/why-big-data-needs-thick-data-b4b3e75e3d7&lt;/a&gt; (visited on Jun. 03, 2018).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;day-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 3&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-lec-lab-classification-machine-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM (Lec &amp;amp; Lab): Classification &amp;amp; Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lecture. &lt;a href=&#34;https://docs.google.com/presentation/d/120Wer0YimQ3SKqOQ-M-MYf0A0-PqQTohYdUESifj1NQ/edit#slide=id.p&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lab Session: Remote sensing classification, machine learning. &lt;a href=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../techniques_hw/hw3.pdf&#34;&gt;Homework&lt;/a&gt;: Due Day 3 11:59 PM in Canvas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm---440-pm-lec-predictive-blackboxes-algorithmic-biases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30 PM - 4:40 PM (Lec): Predictive Blackboxes &amp;amp; Algorithmic Biases&lt;/h3&gt;
&lt;div id=&#34;assigned-readings-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Assigned Readings&lt;/h4&gt;
&lt;p&gt;Rosenblat, Alex (2016). “The truth about how Uber’s app manages drivers”. In: &lt;em&gt;Harvard Business Review&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Tufekci, Zeynep (2015). “Algorithmic Harms beyond Facebook and Google: Emergent Challenges of Computational Agency”. In: &lt;em&gt;Colorado Technology Law Journal&lt;/em&gt; 13, p. 203. URL: &lt;a href=&#34;https://heinonline.org/HOL/Page?handle=hein.journals/jtelhtel13&amp;amp;id=227&amp;amp;div=&amp;amp;collection=&#34; class=&#34;uri&#34;&gt;https://heinonline.org/HOL/Page?handle=hein.journals/jtelhtel13&amp;amp;id=227&amp;amp;div=&amp;amp;collection=&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ziewitz, Malte (2015). “Governing Algorithms”. In: &lt;em&gt;Science, Technology, &amp;amp; Human Values&lt;/em&gt; 41.1, pp. 3-16. ISSN: 1552-8251. DOI: &lt;a href=&#34;https://doi.org/10.1177/0162243915608948&#34;&gt;10.1177/0162243915608948&lt;/a&gt;. URL: &lt;a href=&#34;http://dx.doi.org/10.1177/0162243915608948&#34; class=&#34;uri&#34;&gt;http://dx.doi.org/10.1177/0162243915608948&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;day-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 4&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-lab-scraping-the-web-for-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM (Lab): Scraping the web for data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lab Session: Points of Interest on Baidu. &lt;a href=&#34;https://nkaza.github.io/post/scraping-web-for-data/&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../techniques_hw/hw4.pdf&#34;&gt;Homework&lt;/a&gt;: Due Day 4 11:59 PM in Canvas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm---440-pm-research-talk-misadventures-in-urban-analytics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30 PM - 4:40 PM (Research Talk): (Mis)adventures in urban analytics&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;day-5&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 5&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-lec-lab-visualising-analysing-point-patterns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM (Lec &amp;amp; Lab) : Visualising &amp;amp; Analysing Point Patterns&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lecture:&lt;/li&gt;
&lt;li&gt;Lab Session: Analysing crime clusters in Manchester &lt;a href=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm---440-pm-group-project-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30 PM - 4:40 PM: Group Project Work&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;day-6&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Day 6&lt;/h2&gt;
&lt;div id=&#34;am---1140-am-short-project-presentations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8:30 AM - 11:40 AM: Short Project Presentations&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Identifying Clusters of Events</title>
      <link>https://nkaza.github.io/post/cluster-detection-in-point-data/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/post/cluster-detection-in-point-data/</guid>
      <description>
&lt;script src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I am going to demonstrate some methods that can be used to identify clusters/hotspots. Clusters of points are usually locations where there are higher than expected frequency of incidents happening. These could be clusters of disease incidences, accidents, flood insurance claims, fatalities etc. Identifying where these clusters exists and are emerging is important to take either mitigating or preventative actions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acquire-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acquire Data&lt;/h2&gt;
&lt;p&gt;We are going to use crime data from &lt;a href=&#34;http://data.police.uk&#34;&gt;data.police.uk&lt;/a&gt; for the greater Manchester Area. The data on this site is published by the Home Office, and is provided by the 43 geographic police forces in England and Wales, the British Transport Police, the Police Service of Northern Ireland and the Ministry of Justice. Most of spatial data boundaries can be acquired either from &lt;a href=&#34;https://www.ordnancesurvey.co.uk/opendatadownload/&#34;&gt;Ordinance Survey&lt;/a&gt; or the &lt;a href=&#34;http://webarchive.nationalarchives.gov.uk/20160110200248/http://www.ons.gov.uk/ons/guide-method/geography/products/census/spatial/2011/index.html&#34;&gt;Census&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;After I started creating this tutorial, I found a &lt;a href=&#34;http://r-video-tutorial.blogspot.com/2015/05/introductory-point-pattern-analysis-of.html&#34;&gt;tutorial by Dr. Fabio Veronesi&lt;/a&gt; that is remarkably similar in the illustrative dataset, though the topics covered are slightly different. Check it out!&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;Since I wrote this tutorial, many have transitioned to using &lt;code&gt;sf&lt;/code&gt; objects instead of &lt;code&gt;sp&lt;/code&gt; objects. Please adapt as necessary.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Crime data for Manchester clipped from January 2016 - May 2018 is
&lt;a href=&#34;https://www.dropbox.com/s/vxzpje0pwz6mhwl/monthlydata.zip?dl=0&#34;&gt;here&lt;/a&gt;. You can also download the LSOA and the boundary file from &lt;a href=&#34;https://www.dropbox.com/s/8cabizab939w6jk/BNDfiles.zip?dl=0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional resources&lt;/h2&gt;
&lt;p&gt;I strongly recommend that you read through &lt;a href=&#34;https://www.crcpress.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/p/book/9781482210200&#34;&gt;Spatial Point Patterns: Methodology and Applications with R&lt;/a&gt; by by Adrian Baddeley, Ege Rubak and Rolf Turner.&lt;/p&gt;
&lt;p&gt;It is also quite useful to peruse the documentation of &lt;a href=&#34;https://www.nij.gov/topics/technology/maps/pages/crimestat.aspx&#34;&gt;CrimeStat&lt;/a&gt; and &lt;a href=&#34;https://spatial.uchicago.edu/software&#34;&gt;GeoDa&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;requirements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;p&gt;This requires &lt;a href=&#34;http://r-project.org&#34;&gt;R&lt;/a&gt;, and many libraries including &lt;a href=&#34;https://cran.r-project.org/package=spatstat&#34;&gt;spatstat&lt;/a&gt;,&lt;a href=&#34;https://cran.r-project.org/web/packages/spdep/&#34;&gt;spdep&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/rgdal&#34;&gt;rgdal&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=aspace&#34;&gt;aspace&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/package=leaflet&#34;&gt;leaflet&lt;/a&gt;. You should install them, as necessary, with &lt;code&gt;install.packages()&lt;/code&gt; command.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;The crime data, especially the location data, is &lt;a href=&#34;https://data.police.uk/about/#anonymisation&#34;&gt;anonymised&lt;/a&gt; . This poses some problems, as the anonymisation is primarily assigning the point to the center of the street. There may be many crimes on the street that get the same location. To get around this I randomly re-jitter the points by a small &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. The following function is a quick way to introduce noise. This introduction of noise, might be problematic for some applications. You will have to figure out how to deal with the issue of duplicate locations one way or the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jitter_longlat &amp;lt;- function(coords, km = 1) {
  n &amp;lt;- dim(coords)[1]
  length_at_lat &amp;lt;- rep(110.5742727, n) # in kilometers at the equator. Assuming a sphere
  length_at_long &amp;lt;- cos(coords[,1]* (2 * pi) / 360) * 110.5742727
  randnumber &amp;lt;- coords
  randnumber[,2] &amp;lt;- runif(n, min=-1,max=1) * (1/length_at_lat) * km
  randnumber[,1] &amp;lt;- runif(n, min=-1,max=1) * (1/length_at_long) * km
  out &amp;lt;- coords
  out[,1] &amp;lt;- coords[,1] + randnumber[,1]
  out[,2] &amp;lt;- coords[,2] + randnumber[,2]
  out &amp;lt;- as.data.frame(out)
  return(out)
}

ls()
# [1] &amp;quot;filename&amp;quot;       &amp;quot;i&amp;quot;              &amp;quot;jitter_longlat&amp;quot; &amp;quot;manchesterbnd&amp;quot; 
# [5] &amp;quot;manchesterlsoa&amp;quot; &amp;quot;streetcrime&amp;quot;    &amp;quot;subdirs&amp;quot;

streetcrime[,c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)] &amp;lt;- jitter_longlat(streetcrime[,c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)], km=.6)
sum(duplicated(streetcrime[,c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)]))
# [1] 0

#Convert it into spatial points data frame

coordinates(streetcrime) &amp;lt;- ~Longitude+Latitude
streetcrime@data[,c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)]&amp;lt;- coordinates(streetcrime)

wgs84crs &amp;lt;- CRS(&amp;quot;+proj=longlat +datum=WGS84&amp;quot;)
proj4string(streetcrime) &amp;lt;- wgs84crs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisation-explorations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualisation &amp;amp; Explorations&lt;/h2&gt;
&lt;p&gt;As with any datasets, the first and foremost thing to do is to explore the data to understand its structures, its quirks and what if anything need to be cleaned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_tufte())

g_street &amp;lt;-  ggplot(streetcrime@data) +
  geom_bar(aes(x=fct_infreq(Crime_type))) +
  coord_flip() + 
  facet_wrap(~year)+
  labs(x=&amp;#39;Crime Type&amp;#39;, y=&amp;#39;Count&amp;#39;) 
#table(bicycletheft$Last_outcome_category)

g_street&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(ggrepel)

k &amp;lt;- streetcrime@data %&amp;gt;% count(Month, fct_infreq(Crime_type), sort = FALSE)
names(k) &amp;lt;- c(&amp;#39;Month&amp;#39;, &amp;quot;Crime_type&amp;quot;, &amp;quot;n&amp;quot;)

k %&amp;gt;%
  mutate(label = if_else(Month == max(Month), as.character(Crime_type), NA_character_))%&amp;gt;%
  ggplot(aes(x=Month, y=n, col= Crime_type))+
   geom_smooth()+ 
  scale_colour_discrete(guide = &amp;#39;none&amp;#39;) +
  geom_label_repel(aes(label = label),
                   nudge_x = 1,
                   na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The trends should be readily apparent. Violence and Sexual Offences, Public Order crimes have increased significantly, while Anti-social behaviour crimes have declined from 2016 to 2018 Q2. These statistics include the &lt;a href=&#34;https://www.theguardian.com/uk-news/manchester-arena-explosion&#34;&gt;Manchester Arena bombing in May 2017&lt;/a&gt;. One thing to notice though, is how few reported crimes actually result in an outcome. I am not entirely sure, if this a Manchester issue or if it is a general criminal justice issue.&lt;/p&gt;
&lt;p&gt;Also note the mixing of &lt;code&gt;%&amp;gt;%&lt;/code&gt; and &lt;code&gt;+&lt;/code&gt; in the following code. ggplot uses &lt;code&gt;+&lt;/code&gt; to build its graphics, while the rest of the analysis can be done with maggittr’s &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; g_street_outcome &amp;lt;-  streetcrime@data[!is.na(streetcrime$Last_outcome_category),]%&amp;gt;%
  ggplot() +
  geom_bar(aes(x=fct_infreq(Last_outcome_category), y = (..count..)/sum(..count..) * 100)) +
  coord_flip() + 
  labs(x=&amp;#39;Outcome&amp;#39;, y=&amp;#39;Percent&amp;#39;) 
  
  g_street_outcome&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is also illustrative to see the spatial extent and locations of crimes. We return to our standard way of visualising spatial data with Leaflet and basemaps from &lt;a href=&#34;https://carto.com/&#34;&gt;CartoDB&lt;/a&gt;. This allows us to zoom in and pan around the image to explore the data in some detail.&lt;/p&gt;
&lt;p&gt;From now on, for the sake of convenience, I will focus on “Bicycle Thefts” as they are few of them and it is easy for illustration purposes. You are welcome to try other types of crime. Furthermore, I restrict the analysis to thefts in 2016 Q2. We will return to the full series later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicycletheft &amp;lt;- streetcrime %&amp;gt;% subset(streetcrime$Crime_type == &amp;quot;Bicycle theft&amp;quot;)
bicycletheftQ2 &amp;lt;- bicycletheft[bicycletheft$Quarter == &amp;quot;2016 Q2&amp;quot;,]
nrow(bicycletheftQ2)
# [1] 1151

 map_bicycle &amp;lt;- bicycletheftQ2 %&amp;gt;%
  leaflet() %&amp;gt;%
  addProviderTiles(providers$CartoDB.Positron) %&amp;gt;%
  addMarkers(clusterOptions = markerClusterOptions(),
             popup = ~as.character(Last_outcome_category), 
             label = ~as.character(Crime_ID)
             )

 library(widgetframe)
frameWidget(map_bicycle)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-4.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Mapping the raw data points is not always useful. It is sometimes useful to summarize the data spatially. We can do that either by gridding the extent, aggregating to arbitrary polygonal boundaries or by creating a continuous surface through density estimates. Each has its advantages and disadvantages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gridsquadrat-counts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grids/Quadrat Counts&lt;/h2&gt;
&lt;p&gt;Quadrat counts are simple mechanisms to understand, visualise and test point patterns. It is as simple a overlaying a grid and counting the number of points in the grids. Because grids are of uniform area, the count is a proxy for density and because of the uniform area, unlike choropleth maps, there are no ‘large areas’ to dominate the map. However, the downside is that quadrat counts depends quite heavily on the resolution of the quadrat. To see this see the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra300 &amp;lt;- raster(extent(manchesterbnd), resolution=300, crs=proj4string(manchesterbnd))
ra300 &amp;lt;- mask(ra300, manchesterbnd)
ra300 &amp;lt;- projectRaster(ra300, crs = wgs84crs )
theftcounts300 &amp;lt;- rasterize(coordinates(bicycletheftQ2), ra300, fun=&amp;#39;count&amp;#39;, background=NA)
p1 &amp;lt;- levelplot(theftcounts300, par.settings = plasmaTheme, margin = list(FUN = &amp;#39;median&amp;#39;), main=&amp;quot;300m quadrats&amp;quot;)

ra1k &amp;lt;- raster(extent(manchesterbnd), resolution=1000, crs=proj4string(manchesterbnd))
ra1k &amp;lt;- mask(ra1k, manchesterbnd)
ra1k &amp;lt;- projectRaster(ra1k, crs = wgs84crs )
theftcounts1k &amp;lt;- rasterize(coordinates(bicycletheftQ2), ra1k, fun=&amp;#39;count&amp;#39;, background=NA)
p2 &amp;lt;- levelplot(theftcounts1k, par.settings = plasmaTheme, margin = list(FUN = &amp;#39;median&amp;#39;), main=&amp;quot;1km quadrats&amp;quot;)

print(p1, split=c(1, 1, 2, 1), more=TRUE)
print(p2, split=c(2, 1, 2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arbitary-polygons-zones&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Arbitary Polygons/ Zones&lt;/h2&gt;
&lt;p&gt;One of the problems with grids/quadrats is that they are arbitary and different grids resolutions have different outputs (MAUP). The other is that the grids are artificial, in that they do not follow natural or political geographies that may be relevant. They may be relevant for bringing in other data, or assigning responsibility and other administrative reasons. To overcome this, we can also spatially aggregate the data into arbitrary shaped zones or polygons. MAUP does not disappear with polygons, however.&lt;/p&gt;
&lt;p&gt;In this case, we will use the Lower layer Super Output Areas (LSOA). We need to make sure that they are of the same projections, for topological predicates to work. We use the &lt;code&gt;over&lt;/code&gt; function from sp library, though one could use &lt;code&gt;[&lt;/code&gt; function as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;manchesterlsoa &amp;lt;- spTransform(manchesterlsoa, wgs84crs)
manchesterlsoa$LSOA11CD &amp;lt;- as.character(manchesterlsoa$LSOA11CD)
bicyclelsoa &amp;lt;- over(bicycletheftQ2, manchesterlsoa) %&amp;gt;% group_by(LSOA11CD)%&amp;gt;%
  summarise(crimecount = n())
lsoa_crime &amp;lt;- merge(manchesterlsoa, bicyclelsoa, by=&amp;quot;LSOA11CD&amp;quot;)
lsoa_crime@data$crimecount[is.na(lsoa_crime@data$crimecount)] &amp;lt;- 0

m &amp;lt;-  leaflet(lsoa_crime) %&amp;gt;%
  addProviderTiles(providers$Stamen.TonerLines, group = &amp;quot;Basemap&amp;quot;,
                   options = providerTileOptions(minZoom = 9, maxZoom = 13)) %&amp;gt;%
   addProviderTiles(providers$Stamen.TonerLite, group = &amp;quot;Basemap&amp;quot;,
                    options = providerTileOptions(minZoom = 9, maxZoom = 13))

Npal &amp;lt;- colorNumeric(
  palette = &amp;quot;Reds&amp;quot;, n = 5,
  domain = lsoa_crime$crimecount
)

m2 &amp;lt;- m %&amp;gt;%
     addPolygons(color = &amp;quot;#CBC7C6&amp;quot;, weight = 0, smoothFactor = 0.5,
                 fillOpacity = 0.5,
             fillColor = Npal(lsoa_crime$crimecount),
             group = &amp;quot;LSOA&amp;quot;
             ) %&amp;gt;%
   addLegend(&amp;quot;topleft&amp;quot;, pal = Npal, values = ~crimecount,
             labFormat = function(type, cuts, p) {
               n = length(cuts) 
             paste0(prettyNum(cuts[-n], digits=0, big.mark = &amp;quot;,&amp;quot;, scientific=F), &amp;quot; - &amp;quot;, prettyNum(cuts[-1], digits=0, big.mark=&amp;quot;,&amp;quot;, scientific=F))
             },
             title = &amp;quot;Crimes&amp;quot;,
             opacity = 1
   ) %&amp;gt;%
  addLayersControl(
    overlayGroups = c(&amp;quot;LSOA&amp;quot;, &amp;#39;Basemap&amp;#39;),
    options = layersControlOptions(collapsed = FALSE)
      )

frameWidget(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-6.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;It should be relatively obvious what the issues of a choropleth maps are. Small LSOAs that have high concentrations of crimes are not readily visible, even when they are important. Color is a terrible way to represent the order; it is quite non-intuitive. One is almost better off looking at the table and identifying the top few LSOA’s that have high number of crimes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(bicyclelsoa[order(-bicyclelsoa$crimecount),], n=15)
# # A tibble: 15 × 2
#    LSOA11CD  crimecount
#    &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt;
#  1 E01033658         54
#  2 E01033653         37
#  3 E01033677         28
#  4 E01033664         24
#  5 E01005948         16
#  6 E01005209         13
#  7 E01005062         11
#  8 E01033656         11
#  9 E01033662         10
# 10 E01033669         10
# 11 E01005212          9
# 12 E01032687          9
# 13 E01005108          8
# 14 E01032906          8
# 15 E01006349          7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other issues include the binning of the data into different color introduces visual bias. Depending on the cuts, the maps are different. Analogously different boundaries for the zones/polygons produce radically different maps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kernel-density-heat-maps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kernel Density/ Heat Maps&lt;/h2&gt;
&lt;p&gt;Kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It is a data smoothing problem based on a finite data sample. The standard KDE in 2-dimensions uses a bivariate normal distribution to estimate. We can visualise it using ggplot’s &lt;code&gt;geom_density2d&lt;/code&gt; and &lt;code&gt;stat_density2d&lt;/code&gt; directly.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Note that &lt;code&gt;ggmap&lt;/code&gt; works with Google to identify locations and download basemaps. Often these requirements change. Please refer to the release notes and documentation of Google APIs and ggmap, if the following code does not work.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)

manchester &amp;lt;- get_stamenmap(bbox = bbox(manchesterlsoa), zoom = 10, maptype= &amp;quot;toner-lite&amp;quot;)

ggmap(manchester) + geom_density2d(data = bicycletheftQ2@data, aes(x = Longitude, y = Latitude), size = 0.3) + 
  stat_density2d(data = bicycletheftQ2@data, aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..), size=0.01, bins = 16, geom = &amp;quot;polygon&amp;quot;) +
  scale_fill_gradient(low = &amp;quot;green&amp;quot;, high = &amp;quot;red&amp;quot;, guide=&amp;quot;none&amp;quot;) + 
    scale_alpha(range = c(0, 0.3), guide = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Usually, Gaussian kernels are not all that great. There would be times, when bandwidths need to be changed or kernel forms need to be changed. R provides numerous ways to do this. In particular, check out &lt;code&gt;density&lt;/code&gt; in spatstat package or &lt;a href=&#34;https://cran.r-project.org/web/packages/spatialkernel/&#34;&gt;&lt;code&gt;spatialkernel&lt;/code&gt;&lt;/a&gt; package or use &lt;a href=&#34;https://gis.stackexchange.com/questions/150141/r-spatial-kernel-density-estimation&#34;&gt;SAGA GIS from R&lt;/a&gt;.
One could also use spatstat package, however, this requires creating a separate data structure that spatstat can understand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicycletheftQ2 &amp;lt;- spTransform(bicycletheftQ2, CRS(proj4string(manchesterbnd)))
bicycletheftQ2.ppp &amp;lt;- ppp(x=bicycletheftQ2@coords[,1],y=bicycletheftQ2@coords[,2],as.owin(manchesterbnd))
summary(bicycletheftQ2.ppp)
# Planar point pattern:  1151 points
# Average intensity 9.02051e-07 points per square unit
# 
# Coordinates are given to 2 decimal places
# i.e. rounded to the nearest multiple of 0.01 units
# 
# Window: polygonal boundary
# single connected closed polygon with 13669 vertices
# enclosing rectangle: [351662.6, 406087.2] x [381165.4, 421037.7] units
#                      (54420 x 39870 units)
# Window area = 1275980000 square units
# Fraction of frame area: 0.588
plot(density(bicycletheftQ2.ppp))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
The default kernel is Gaussian in spatstat. We can define arbitrary kernel shapes using a pixel image. See &lt;code&gt;?density.ppp&lt;/code&gt;. We can also use a &lt;code&gt;focal&lt;/code&gt; or &lt;code&gt;focalWeight&lt;/code&gt; functions to calculate local smoothed counts in the raster package.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Local intensity is not the same as kernel density. Kernel density is an estimate of probability, which means that it is non-negative and should sum to 1. Local intensity is simply a measure of neigborhood density (count of points in the neigborhood). The values produced by these methods will be different, but visual interpretations for most purposes, should be similar. Also note that intensity calculations in spatstat uses area, whose units depend on the coordinate system used.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;global-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Global Clustering&lt;/h2&gt;
&lt;p&gt;Global clustering is a way of determining if points are significantly different from the Complete Spatial Randomness or if there is a spatial stucture to it. We can estimate the level of global clustering using clark-evans test or Ripley’s K-function or nearest neighbor distance function G or empty space function F. In general, global clustering metrics is not useful to planners working on a local scale, therefore I am ignoring it here. You are referred to Baddeley et.al excellent practical book or &lt;a href=&#34;https://www.amazon.com/Statistics-Spatial-Wiley-Classics-Library/dp/1119114616&#34;&gt;Cressie’s classic book&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;local-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Local Clustering&lt;/h2&gt;
&lt;p&gt;Extracting local clusters of points is lot more complicated due to the issues of multiple testing and the presence of noise. The simplest way, widely accepted, is to aggregate the points to zones and estimate if the values are spatially autocorrelated using Moran’s I statistic.&lt;/p&gt;
&lt;div id=&#34;local-morans-i-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Local Moran’s I statistic&lt;/h3&gt;
&lt;p&gt;The local spatial statistic Moran’s I is calculated for each zone based on the spatial weights object used. The values returned include a Z-value, and may be used as a diagnostic tool. The statistic is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_i = \frac{(x_i-\bar{x})}{{∑_{k=1}^{n}(x_k-\bar{x})^2}/(n-1)}{∑_{j=1}^{n}w_{ij}(x_j-\bar{x})}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;, and its expectation and variance are given in Anselin (1995).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(spdep)

lsoa_crime_tmp &amp;lt;- lsoa_crime@data
row.names(lsoa_crime_tmp) &amp;lt;- sapply(slot(lsoa_crime, &amp;quot;polygons&amp;quot;), function(x) slot(x, &amp;quot;ID&amp;quot;)) 

lsoa_crime &amp;lt;- rgeos::gMakeValid(lsoa_crime, byid=T) # Fix invalid geometries
lsoa_crime  &amp;lt;- SpatialPolygonsDataFrame(lsoa_crime, lsoa_crime_tmp)

lsoa_nb &amp;lt;- poly2nb(lsoa_crime)  #queen&amp;#39;s neighborhood
(lsoa_nb_w &amp;lt;- nb2listw(lsoa_nb)) #convert to listw object
# Characteristics of weights list object:
# Neighbour list object:
# Number of regions: 1740 
# Number of nonzero links: 10402 
# Percentage nonzero weights: 0.3435725 
# Average number of links: 5.978161 
# 
# Weights style: W 
# Weights constants summary:
#      n      nn   S0       S1       S2
# W 1740 3027600 1740 607.6169 7209.152
lsoa_crime$s_crimecount &amp;lt;- scale(lsoa_crime$crimecount)  #Scale and Center the variable of interst
lsoa_crime$lag_scrimecount &amp;lt;- lag.listw(lsoa_nb_w, lsoa_crime$s_crimecount) #Create lagged variable

p&amp;lt;-ggplot(lsoa_crime@data, aes(x=s_crimecount, y=lag_scrimecount)) +
  geom_point() +
  coord_fixed() +  
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)  + 
  labs(x=&amp;quot;Bicycle Thefts (standardised)&amp;quot;, y=&amp;quot;Spatially Lagged Bicycle Thefts (standardised)&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
lsoa_moran &amp;lt;- localmoran(lsoa_crime@data$crimecount, lsoa_nb_w)  #calculate the local moran&amp;#39;s I

nrow(lsoa_moran[lsoa_moran[,5] &amp;lt;= 0.05,]) #Count the number of zones that are significant.
# [1] 71

lsoa_crime@data  &amp;lt;- lsoa_crime@data %&amp;gt;%
    plyr::mutate(sig_char =
            dplyr::case_when(lsoa_moran[,5] &amp;lt;=.05 &amp;amp; s_crimecount&amp;gt;0 &amp;amp; lag_scrimecount&amp;gt;0     ~ &amp;quot;High-High&amp;quot;,
                      lsoa_moran[,5] &amp;lt;=.05 &amp;amp; s_crimecount&amp;gt;0 &amp;amp; lag_scrimecount&amp;lt;0  ~ &amp;quot;High-Low&amp;quot;,
                      lsoa_moran[,5] &amp;lt;=.05 &amp;amp; s_crimecount&amp;lt;0 &amp;amp; lag_scrimecount&amp;gt;0  ~ &amp;quot;Low-High&amp;quot;,
                      lsoa_moran[,5] &amp;lt;=.05 &amp;amp; s_crimecount&amp;lt;0 &amp;amp; lag_scrimecount&amp;lt;0  ~ &amp;quot;Low-Low&amp;quot;,
                      TRUE                     ~ &amp;quot;Not Significant&amp;quot;
            )) 

lsoa_crime@data$sig_char &amp;lt;- as.factor(lsoa_crime@data$sig_char)
summary(lsoa_crime@data$sig_char) # Check to see if the refactorisation worked ok.
#       High-High        Low-High Not Significant 
#              66               5            1669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we know that there are only one category of significant autocorrelation, we will just use two colors (Red and White) to visualise. However, in general case, we are usually interested in both High-High and High-Low clusters, i.e. zones that have high values surrounded by high values and zones that have high values surrounded by low values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
Fpal &amp;lt;- colorFactor(c(&amp;quot;#EE0000&amp;quot;, &amp;quot;#FFFFFF&amp;quot;), lsoa_crime@data$sig_char)

m3 &amp;lt;- m %&amp;gt;%
     addPolygons(color = &amp;quot;#CBC7C6&amp;quot;, weight = .5, smoothFactor = 0.5,
                 fillOpacity = 0.7,
             fillColor = Fpal(lsoa_crime@data$sig_char),
             group = &amp;quot;LSOA&amp;quot;
             ) %&amp;gt;%
   addLegend(&amp;quot;topleft&amp;quot;, pal = Fpal, values = ~lsoa_crime@data$sig_char,
             title = &amp;quot;Bicycle Thefts (Significant Clusters)&amp;quot;,
             opacity = 1
   ) %&amp;gt;%
  addLayersControl(
    overlayGroups = c(&amp;quot;LSOA&amp;quot;, &amp;#39;Basemap&amp;#39;),
    options = layersControlOptions(collapsed = FALSE)
      )

frameWidget(m3)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-11.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Few points to note here.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The geography and zone size matters quite a bit. Changing the LSOA to a grid of arbitrary size changes the statistics and locations of the clusters. Instead of LSOA, if you use census output area (OA) or Middle Layer output area (MSOA) the results will be different. Try this as an exercise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The weight matrix matters quite a bit as well. In this exercise, the weights are built from queen continguity of zones and are row standardised (default). There are any number of other formulations that we could have used and may be more appropriate for specific use cases. In particular, see &lt;code&gt;dnearneigh&lt;/code&gt;, &lt;code&gt;knearneigh&lt;/code&gt;, &lt;code&gt;poly2nb&lt;/code&gt;, &lt;code&gt;graphneigh&lt;/code&gt; in the &lt;code&gt;spdep&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The presence of NAs in the variable of interest, throws off the calculations. Care should be taken to adjust NA especially when using neighbours. In some cases, NAs can be turned to 0s. In others, it is not appropriate. In any case, care should be taken about the neighbour lists, when the observations are dropped from the analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally and more importantly, this cluster analysis does not account for the underlying population at risk. It may very well be the case that the clusters we are observing are an artefact of underlying population distribution. This can be easily rectified by using the density/propensity of bike thefts instead of the raw counts. The choice of a denominator (population, employees, number of bicycles etc.) is arbitrary and should be externally justified. I leave this as an exercise.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-dbscan-or-optics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using DBSCAN or Optics&lt;/h2&gt;
&lt;p&gt;If we do not need statistically significant clusters, we can use one of the more popular clustering algorithms in Unsupervised classification called Density Based Spatial Clustering of Applications with Noise (DBSCAN). DBSCAN was detailed by &lt;a href=&#34;https://rdcu.be/3i8O&#34;&gt;Ester et.al&lt;/a&gt; in 1996 and received the “Test of Time” award in 2014.&lt;/p&gt;
&lt;p&gt;The algorithm uses two parameters:
- &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (eps) is the radius of our neighbourhoods around a data point &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.
- &lt;span class=&#34;math inline&#34;&gt;\(minPts\)&lt;/span&gt; is the minimum number of data points we want in a neighborhood to define a cluster.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/DBSCAN-Illustration.svg&#34; width=&#34;400&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Illustration of DBSCAN algorithm, from &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:DBSCAN-Illustration.svg&#34; class=&#34;uri&#34;&gt;https://commons.wikimedia.org/wiki/File:DBSCAN-Illustration.svg&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Once these parameters are defined, the algorithm divides the data points into three points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Core points&lt;/code&gt;. A point &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is a core point if at least &lt;span class=&#34;math inline&#34;&gt;\(minPts\)&lt;/span&gt; points are within distance &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; .&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Border points&lt;/code&gt;. A point &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is border point for &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; if there is a path &lt;span class=&#34;math inline&#34;&gt;\(P_1\)&lt;/span&gt;, …, &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(P_1\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, where each &lt;span class=&#34;math inline&#34;&gt;\(P_{i+1}\)&lt;/span&gt;is directly reachable from &lt;span class=&#34;math inline&#34;&gt;\(P_i\)&lt;/span&gt; (all the points on the path must be core points, with the possible exception of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Outliers&lt;/code&gt;. All points not reachable from any other point are outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The steps in DBSCAN are simple after defining the previous steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pick at random a point which is not assigned to a cluster and calculate its &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-neighborhood. If there are atleast &lt;span class=&#34;math inline&#34;&gt;\(minPoints\)&lt;/span&gt; in the neighborhood, mark it a core point and a cluster; otherwise, mark it as outlier.&lt;/li&gt;
&lt;li&gt;Once all core points are found, start expanding that to include border points.&lt;/li&gt;
&lt;li&gt;Repeat these steps until all the points are either assigned to a cluster or to an outlier.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parameters are key and significantly affect the results. One heuristic to detrmine &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is to look at the kink in the dist plot of k-nearest neighbors. The intutition is that, at the kink, each points starts having a lot of neighbors. &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is usually dimension of the data + 1, in our case is 3. This is also the &lt;span class=&#34;math inline&#34;&gt;\(minPts\)&lt;/span&gt;. Other values are possible depending on the external criteria.&lt;/p&gt;
&lt;p&gt;We will use dbscan library instead of the dbscan in fpc library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dbscan)
bicycletheftQ2 &amp;lt;- bicycletheftQ2 %&amp;gt;% spTransform(wgs84crs) # Doing this for visualisation using leaflet. Non-WGS84 CRS is more complicated with leaflet.
kNNdistplot(bicycletheftQ2@data[,c(&amp;#39;Longitude&amp;#39;, &amp;#39;Latitude&amp;#39;)], k = 5)
abline(h=.012, col=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/cluster-detection-in-point-data/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;DBscan is relatively quick as evidenced by the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db_clusters_bicycletheft&amp;lt;- dbscan(bicycletheftQ2@data[,c(&amp;#39;Longitude&amp;#39;, &amp;#39;Latitude&amp;#39;)], eps=0.012, minPts=5, borderPoints = TRUE)
print(db_clusters_bicycletheft)
# DBSCAN clustering for 1151 objects.
# Parameters: eps = 0.012, minPts = 5
# The clustering contains 21 cluster(s) and 143 noise points.
# 
#   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 
# 143  10  20  10 663   9  63   8  21   3   9  31  12  54  10  14  24  17   6   9 
#  20  21 
#   8   7 
# 
# Available fields: cluster, eps, minPts

bicycletheftQ2@data$dbscan_cluster &amp;lt;- db_clusters_bicycletheft$cluster&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;21 clusters of points are identified with about 12% of the points are not assigned to a cluster (noise).
We can construct a concavehull around these points to identify the cluster boundaries and visualise them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(concaveman)
library(sf)

clusterpolys &amp;lt;- bicycletheftQ2 %&amp;gt;% st_as_sf() %&amp;gt;% 
  split(bicycletheftQ2@data$dbscan_cluster) %&amp;gt;%
  lapply(concaveman, concavity=3) %&amp;gt;%
  lapply(as_Spatial)

clusterpolys &amp;lt;- lapply(2:length(clusterpolys), function(x){spChFIDs(clusterpolys[[x]],names(clusterpolys)[x])}) %&amp;gt;%
  lapply(function(x){x@polygons[[1]]}) %&amp;gt;%
  SpatialPolygons

clusterpolys &amp;lt;- SpatialPolygonsDataFrame(clusterpolys, data=data.frame(ID=row.names(clusterpolys)))

library(RColorBrewer)  

factpal &amp;lt;- colorFactor(brewer.pal(nrow(clusterpolys),&amp;quot;Set3&amp;quot;), clusterpolys$ID)


map_bicycle2 &amp;lt;- clusterpolys %&amp;gt;% 
  leaflet() %&amp;gt;%
  addProviderTiles(providers$CartoDB.Positron) %&amp;gt;%
  addPolygons(color = ~factpal(clusterpolys$ID), weight = 5, smoothFactor = 0.5,
              opacity = 1,
              fillColor = ~factpal(clusterpolys$ID), fillOpacity = .5,
              highlightOptions = highlightOptions(color = &amp;quot;green&amp;quot;, weight = 2, bringToFront = TRUE)
  )%&amp;gt;%
  addCircles(data=bicycletheftQ2, weight = 3, radius=40, 
             color=~~factpal(clusterpolys$ID), stroke = TRUE, fillOpacity = 0.8)

frameWidget(map_bicycle2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-14.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Few points to note here.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;DBSCAN is quite popular and the advantage is that we do not need to define the number of clusters unlike k-means or Partition around Medoids. However, it is very sensitive to the parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is no generative model that DBSCAN uses. So the clusters idenitified could be spurious depending on underlying variation in the population distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the density of the clusters vary, DBSCAN is less likely to identify them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extensions for anisotropic neighborhoods can be found and may be more useful than a spherical neighborhood.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DBSCAN can be relatively easily expanded to space-time cluster detection. The distance in time should appropriately be scaled to match Euclidean distance in space. dbscan can take a precomputed distance object, so the implemenation is relatively straightforward. I leave this as an exercise.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;From all the visualisations, it should be apparent that the results are dependent on what analyses you pick. There is no right way to proceed with the analyses, but mostly conventions and acceptability in the field determine the choice of the analytical method. These soft skills and external justifications are as important, if not more, than using the cutting-edge algorithms and methods. These skills come from experience and practise and is contingent on time and place.&lt;/p&gt;
&lt;p&gt;In these analyses, we have ignored the time dimension. We also ignored many other methods and techniques that are more prevalent in other fields (e.g.scan statistics in epidemiology. See &lt;a href=&#34;https://dx.doi.org/10.1177/0042098013484540&#34;&gt;Kaza et.al (2013)&lt;/a&gt; for an application to planning.)It is worth noting that the absence of observations do not matter as much in the above analyses, but could be quite important. It is also important to understand the representational assumptions. For example, crime is represented as a point, though it could have happened at an address whose buildings might have different areas in space. Such abstractions have to be justified for particular analytical purposes. Nonetheless, it is useful to keep abreast of various techniques that might help identify clusters of observed points, so that we can deliberate how resources can be prioritised and directed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accomplishments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accomplishments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reading point data into R. Constructing a point pattern for use with spatstat&lt;/li&gt;
&lt;li&gt;Vector data operations (Reproject, Buffer)&lt;/li&gt;
&lt;li&gt;Point-in-Polygon Operations&lt;/li&gt;
&lt;li&gt;Spatial kernel density estimation&lt;/li&gt;
&lt;li&gt;Visualising spatial data in R&lt;/li&gt;
&lt;li&gt;Exploratory spatial data analysis&lt;/li&gt;
&lt;li&gt;Spatial clustering using DBSCAN&lt;/li&gt;
&lt;li&gt;Neighborhood weight matrices&lt;/li&gt;
&lt;li&gt;Local spatial autocorrelation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Smart Water Management and Internet of Things</title>
      <link>https://nkaza.github.io/project/smartwater-iot/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/project/smartwater-iot/</guid>
      <description>&lt;p&gt;Urban areas face daunting environmental, socioecological, and infrastructure challenges. Information and communications technology (ICT) promises unprecedented capabilities to enable cities to improve the quality of life, efficiency of urban operations and services, and competitiveness, while maintaining sustainable use of resources. Ubiquitous sensing, information processing, and wireless networks are quickly becoming embedded in the fabric of contemporary cities, and the Internet of Things (IoT) connects everyday objects and devices to network technologies. The proliferation of these technologies enables the advent of “Smart Cities” that utilize ICT, IoT, and big data analytics to address critical urban challenges.&lt;/p&gt;
&lt;p&gt;Water supply and infrastructure are major areas of concern for American cities and grand challenges facing engineers. Urban areas are running out of clean reliable sources of water, and innovative solutions are needed for long-term planning [7]. The US drinking water infrastructure serves 315 million people and is in need of replacement, upgrading, and maintenance to continue to support projected population growth. The American Society of Civil Engineers rated drinking water infrastructure with a grade of D- in 2009 and D in 2013, while the American Water Works Association estimates the cost of repairing and expanding US drinking water infrastructure at over $1 trillion through 2035 or $1.7 trillion through 2050. Because of the lapse in infrastructure maintenance, 77 million people are served by more than 18,000 water systems with water quality violations (based on 2015 data), and 2.1 trillion gallons of water are lost annually due to aging and leaky pipes, broken water mains, and faulty meters.&lt;/p&gt;
&lt;p&gt;A promising application for smart and connected communities is the use of ICT and IoT technologies to improve urban water supply management. The IoT can connect personal smart devices with faucets and pipelines that are embedded with sensors, actuators, and network connectivity to collect and report real-time information about water consumption, quality, and losses. A water-smart city can sustainably use and reuse water resources by adapting real-time operations and planning practices in response to ubiquitous sensor networks and disparate but interconnected and heterogeneous data streams. While a survey of the water industry shows that 33% of utilities are interested in real-time control and big data system analytics, water utilities have predominantly not harnessed these technologies,  due to a number of challenges associated with managing and analyzing big data. Technological gaps, workforce challenges, and community disengagement undermine the alignment of critical municipal management priorities with the analysis and application of smart water data. Installing data analytics systems can worsen data deluge, which is a serious challenge for municipalities, utilities, and their constituencies. The ubiquity of different types of sensors and data collection mechanisms obscures the issues with frequency and asynchronicity of data collection, the types of data generated, and gaps in datasets. Utilities that have installed smart meter systems need support to make sense of and apply data for decision-making, and applications are lacking that would demonstrate that the use of smart systems will support long-term sustainability and urban planning goals. The next generation smart water system should provide the analysis to guide water resources sustainability, stand as a first line of defense for communities that suffer from water quality issues, and catalyze a culture of water conservation within communities.&lt;/p&gt;
&lt;h2 id=&#34;collaborators&#34;&gt;Collaborators&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ccee.ncsu.edu/people/emzechma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Emily Berglund&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Urban Analytics</title>
      <link>https://nkaza.github.io/slides/machinelearning/ml_slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/slides/machinelearning/ml_slides/</guid>
      <description>
&lt;script src=&#34;https://nkaza.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;class: right, bottom&lt;/p&gt;
&lt;div id=&#34;machine-learning-for-urban-analytics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning for Urban Analytics&lt;/h2&gt;
&lt;div id=&#34;nikhil-kaza&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Nikhil Kaza&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;department-of-city-regional-planning-university-of-north-carolina-at-chapel-hill&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Department of City &amp;amp; Regional Planning &lt;br /&gt; University of North Carolina at Chapel Hill&lt;/h5&gt;
&lt;div id=&#34;updated-2022-01-22&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;updated: 2022-01-22&lt;/h6&gt;
&lt;table style=&#34;width:6%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;# What is Machine Learning ?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;img src=&#34;figs/ML_what.jpg&#34; width=&#34;360&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-purpose-of-machine-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The purpose of Machine Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Mostly for Prediction…
&lt;ul&gt;
&lt;li&gt;Classification (Categories of objects e.g. spam/not spam; median strip /side walk/road, default/ prepayment / Current)&lt;/li&gt;
&lt;li&gt;Regression (Continous variables, e.g. volume of water consumption/ energy use )
&lt;ul&gt;
&lt;li&gt;Not the same as statistical inference such as linear regression.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;### OK. What kinds of prediction?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local governments: Traffic congestion&lt;/li&gt;
&lt;li&gt;Google: What ads to show&lt;/li&gt;
&lt;li&gt;Amazon: What products to buy&lt;/li&gt;
&lt;li&gt;Insurance: Risk based on prior claims&lt;/li&gt;
&lt;li&gt;UNC: Sakai use to identify students in need of intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;different-terms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Different Terms&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Prediction&lt;/li&gt;
&lt;li&gt;Projection&lt;/li&gt;
&lt;li&gt;Forecast&lt;/li&gt;
&lt;li&gt;Scenarios&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;what-do-you-think-the-differences-are&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What do you think the differences are?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figs/sunspots.png&#34; width=&#34;1200&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-central-dogma-of-prediction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The central dogma of prediction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;figs/centraldogma.png&#34; width=&#34;678&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;components-of-a-predictor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Components of a predictor&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; features -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
&lt;redtext&gt;question&lt;/redtext&gt; -&amp;gt; input data -&amp;gt; features -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Start with a general question&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Can I automatically detect emails that are SPAM that are not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Make it concrete&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
question -&amp;gt; &lt;redtext&gt;input data &lt;/redtext&gt; -&amp;gt; features -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;p&gt;&lt;img class=center src=./figs/spamR.png height=&#39;400&#39; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html&#34;&gt;http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; &lt;redtext&gt;features&lt;/redtext&gt; -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;
Dear Jeff,&lt;/p&gt;
&lt;p&gt;Can you send me your address so I can send you the invitation?&lt;/p&gt;
&lt;p&gt;Thanks,&lt;/p&gt;
&lt;p&gt;Ben
&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; &lt;redtext&gt;features&lt;/redtext&gt; -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;/p&gt;
&lt;p&gt;Dear Jeff,&lt;/p&gt;
&lt;p&gt;Can &lt;rt&gt;you&lt;/rt&gt; send me your address so I can send &lt;rt&gt;you&lt;/rt&gt; the invitation?&lt;/p&gt;
&lt;p&gt;Thanks,&lt;/p&gt;
&lt;p&gt;Ben
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Frequency of you &lt;span class=&#34;math inline&#34;&gt;\(= 2/17 = 0.118\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; &lt;redtext&gt;features&lt;/redtext&gt; -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kernlab)
data(spam)
str(spam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    4601 obs. of  58 variables:
##  $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
##  $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...
##  $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
##  $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ our              : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
##  $ over             : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...
##  $ remove           : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...
##  $ internet         : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...
##  $ order            : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...
##  $ mail             : num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...
##  $ receive          : num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...
##  $ will             : num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...
##  $ people           : num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...
##  $ report           : num  0 0.21 0 0 0 0 0 0 0 0 ...
##  $ addresses        : num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...
##  $ free             : num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...
##  $ business         : num  0 0.07 0.06 0 0 0 0 0 0 0 ...
##  $ email            : num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...
##  $ you              : num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...
##  $ credit           : num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...
##  $ your             : num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...
##  $ font             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num000           : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...
##  $ money            : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...
##  $ hp               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hpl              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ george           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num650           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ lab              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ labs             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ telnet           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num857           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ data             : num  0 0 0 0 0 0 0 0 0.15 0 ...
##  $ num415           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num85            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ technology       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num1999          : num  0 0.07 0 0 0 0 0 0 0 0 ...
##  $ parts            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ pm               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ direct           : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ cs               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ meeting          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ original         : num  0 0 0.12 0 0 0 0 0 0.3 0 ...
##  $ project          : num  0 0 0 0 0 0 0 0 0 0.06 ...
##  $ re               : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ edu              : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ table            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ conference       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charSemicolon    : num  0 0 0.01 0 0 0 0 0 0 0.04 ...
##  $ charRoundbracket : num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...
##  $ charSquarebracket: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charExclamation  : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
##  $ charDollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
##  $ charHash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...
##  $ capitalAve       : num  3.76 5.11 9.82 3.54 3.54 ...
##  $ capitalLong      : num  61 101 485 40 40 15 4 11 445 43 ...
##  $ capitalTotal     : num  278 1028 2259 191 191 ...
##  $ type             : Factor w/ 2 levels &amp;quot;nonspam&amp;quot;,&amp;quot;spam&amp;quot;: 2 2 2 2 2 2 2 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(spam$type)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nonspam
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
spam
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2788
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1813
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; features -&amp;gt; &lt;redtext&gt;algorithm&lt;/redtext&gt; -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(spam$your[spam$type==&amp;quot;nonspam&amp;quot;]),
     col=&amp;quot;blue&amp;quot;,main=&amp;quot;&amp;quot;,xlab=&amp;quot;Frequency of &amp;#39;your&amp;#39;&amp;quot;)
lines(density(spam$your[spam$type==&amp;quot;spam&amp;quot;]),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-7&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; features -&amp;gt; &lt;redtext&gt;algorithm&lt;/redtext&gt; -&amp;gt; parameters -&amp;gt; evaluation
&lt;/center&gt;
&lt;p&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find a value &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;frequency of ‘your’ &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;\)&lt;/span&gt; C&lt;/strong&gt; predict “spam”&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-8&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; features -&amp;gt; algorithm -&amp;gt; &lt;redtext&gt;parameters&lt;/redtext&gt; -&amp;gt; evaluation
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(spam$your[spam$type==&amp;quot;nonspam&amp;quot;]),
     col=&amp;quot;blue&amp;quot;,main=&amp;quot;&amp;quot;,xlab=&amp;quot;Frequency of &amp;#39;your&amp;#39;&amp;quot;)
lines(density(spam$your[spam$type==&amp;quot;spam&amp;quot;]),col=&amp;quot;red&amp;quot;)
abline(v=0.5,col=&amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example-9&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SPAM Example&lt;/h1&gt;
&lt;center&gt;
question -&amp;gt; input data -&amp;gt; features -&amp;gt; algorithm -&amp;gt; parameters -&amp;gt; &lt;redtext&gt;evaluation&lt;/redtext&gt;
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prediction &amp;lt;- ifelse(spam$your &amp;gt; 0.5,&amp;quot;spam&amp;quot;,&amp;quot;nonspam&amp;quot;)
table(prediction,spam$type)/length(spam$type)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nonspam
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
spam
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
nonspam
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4590306
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1017170
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
spam
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1469246
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2923278
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Accuracy &lt;span class=&#34;math inline&#34;&gt;\(\approx 0.459 + 0.292 = 0.751\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bike-sharing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bike Sharing&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;General Q: Can you predict which stations will need to be restocked with bikes at different times of the day?
&lt;ul&gt;
&lt;li&gt;How can you use neighborhood characterstics (demographics, economics, proxmity to other stations) and time of day, day of the week, season, weather etc. to predict number of open slots on bike stations?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class=center src=./figs/ML_process.png height=&#39;350&#39; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;urban-sprawl-environmental-impacts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Urban Sprawl &amp;amp; Environmental Impacts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;General Q: Can you predict number of bad air quality days from urban form characteristics?
&lt;ul&gt;
&lt;li&gt;Can urban landscape metrics and other demographic characteristics predict bad air quality days in a year?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class=center src=./figs/ML_process.png height=&#39;350&#39; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;urban-form-healthy-behaviours&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Urban Form &amp;amp; Healthy Behaviours&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;General Q: How does urban form characteristics relate to healthy outcomes?
&lt;ul&gt;
&lt;li&gt;How does street density, intersection density, activity density etc. impact residents’ healthy behaviours (healthy food consumption, exercise etc.)?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class=center src=./figs/ML_process.png height=&#39;350&#39; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;energy-conservation-mortgage-risks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Energy Conservation &amp;amp; Mortgage Risks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;General Q: Should we reward households with conservation proclivities with a break on mortgage interest rates?
&lt;ul&gt;
&lt;li&gt;Is the choice to buy energy star appliances and houses in infill urban areas correlated with lower default/prepayment rate?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class=center src=./figs/ML_process.png height=&#39;350&#39; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;mode-choice&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mode Choice&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;General Q: Can we predict household transportation mode choice?
&lt;ul&gt;
&lt;li&gt;Given the weather, cost of travel, cost of parking etc. what is the likelihood that a household will choose to drive vs. taking public transit.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class=center src=./figs/ML_process.png height=&#39;350&#39; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;experimental-design&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experimental Design&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/studydesign.png height = &#39;200&#39;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=center src=./figs/cross_validation.png height=&#39;300&#39; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;K Nearest Neighbor&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/knnClassification.png height = &#39;400&#39;/&gt;&lt;/p&gt;
&lt;div id=&#34;footnote-httpsen.wikipedia.orgwikik-nearest_neighbors_algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;.footnote[ &lt;a href=&#34;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&lt;/a&gt;]&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-misnomer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression (Misnomer)&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/logisticregression.png height = &#39;400&#39;/&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/&#34; class=&#34;uri&#34;&gt;http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;trees&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trees&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/regressiontree.gif height = &#39;400&#39;/&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;https://www.techemergence.com/what-is-machine-learning/&#34; class=&#34;uri&#34;&gt;https://www.techemergence.com/what-is-machine-learning/&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;trees-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trees&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/obamatree.png height = &#39;600&#39;/&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;https://nyti.ms/2QRnQxI&#34; class=&#34;uri&#34;&gt;https://nyti.ms/2QRnQxI&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;forests-ensembles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Forests (Ensembles)&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/forests.png height=400&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf&#34; class=&#34;uri&#34;&gt;http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;and-lots-more&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And lots more…&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/caretmodels.png height=700&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;class: right, bottom, inverse&lt;/p&gt;
&lt;div id=&#34;some-terminology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Terminology&lt;/h2&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ensembling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensembling&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/ensembling2.png height=350&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/&#34; class=&#34;uri&#34;&gt;https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-aggregating-bagging&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bootstrap aggregating (bagging)&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Resample cases and recalculate predictions&lt;/li&gt;
&lt;li&gt;Average or majority vote&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img class=center src=./figs/bagging.png height=400&gt;&lt;/p&gt;
&lt;p&gt;.footnote[
list()]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;boosting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Boosting&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a model&lt;/li&gt;
&lt;li&gt;Focus on the errors of the model and create another model&lt;/li&gt;
&lt;li&gt;Continue this process until no improvement occurs&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img class=center src=./figs/boosted-trees-process.png height=400&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/&#34; class=&#34;uri&#34;&gt;https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/&lt;/a&gt;]&lt;/p&gt;
&lt;table style=&#34;width:6%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;# Boosting Explained&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;img class=center src=./figs/boosting1.png height=500&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;.footnote[&lt;a href=&#34;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&#34; class=&#34;uri&#34;&gt;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&lt;/a&gt;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;boosting-explained&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Boosting Explained&lt;/h1&gt;
&lt;p&gt;&lt;img class=center src=./figs/boosting2.png height=500&gt;&lt;/p&gt;
&lt;p&gt;.footnote[&lt;a href=&#34;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&#34; class=&#34;uri&#34;&gt;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&lt;/a&gt;]&lt;/p&gt;
&lt;table style=&#34;width:6%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;## Basic terms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;In general, &lt;strong&gt;Positive&lt;/strong&gt; = identified and &lt;strong&gt;negative&lt;/strong&gt; = rejected. Therefore:&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;True positive&lt;/strong&gt; (TP) = correctly identified (e.g. Real buildings identified as buildings by the model.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;strong&gt;False positive&lt;/strong&gt; (FP) = incorrectly identified (e.g. Real non-buildings identified as buildings)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;True negative&lt;/strong&gt; (TN) = correctly rejected (e.g. Real non-buildings identified as non-buildings by the model)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;strong&gt;False negative&lt;/strong&gt; (FN) = incorrectly rejected (e.g. Real buildings identified as roads by the model)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Sensitivity_and_specificity&#34;&gt;http://en.wikipedia.org/wiki/Sensitivity_and_specificity&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;accuracy-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Accuracy Metrics&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Mean squared error (or root mean squared error)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Continuous data, sensitive to outliers&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Median absolute deviation&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Continuous data, often more robust&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sensitivity (recall): &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;If you want few missed positives (e.g. identify as many buildings as possible, even if you misidentify some non-buildings as buildings)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specificity: &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;If you want few negatives called positives (e.g. identify more buildings correctly, even if you miss some true buildings )&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Accuracy &lt;span class=&#34;math inline&#34;&gt;\((TP+TN)/(TP + TN + FP + FN)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Weights false positives/negatives equally&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Concordance&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;One example is &lt;a href=&#34;http://en.wikipedia.org/wiki/Cohen%27s_kappa&#34;&gt;kappa&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Predictive value of a positive (precision): &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP +FP)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;When the prevalance is low (e.g. identify a rare class of a ‘tent city’ in US cities)&lt;/li&gt;
&lt;/ul&gt;
&lt;table style=&#34;width:6%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;# Conclusion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;img class=center src=./figs/mlconsiderations.jpg height=500&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;practical-advice&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Practical Advice&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Focus on the importance of the problem&lt;/li&gt;
&lt;li&gt;Try simple models first&lt;/li&gt;
&lt;li&gt;Much of machine learning is about trying to create good features (variables); Models are secondary&lt;/li&gt;
&lt;li&gt;Scale the features to have similar values (sale price in millions, sq.ft in 1000s don’t work well)&lt;/li&gt;
&lt;li&gt;Ideally you want these features to be minimally correlated&lt;/li&gt;
&lt;li&gt;Some algorithms requires lots of training data. Focus on creating good labelled data. Share it with others&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
