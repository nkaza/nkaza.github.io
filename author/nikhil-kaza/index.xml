<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nikhil Kaza</title>
    <link>https://nkaza.github.io/author/nikhil-kaza/</link>
      <atom:link href="https://nkaza.github.io/author/nikhil-kaza/index.xml" rel="self" type="application/rss+xml" />
    <description>Nikhil Kaza</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2018-2023 Nikhil Kaza</copyright><lastBuildDate>Sun, 15 Sep 2024 09:22:28 +0000</lastBuildDate>
    <image>
      <url>https://nkaza.github.io/media/icon_hu1ca6a6912ef6c300619228a995d3f134_46128_512x512_fill_lanczos_center_3.png</url>
      <title>Nikhil Kaza</title>
      <link>https://nkaza.github.io/author/nikhil-kaza/</link>
    </image>
    
    <item>
      <title>Do development disincentives influence land conservation activity?</title>
      <link>https://nkaza.github.io/publication/branham-conservation/</link>
      <pubDate>Sun, 15 Sep 2024 09:22:28 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/branham-conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Retraining for Energy Transition: A Workforce Development Approach Using Occupational Similarity and Unsupervised Clustering</title>
      <link>https://nkaza.github.io/publication/khanal-energytransition/</link>
      <pubDate>Sun, 15 Sep 2024 09:22:27 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/khanal-energytransition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Dashboard Is Not Dead: Dashboards as Effective Tools in Skills Building, Sense-Making and Community Collaboration</title>
      <link>https://nkaza.github.io/publication/donald-dashboard-not-dead/</link>
      <pubDate>Sun, 15 Sep 2024 09:22:27 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/donald-dashboard-not-dead/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extension, Densification, Dispersion and Stagnation: Patterns in Urban Spatial Development in Metropolitan United States 2001-2021</title>
      <link>https://nkaza.github.io/publication/cardwell-aa/</link>
      <pubDate>Sun, 15 Sep 2024 09:22:25 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/cardwell-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Manufactured Housing in North Carolina: A Computer Vision Approach</title>
      <link>https://nkaza.github.io/publication/khanal-aa/</link>
      <pubDate>Sun, 15 Sep 2024 09:22:25 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/khanal-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiclass Compactness Index for Urban Areas</title>
      <link>https://nkaza.github.io/publication/kaza-multiclass-compactness-index-2024/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-multiclass-compactness-index-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advanced Spatial Analysis in R</title>
      <link>https://nkaza.github.io/talk/advanced-spatial-analysis-in-r/</link>
      <pubDate>Thu, 11 May 2023 08:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/advanced-spatial-analysis-in-r/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spatial Analysis in R</title>
      <link>https://nkaza.github.io/talk/spatial-analysis-in-r/</link>
      <pubDate>Thu, 04 May 2023 08:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/spatial-analysis-in-r/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visualisation Principles</title>
      <link>https://nkaza.github.io/talk/visualisation-principles/</link>
      <pubDate>Sun, 30 Apr 2023 08:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/visualisation-principles/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to R &amp; Urban Analytics</title>
      <link>https://nkaza.github.io/talk/introduction-to-r-urban-analytics/</link>
      <pubDate>Sun, 23 Apr 2023 08:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/introduction-to-r-urban-analytics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data driven approaches to quantifying relationships between urban form &amp; livability</title>
      <link>https://nkaza.github.io/talk/data-driven-approaches-to-quantifying-relationships-between-urban-form-livability/</link>
      <pubDate>Fri, 24 Feb 2023 13:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/data-driven-approaches-to-quantifying-relationships-between-urban-form-livability/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Targeting Occupations to Retrain for Clean Energy Workforce Development And Implications for Labor Market Dynamics</title>
      <link>https://nkaza.github.io/publication/khanal-2023-aa/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/khanal-2023-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating generalisation of deep learning computer vision models for satellite data and global development</title>
      <link>https://nkaza.github.io/publication/khanal-2022-aa/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/khanal-2022-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Against the Idea of Implementation in Planning</title>
      <link>https://nkaza.github.io/talk/against-the-idea-of-implementation-in-planning/</link>
      <pubDate>Wed, 08 Dec 2021 13:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/against-the-idea-of-implementation-in-planning/</guid>
      <description>&lt;p&gt;Why are many plans not implemented? Common explanations are planners have little power,
they fail to account for political or environmental uncertainty in the plans or they failed to
include enough voices during the planning process. The theoretical frameworks on which
we base our understanding of plans focus on implementation as a key evaluative mechanism.
I challenge the premise that plans realise their potential only when they are implemented.
Monitoring implementation of plans presupposes that we know what plans there are to monitor.
Such monitoring privileges published plans and ignores all the other plans that guide urban
development. It assumes that the decision situations in which plans are used are observable.
By jettisoning implementation as a key criterion by which to evaluate the effectiveness of plans, we can begin to focus on the myriad ways in which plan makers and others use plans. We can instead ask, ‘How are these plans used? Who uses them? When are they useful? How to make
useful plans?’ With these questions, we can create different evaluative frameworks for different
types of plans. Some unimplementable plans are worth making.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explaining spatial variations in residential energy usage intensity in Chicago: The role of urban form and geomorphometry</title>
      <link>https://nkaza.github.io/publication/li-2018/</link>
      <pubDate>Thu, 18 Feb 2021 16:49:57 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/li-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Does Subsidy Removal Reduce Coastal Development? Measuring the Effect of US Coastal Barrier Resources Act</title>
      <link>https://nkaza.github.io/publication/branham-2021-aa/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/branham-2021-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One Step Forward, Two Steps Back: Managing Floodplain Development in North Carolina</title>
      <link>https://nkaza.github.io/publication/hino-2021-ab/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/hino-2021-ab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One Step Forward, Two Steps Back: Managing Floodplain Development in North Carolina (Invited)</title>
      <link>https://nkaza.github.io/publication/hino-2021-aa/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/hino-2021-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analysing Free Form Text</title>
      <link>https://nkaza.github.io/post/analysing-free-form-text/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/post/analysing-free-form-text/</guid>
      <description>&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/crosstalk/css/crosstalk.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://nkaza.github.io/post/analysing-free-form-text/index_files/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I am going to introduce basic methods for analysing free form (i.e., unstructured) text. In a different post, we analysed some [free form text that is somewhat semi-structured] (/post/matching-messy-texts/) (e.g., addresses, firm names, etc.). In this post, we will expand on some of those techniques and explicitly focus on quantifying frequencies of words, topics, and sentiments. Free form texts are ubiquitous in the planning field. For example, you may want to understand public sentiment around a proposed project by analysing tweets or public comments. Or you may want to analyse newspaper articles and blogs to understand trending topics. The possibilities are endless.&lt;/p&gt;
&lt;p&gt;However, by its very nature, unstructured text and natural language is very hard to pin down in numbers. Even things that are amenable to quantification (e.g., frequencies of words) lose context–and thus meaning and nuance–when quantified. Qualitative analysis of text is always more meaningful than reductive quantitative analyses. Nonetheless, quantitative approaches are useful for examinining large bodies of text (also referred to as a “corpus” of texts), and they can provide some advantages over qualitative analyses, including replicability and scalability.&lt;/p&gt;
&lt;h2 id=&#34;acquire-data-and-packages&#34;&gt;Acquire Data and Packages&lt;/h2&gt;
&lt;p&gt;This post draws heavily from Will Curran-Groome’s final project for the &lt;a href=&#34;https://nkaza.github.io/teaching/techniques-course/&#34;&gt;Urban Analytics course&lt;/a&gt; in Spring 2020. We are going to analyse emails from a listserv (Cohousing-L) that focuses on cohousing. Cohousing is an intentional community of private homes clustered around shared space with some shared norms about voluntary contributions, management, and governance structures. US cohousing communities often comprise both rental and owner-occupied units; they frequently are multi-generational; they leverage existing legal structures, most often the home owner association (HOA), but the lived experience is often very different from that of conventional HOAs; and they also reflect a diversity of housing types, including apartment buildings, side-by-side duplexes and row homes, and detached single-family units.&lt;/p&gt;
&lt;p&gt;Web-scraped emails (~45,000) and community characteristics are available &lt;a href=&#34;https://www.dropbox.com/sh/q1sp6a8uhremq1m/AAC-J0t48mFeKjiQlzAbeWGPa?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are going to use packages such as &lt;code&gt;tidytext&lt;/code&gt;, &lt;code&gt;textclean&lt;/code&gt;, and &lt;code&gt;sentimentr&lt;/code&gt; in addition to other packages we have used previously. Please install them and call them into your library appropriately.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    I am using the library calls to packages where I need them for pedagogical purposes. In general, you want to put all your library calls at the top of the script. Please pay particular attention to conflicts in the functions of the same name in different packages. Packages that are loaded later will take precedence over ones that are loaded earlier. If you want use the function from an earlier loaded package you can use &lt;code&gt;packagename::function()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ids)
library(lubridate)
library(textclean)

msgs &amp;lt;- here(&amp;quot;tutorials_datasets&amp;quot;, &amp;quot;cohousingemails&amp;quot;, &amp;quot;cohousing_emails.csv&amp;quot;) %&amp;gt;% read_csv()

str(msgs)
# spec_tbl_df [45,000 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ subject : chr [1:45000] &amp;quot;Test message 10/22/92 6:37 pm&amp;quot; &amp;quot;test message 10/22/92 6:47 pm&amp;quot; &amp;quot;Places to announce COHOUSING-L&amp;quot; &amp;quot;Discussion style&amp;quot; ...
#  $ author  : chr [1:45000] &amp;quot;Fred H Olson -- WB0YQM&amp;quot; &amp;quot;STAN&amp;quot; &amp;quot;Fred H Olson -- WB0YQM&amp;quot; &amp;quot;Fred H Olson -- WB0YQM&amp;quot; ...
#  $ email   : chr [1:45000] NA NA NA NA ...
#  $ date    : POSIXct[1:45000], format: NA NA ...
#  $ msg_body: chr [1:45000] &amp;quot;This is a test message. Fred&amp;quot; &amp;quot;Thes message was set to the list with the address in all caps: COHOUSING-L [at] UCI.COM Fred&amp;quot; &amp;quot;Judy, below is my list of places to announce the list including the Inovative Housing newsletter. Can you dig u&amp;quot;| __truncated__ &amp;quot;Another topic is what tone, style or whatever should we encourage on this list. Some discussions people introdu&amp;quot;| __truncated__ ...
#  $ thread  : chr [1:45000] NA NA NA NA ...
#  $ content : chr [1:45000] &amp;quot;\n\n\n\n\n\n\n\n\nTest message 10/22/92 6:37 pm\n\t  &amp;lt;– Date –&amp;gt;    &amp;lt;– Thread –&amp;gt;\n\n\tFrom: Fred H Olson -- WB0Y&amp;quot;| __truncated__ &amp;quot;\n\n\n\n\n\n\n\n\ntest message 10/22/92 6:47 pm\n\t  &amp;lt;– Date –&amp;gt;    &amp;lt;– Thread –&amp;gt;\n\n\tFrom: STAN (STAN%MNHEPvx.c&amp;quot;| __truncated__ &amp;quot;\n\n\n\n\n\n\n\n\nPlaces to announce COHOUSING-L\n\t  &amp;lt;– Date –&amp;gt;    &amp;lt;– Thread –&amp;gt;\n\n\tFrom: Fred H Olson -- WB0&amp;quot;| __truncated__ &amp;quot;\n\n\n\n\n\n\n\n\nDiscussion style\n\t  &amp;lt;– Date –&amp;gt;    &amp;lt;– Thread –&amp;gt;\n\n\tFrom: Fred H Olson -- WB0YQM (FRED%JWHv&amp;quot;| __truncated__ ...
#  - attr(*, &amp;quot;spec&amp;quot;)=
#   .. cols(
#   ..   subject = col_character(),
#   ..   author = col_character(),
#   ..   email = col_character(),
#   ..   date = col_datetime(format = &amp;quot;&amp;quot;),
#   ..   msg_body = col_character(),
#   ..   thread = col_character(),
#   ..   content = col_character()
#   .. )
#  - attr(*, &amp;quot;problems&amp;quot;)=&amp;lt;externalptr&amp;gt;

msgs2 &amp;lt;- msgs %&amp;gt;%
  mutate_all(as.character) %&amp;gt;% ## I (Will) added this because I was getting an error stemming from read_csv() returning all factor variables. 
  filter(!is.na(content)) %&amp;gt;%
  mutate(
    msg_id = random_id(n = nrow(.)), #  Create a random ID
    email = case_when(
      is.na(email) ~ content %&amp;gt;% 
        str_match(&amp;quot;\\(.*\\..*\\)&amp;quot;) %&amp;gt;%
        str_sub(2,-2),
      T ~ email
    ),
    content = content %&amp;gt;%
      str_replace_all(&amp;quot;\\\n&amp;quot;, &amp;quot; &amp;quot;) %&amp;gt;%
      str_squish(),

# Note that this section takes a long time; I recommend patience. It may make sense to save intermittent steps
# instead of sequencing a long chain of pipes.

    msg_body = msg_body %&amp;gt;% 
                stringi::stri_trans_general(&amp;quot;Latin-ASCII&amp;quot;) %&amp;gt;%
                replace_html() %&amp;gt;%
                replace_emoticon() %&amp;gt;%
                replace_time(replacement = &#39;&amp;lt;&amp;lt;TIME&amp;gt;&amp;gt;&#39;) %&amp;gt;%
                replace_number(remove = TRUE) %&amp;gt;%
                replace_url() %&amp;gt;%
                replace_tag() %&amp;gt;%
                replace_email(),
    
    date = as.POSIXct(date),
    
    date = case_when(
      is.na(date) ~ str_match(
        content,
        &amp;quot;[0-9]{1,2} [A-Za-z]{3} [0-9]{2,4} [0-9]{2}:[0-9]{2}&amp;quot;
      ) %&amp;gt;%
      lubridate::dmy_hm(),
      T ~ date
    ),
    author = author %&amp;gt;% tolower %&amp;gt;% str_replace_all(&amp;quot;[^a-z]&amp;quot;, &amp;quot; &amp;quot;),
    email = email %&amp;gt;% tolower %&amp;gt;% str_replace_all(&amp;quot;[^a-z\\.@_\\d ]&amp;quot;, &amp;quot;&amp;quot;)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;a-digression-into-regular-expressions&#34;&gt;A Digression into Regular Expressions&lt;/h2&gt;
&lt;p&gt;In the above code, you see a number of regular expressions (also referred to as regex) that are used to find and manipulate particular sequences of characters. Both the &lt;code&gt;textclean&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; packages provide functions (e.g., replace_emoticon and str_match, respectively) that leverage regular expressions. Regular expressions provide a very powerful and concise syntax for working with text, but they can be very difficult to read, and if you’re not careful, they can return unintended results. Use them sparingly and add comments for clarity.&lt;/p&gt;
&lt;p&gt;At their core, regular expressions match patterns in text. A pattern can be as simple as “abc,” or can be significantly more complicated. For a quick introduction to regular expressions in R, read through the corresponding R for Data Science chapter: &lt;a href=&#34;https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the above code, we have used the following regular expressions. Can you tell what pattern each of these regular expressions will match?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;\\(.*\\..*\\)&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;\\\n&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;[0-9]{1,2} [A-Za-z]{3} [0-9]{2,4} [0-9]{2}:[0-9]{2}&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;[^a-zA-Z]&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;[^a-zA-Z\\.@_\\d ]&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hint: see &lt;a href=&#34;https://cheatography.com/davechild/cheat-sheets/regular-expressions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cheatography.com/davechild/cheat-sheets/regular-expressions/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the following &lt;code&gt;string_lowercase&lt;/code&gt;, develop a regular expression that capitalizes the first letter of each sentence of string_lowercase (taken from &lt;a href=&#34;https://en.wikipedia.org/wiki/Regular_expression%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Regular_expression)&lt;/a&gt;. Is there an alternate or better way to do this without using regular expressions?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;string_lowercase &amp;lt;- &amp;quot;a regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that define a search pattern. usually such patterns are used by string-searching algorithms for &#39;find&#39; or &#39;find and replace&#39; operations on strings, or for input validation. it is a technique developed in theoretical computer science and formal language theory. the concept arose in the 1950s when the american mathematician stephen cole kleene formalized the description of a regular language. the concept came into common use with unix text-processing utilities. different syntaxes for writing regular expressions have existed since the 1980s, one being the posix standard and another, widely used, being the perl syntax. regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and awk and in lexical analysis. many programming languages provide regex capabilities either built-in or via libraries.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hint: Read the documentation for &lt;code&gt;str_replace&lt;/code&gt; and &lt;code&gt;str_replace_all&lt;/code&gt;. Explore ?case for case conversion. You may need to use a different regular expression or different approach to capitalize the first sentence.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tokenization-stemming--lemmatization&#34;&gt;Tokenization, Stemming &amp;amp; Lemmatization&lt;/h2&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h3&gt;
&lt;p&gt;Tokenization is the task of chopping up your text into pieces, called tokens; it can also involve throwing away certain characters, such as punctuation. Tokenization is important in that it defines the smallest unit of analysis at which you can examine your text. In the simplest cases, tokens are simply words. However, there are number of rules that you may want to employ that would work in certain instances and would not work in others. For example, &lt;code&gt;aren&#39;t&lt;/code&gt; can be tokenised as &lt;code&gt;are&lt;/code&gt; and &lt;code&gt;n&#39;t&lt;/code&gt; or &lt;code&gt;aren&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt; or as &lt;code&gt;arent&lt;/code&gt;, depending on which rules you want to use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidytext)

data(stop_words)

stop_words
# # A tibble: 1,149 × 2
#    word        lexicon
#    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;  
#  1 a           SMART  
#  2 a&#39;s         SMART  
#  3 able        SMART  
#  4 about       SMART  
#  5 above       SMART  
#  6 according   SMART  
#  7 accordingly SMART  
#  8 across      SMART  
#  9 actually    SMART  
# 10 after       SMART  
# # … with 1,139 more rows

other_stop_words &amp;lt;- tibble( word = 
      c(&amp;quot;cohousing&amp;quot;,
        &amp;quot;mailing&amp;quot;, 
      &amp;quot;list&amp;quot;,
      &amp;quot;unsubscribe&amp;quot;,
      &amp;quot;mailman&amp;quot;, 
      &amp;quot;listinfo&amp;quot;,
      &amp;quot;list&amp;quot;,
      &amp;quot;Âº&amp;quot;,
      &amp;quot;org&amp;quot;,
      &amp;quot;rob&amp;quot;,
      &amp;quot;ann&amp;quot;,
      &amp;quot;sharon&amp;quot;,
      &amp;quot;villines&amp;quot;,
      &amp;quot;sandelin&amp;quot;, 
      &amp;quot;zabaldo&amp;quot;,
      &amp;quot;fholson&amp;quot;),
      
      lexicon = &amp;quot;CUSTOM&amp;quot;)


stop_words &amp;lt;- bind_rows(stop_words, other_stop_words)

body_tokens &amp;lt;- msgs2 %&amp;gt;%
  unnest_tokens(word, msg_body, token=&#39;words&#39;) %&amp;gt;%
  anti_join(stop_words)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other types of tokens include characters, n-grams, sentences, lines, paragraphs, and tweets. Explore these. anti_join is a useful function to only keep words that are not in the stop_words tibble. Notice why we needed to use word as a column name.&lt;/p&gt;
&lt;p&gt;A naive word count and frequency would look as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;top_word_counts &amp;lt;- body_tokens %&amp;gt;%
  filter(
      !str_detect(word,&amp;quot;\\d&amp;quot;),
    !str_detect(word, &amp;quot;_&amp;quot;)
  ) %&amp;gt;%
  group_by(word) %&amp;gt;%
  summarise(count = n()) %&amp;gt;%
  select(word = word, count) %&amp;gt;%
  arrange(desc(count))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the body_tokens above, we tried to remove common words that might skew the frequencies. Looking at the top_word_counts, iteratively build a list to remove more words to make the analysis more compelling and interesting.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;stemming&#34;&gt;Stemming&lt;/h3&gt;
&lt;p&gt;Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. There are different algorithms that can be used in the stemming process, but the most common in English is the Porter stemmer. The purpose of stemming is to reduce similar words to their shared root; e.g., “talking,” “talked,” and “talks” might all be reduced to “talk.” We can use the &lt;code&gt;SnowballC&lt;/code&gt; package to stem our tokens.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(SnowballC)

top_stem_counts &amp;lt;- body_tokens %&amp;gt;%
                      select(word)%&amp;gt;%
                      mutate(stem_word = wordStem(word)) %&amp;gt;%
                       group_by(stem_word) %&amp;gt;%
                       summarise(count = n()) %&amp;gt;%
                       select(word = stem_word, count) %&amp;gt;%
                       arrange(desc(count))

top_stem_counts
# # A tibble: 73,015 × 2
#    word     count
#    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
#  1 commun  104195
#  2 time     50521
#  3 peopl    50314
#  4 skeptic  47595
#  5 hous     42853
#  6 stick    37263
#  7 tongu    36576
#  8 common   27568
#  9 live     26204
# 10 info     24591
# # … with 73,005 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;lemmatization&#34;&gt;Lemmatization&lt;/h3&gt;
&lt;p&gt;Lemmatization takes into consideration the morphological analysis of the words. For example, “ran” and “run” are derived from the same lemma. Lemmatization requires a language-specific dictonary for translating words to their lemmas; we will use a dictionary provided in the &lt;code&gt;textstem&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(textstem)

top_lemm_counts &amp;lt;- body_tokens %&amp;gt;%
                      select(word)%&amp;gt;%
                       mutate(lemm_word = lemmatize_words(word))%&amp;gt;%
                       group_by(lemm_word) %&amp;gt;%
                       summarise(count = n()) %&amp;gt;%
                       select(word = lemm_word, count) %&amp;gt;%
                       arrange(desc(count))

g1 &amp;lt;- top_lemm_counts %&amp;gt;%
        top_n(30) %&amp;gt;%
        ggplot() + 
        geom_bar(aes(x=  reorder(word, count), y = count), stat = &#39;identity&#39;) +
        coord_flip() + 
        xlab(&amp;quot;&amp;quot;)+
        theme_bw()

library(plotly)

ggplotly(g1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;orientation&#34;:&#34;h&#34;,&#34;width&#34;:[0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.9,0.9,0.9],&#34;base&#34;:[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],&#34;x&#34;:[98528,50346,50314,47557,42836,37684,36576,27664,27491,24747,24590,23893,22425,21361,19070,17014,15791,15768,14937,13695,13424,13422,13102,13102,13024,12750,12677,12499,12061,11722],&#34;y&#34;:[30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,7,8,6,5,4,3,2,1],&#34;text&#34;:[&#34;reorder(word, count): community&lt;br /&gt;count: 98528&#34;,&#34;reorder(word, count): time&lt;br /&gt;count: 50346&#34;,&#34;reorder(word, count): people&lt;br /&gt;count: 50314&#34;,&#34;reorder(word, count): skeptical&lt;br /&gt;count: 47557&#34;,&#34;reorder(word, count): house&lt;br /&gt;count: 42836&#34;,&#34;reorder(word, count): stick&lt;br /&gt;count: 37684&#34;,&#34;reorder(word, count): tongue&lt;br /&gt;count: 36576&#34;,&#34;reorder(word, count): write&lt;br /&gt;count: 27664&#34;,&#34;reorder(word, count): common&lt;br /&gt;count: 27491&#34;,&#34;reorder(word, count): build&lt;br /&gt;count: 24747&#34;,&#34;reorder(word, count): info&lt;br /&gt;count: 24590&#34;,&#34;reorder(word, count): live&lt;br /&gt;count: 23893&#34;,&#34;reorder(word, count): home&lt;br /&gt;count: 22425&#34;,&#34;reorder(word, count): unit&lt;br /&gt;count: 21361&#34;,&#34;reorder(word, count): message&lt;br /&gt;count: 19070&#34;,&#34;reorder(word, count): cohousing.org&lt;br /&gt;count: 17014&#34;,&#34;reorder(word, count): share&lt;br /&gt;count: 15791&#34;,&#34;reorder(word, count): lot&lt;br /&gt;count: 15768&#34;,&#34;reorder(word, count): issue&lt;br /&gt;count: 14937&#34;,&#34;reorder(word, count): village&lt;br /&gt;count: 13695&#34;,&#34;reorder(word, count): move&lt;br /&gt;count: 13424&#34;,&#34;reorder(word, count): include&lt;br /&gt;count: 13422&#34;,&#34;reorder(word, count): archive&lt;br /&gt;count: 13102&#34;,&#34;reorder(word, count): process&lt;br /&gt;count: 13102&#34;,&#34;reorder(word, count): meet&lt;br /&gt;count: 13024&#34;,&#34;reorder(word, count): decision&lt;br /&gt;count: 12750&#34;,&#34;reorder(word, count): cost&lt;br /&gt;count: 12677&#34;,&#34;reorder(word, count): project&lt;br /&gt;count: 12499&#34;,&#34;reorder(word, count): plan&lt;br /&gt;count: 12061&#34;,&#34;reorder(word, count): design&lt;br /&gt;count: 11722&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;textposition&#34;:&#34;none&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(89,89,89,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:26.2283105022831,&#34;r&#34;:7.30593607305936,&#34;b&#34;:40.1826484018265,&#34;l&#34;:86.9406392694064},&#34;plot_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;paper_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-4926.4,103454.4],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;0&#34;,&#34;25000&#34;,&#34;50000&#34;,&#34;75000&#34;,&#34;100000&#34;],&#34;tickvals&#34;:[0,25000,50000,75000,100000],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;0&#34;,&#34;25000&#34;,&#34;50000&#34;,&#34;75000&#34;,&#34;100000&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;count&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[0.4,30.6],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;design&#34;,&#34;plan&#34;,&#34;project&#34;,&#34;cost&#34;,&#34;decision&#34;,&#34;meet&#34;,&#34;archive&#34;,&#34;process&#34;,&#34;include&#34;,&#34;move&#34;,&#34;village&#34;,&#34;issue&#34;,&#34;lot&#34;,&#34;share&#34;,&#34;cohousing.org&#34;,&#34;message&#34;,&#34;unit&#34;,&#34;home&#34;,&#34;live&#34;,&#34;info&#34;,&#34;build&#34;,&#34;common&#34;,&#34;write&#34;,&#34;tongue&#34;,&#34;stick&#34;,&#34;house&#34;,&#34;skeptical&#34;,&#34;people&#34;,&#34;time&#34;,&#34;community&#34;],&#34;tickvals&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;design&#34;,&#34;plan&#34;,&#34;project&#34;,&#34;cost&#34;,&#34;decision&#34;,&#34;meet&#34;,&#34;archive&#34;,&#34;process&#34;,&#34;include&#34;,&#34;move&#34;,&#34;village&#34;,&#34;issue&#34;,&#34;lot&#34;,&#34;share&#34;,&#34;cohousing.org&#34;,&#34;message&#34;,&#34;unit&#34;,&#34;home&#34;,&#34;live&#34;,&#34;info&#34;,&#34;build&#34;,&#34;common&#34;,&#34;write&#34;,&#34;tongue&#34;,&#34;stick&#34;,&#34;house&#34;,&#34;skeptical&#34;,&#34;people&#34;,&#34;time&#34;,&#34;community&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:&#34;transparent&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;width&#34;:0.66417600664176,&#34;linetype&#34;:&#34;solid&#34;},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;bordercolor&#34;:&#34;transparent&#34;,&#34;borderwidth&#34;:1.88976377952756,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;modeBarButtonsToAdd&#34;:[&#34;hoverclosest&#34;,&#34;hovercompare&#34;],&#34;showSendToCloud&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;58e11982ce77&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;type&#34;:&#34;bar&#34;}},&#34;cur_data&#34;:&#34;58e11982ce77&#34;,&#34;visdat&#34;:{&#34;58e11982ce77&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    What’s going on with stick and tongue? Is this an issue with replacing the emoticons?
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;n-grams&#34;&gt;N-grams&lt;/h3&gt;
&lt;p&gt;Looking at words in isolation has all sorts of problems. For example, ‘not happy’ refers to a single concept than two different concepts of negation and happiness. This will affect sentiment analysis later on. To deal with this issue, we could potentially use n-grams. An n-gram reflects a token of n sequenced units. By looking at multiple units of text as a single token, we can overcome some of the challenges of looking at single words in isolation from their context. Here, we construct bigrams, or two-word n-grams.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bigrams &amp;lt;- msgs2 %&amp;gt;%
  select(-thread) %&amp;gt;%
  unnest_tokens(
    bigram,
    msg_body,
    token = &amp;quot;ngrams&amp;quot;,
    n = 2
  )

negation_words &amp;lt;- c(
  &amp;quot;not&amp;quot;,
  &amp;quot;no&amp;quot;,
  &amp;quot;never&amp;quot;, 
  &amp;quot;without&amp;quot;,
  &amp;quot;don&#39;t&amp;quot;,
  &amp;quot;cannot&amp;quot;,
  &amp;quot;can&#39;t&amp;quot;,
  &amp;quot;isn&#39;t&amp;quot;,
  &amp;quot;wasn&#39;t&amp;quot;,
  &amp;quot;hadn&#39;t&amp;quot;,
  &amp;quot;couldn&#39;t&amp;quot;,
  &amp;quot;wouldn&#39;t&amp;quot;,
  &amp;quot;won&#39;t&amp;quot;
)

modified_stops &amp;lt;- stop_words %&amp;gt;%
  filter(!(word %in% negation_words))

refined_bigrams &amp;lt;- bigrams %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;)) %&amp;gt;%
  filter(
    !word1 %in% modified_stops$word,
    !word2 %in% modified_stops$word
  ) %&amp;gt;%
  mutate(lemm_word1 = lemmatize_words(word1),
         lemm_word2 = lemmatize_words(word2))

refined_bigrams &amp;lt;- refined_bigrams %&amp;gt;%
  count(lemm_word1, lemm_word2, sort = T) %&amp;gt;%
  unite(bigram, lemm_word1, lemm_word2, sep = &amp;quot; &amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualise the top 20 bigrams using ggplot.&lt;/li&gt;
&lt;li&gt;Wordclouds are bad statistical graphics. However, they are popular. Create a Wordcloud for these bigrams using &lt;code&gt;wordcloud2&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;There is no reason to think that bigrams are the right tokens. You can use any combination of words. But you should be cognizant about the the trade offs between increasing the n and the marginal value to the analysis. As you can see, bigrams are a substantially larger dataset than words. For a vocabulary of 1,000 terms, the universe of potential bigrams are 1,000,000, though you will see much smaller dataset in practice because of linguistic conventions.&lt;/p&gt;
&lt;h2 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is the interpretation and classification of emotions (positive, negative, and neutral) within text data. It is notoriously unreliable without proper understanding of the context and linguistic patterns such as sarcasm, subtweeting, etc.&lt;/p&gt;
&lt;p&gt;There are a number of dictionaries that exist for evaluating opinion or emotion in text. The &lt;code&gt;tidytext&lt;/code&gt; package provides access to several sentiment lexicons. Three general-purpose lexicons are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AFINN from Finn Årup Nielsen,&lt;/li&gt;
&lt;li&gt;bing from Bing Liu and collaborators, and&lt;/li&gt;
&lt;li&gt;nrc from Saif Mohammad and Peter Turney.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All three of these lexicons are based on unigrams, i.e., tokens of single words. These lexicons contain many English words, and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, and sadness. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. There are also specialized lexicons, such as the loughran lexicon, which is designed for analysis of financial documents. This lexicon labels words with six possible sentiments important in financial contexts: “negative,” “positive,” “litigious,” “uncertainty,” “constraining,” or “superfluous.”&lt;/p&gt;
&lt;p&gt;You can access these lexicons via:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(textdata)
nrc_sentiment &amp;lt;- get_sentiments(&#39;nrc&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can estimate the sentiment of each message by looking at the frequency of words with a particular sentiment using the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;body_tokens %&amp;gt;%
      select(word, msg_id) %&amp;gt;%
      mutate(lemm_word = lemmatize_words(word)) %&amp;gt;%
     inner_join(nrc_sentiment, by=c(&#39;lemm_word&#39; = &#39;word&#39;)) %&amp;gt;%
     group_by(msg_id, sentiment) %&amp;gt;%
     summarize(count = n()) %&amp;gt;%
     mutate(freq = count/sum(count)) %&amp;gt;%
     pivot_wider(id_cols = msg_id, values_from=freq, names_from=sentiment, values_fill = 0) %&amp;gt;%
    top_n(5) 
# # A tibble: 44,609 × 11
# # Groups:   msg_id [44,609]
#    msg_id      anger anticipation disgust  fear negative positive sadness  trust
#    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#  1 0000c5777… 0.0870       0.217   0.0435 0.130   0.217     0.174  0.0870 0.0435
#  2 000169c3f… 0            0.143   0      0       0         0.429  0      0.143 
#  3 0001b31d1… 0            0.15    0      0       0.05      0.5    0      0.2   
#  4 000364e2c… 0.02         0.16    0.01   0.02    0.1       0.34   0.08   0.15  
#  5 0004d9c5f… 0            0.2     0      0       0.2       0.4    0      0.1   
#  6 000551075… 0.02         0.24    0      0       0.08      0.34   0.02   0.1   
#  7 0007194c3… 0            0.2     0      0.133   0.0667    0.267  0      0.133 
#  8 00087d159… 0.0819       0.111   0.0234 0.111   0.129     0.240  0.0292 0.158 
#  9 0009e398e… 0.130        0.0926  0.0556 0.148   0.130     0.231  0.0648 0.0648
# 10 000a0aa90… 0            0.08    0.12   0       0.2       0.24   0.12   0.12  
# # … with 44,599 more rows, and 2 more variables: joy &amp;lt;dbl&amp;gt;, surprise &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the problems with the above approach?
&lt;ul&gt;
&lt;li&gt;Hint 1: There’s a one-to-many relationship in NRC. Does this need fixing?&lt;/li&gt;
&lt;li&gt;Hint 2: We know that the sentiment of a single word is often context-dependent. How can we address this (e.g., by using bigrams)?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;N-grams are better than unigrams in many instances in detecting sentiment, but they have their own challenges. For example, it is not clear what the appropriate value of n is. In such instances, it may be useful to think about the sentiment of the sentence as a whole. Since sentences are denumerably infinite, it is not possible to create a sentiment dictionary for sentences.&lt;/p&gt;
&lt;p&gt;Furthermore, the presence of valence shifters (negation, amplifier, deamplifier etc.) changes the meaning and sentiment of the sentences. In such instances, it may be useful to use a package that can consider the entire sentence rather than combinations of words. &lt;code&gt;sentimentr&lt;/code&gt; is one such package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sentimentr)

msgs2 %&amp;gt;% 
  top_n(50) %&amp;gt;% 
  get_sentences() %&amp;gt;%
  sentiment_by(by=c(&#39;date&#39;, &#39;author&#39;)) %&amp;gt;% 
  top_n(10)
#                    date            author word_count   sd ave_sentiment
#  1: 1997-12-16 14:15:00      paul viscuso        164 0.37          0.26
#  2: 2001-10-02 04:00:00    molly williams        724 0.33          0.28
#  3: 2006-07-12 04:00:00     martin sheehy        372 0.32          0.41
#  4: 2008-02-22 05:00:00     craig ragland        252 0.23          0.28
#  5: 2008-04-11 04:00:00      steven hecht        435 0.35          0.55
#  6: 2011-04-13 04:00:00     craig ragland        305 0.27          0.40
#  7: 2012-09-21 04:00:00    jerry mcintire        193 0.23          0.26
#  8: 2014-07-14 04:00:00 fred list manager        200 0.34          0.24
#  9: 2015-05-05 04:00:00       allison tom        315 0.27          0.32
# 10: 2015-08-26 04:00:00             diane        705 0.21          0.24
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a different package, such as &lt;code&gt;syuzhet&lt;/code&gt; instead of &lt;code&gt;sentimentr&lt;/code&gt;. What are the similarities and differences?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;This is but a scratch in the vast field of text mining and natural language processing. As you may have noticed, much of this analysis is domain specific and, more importantly, language specific. While some principles are transferable, it is always a good idea to learn about a domain prior to devising an analytical strategy.&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Silge, Julia, and David Robinson. 2020. Text Mining with R. Sebastapol, CA: O’ Reilly. &lt;a href=&#34;https://www.tidytextmining.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tidytextmining.com/&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Extracting Data from OpenStreetMap</title>
      <link>https://nkaza.github.io/post/extracting-data-from-osm/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/post/extracting-data-from-osm/</guid>
      <description>&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://nkaza.github.io/post/extracting-data-from-osm/index_files/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This tutorial covers obtaining free OpenStreetMap (OSM) data. &lt;em&gt;OpenstreetMap (OSM)&lt;/em&gt; is a free and open map of the world created largely by voluntary contribution of millions of people around the world. Much like Wikipedia. Since the data is free and open, there are much less restrictions to obtaining and using the data. The only condition of using OSM data is proper attribution to OSM contributors.&lt;/p&gt;
&lt;h2 id=&#34;comparison-between-osm-and-google&#34;&gt;Comparison between OSM and Google&lt;/h2&gt;
&lt;p&gt;Availability of data is reasonably good in both Goolge and OSM in most parts of the world. However, the availability of Google data is better in places where there is more commercial interest and that in OSM where there is more humanitarian interst. You can use &lt;a href = &#34;https://tools.geofabrik.de/mc/&#34;&gt;Map Compare&lt;/a&gt; to compare OSM and Google in particular locations.&lt;/p&gt;
&lt;p&gt;OSM data is free to download. Overpass can be used free of cost to download small amount of data. For large datasets, use &lt;a href = &#34;https://download.geofabrik.de/&#34;&gt;Geofabrik&lt;/a&gt;. Google requires users to pay based on volume of data served after a limited daily quota. Policies of Google change frequently, so note that your code will eventually and frequently break.&lt;/p&gt;
&lt;h2 id=&#34;downloading-data&#34;&gt;Downloading data&lt;/h2&gt;
&lt;p&gt;OSM serves two APIs, namely Main API for editing OSM, and Overpass API for providing OSM data. We will use Overpass API to gather data in this tutorial just like in the &lt;a href=&#34;https://nkaza.github.io/post/intersection-density-from-osm-using-qgis-r/&#34;&gt;tutorial about OSM and QGIS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data can be queried for download using a combination of search criteria lke location and type of objects. It helps to understand how OSM data is structured. OSM data is stored as a list of attributes tagged in key - value pairs of geospatial objects (points, lines or polygons). For example, for an architect’s office, the key is “office”, and the value is “architect.” For the name of the office, key is “name” and value is “ABC Design Studio.” Access an extensive list of key-value pairs through &lt;a href=&#34;https://wiki.openstreetmap.org/wiki/Map_Features&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OSM Wiki&lt;/a&gt; Map features.&lt;/p&gt;
&lt;h2 id=&#34;obtaining-point-locations-of-restaurants-in-durham-from-osm&#34;&gt;Obtaining point locations of restaurants in Durham from OSM&lt;/h2&gt;
&lt;p&gt;Restaurants are tagged under amenities. Amenities, according to OSM Wiki are facilities used by visitors and residents. Here, ‘key’ is “amenity” and ‘value’ is “restaurant.” Do not forget to look for related amenities such as “pub”, “food court”, “cafe”, “fast food”, etc. Other amenities include: “university”, “music school”, “kindergarten” and the likes in education, “bus station”, “fuel”, “parking” and others in transportation, and much more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(osmdata)
library(sf)
library(tidyverse)
library(leaflet)
library(widgetframe)

data_from_osm_df &amp;lt;- opq (getbb (&amp;quot;Durham, North carolina&amp;quot;)) %&amp;gt;% #gets bounding box
  add_osm_feature(key = &amp;quot;amenity&amp;quot;, value = &amp;quot;restaurant&amp;quot;) %&amp;gt;% #searches for restaurants
  osmdata_sf() #download OSM data as sf

#select name and geometry from point data for restaurants
cafe_osm &amp;lt;- data_from_osm_df$osm_points %&amp;gt;% #select point data from downloaded OSM data
  select(name, geometry) #for now just selecting the name and geometry to plot

#create a plot in leaflet
m1 &amp;lt;-
leaflet() %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;%
  addCircles(data = cafe_osm)

frameWidget(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-1.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;It is helpful to learn about the distinctions between different tags (key-value pairs). For example, the key “landuse” is used to describe the purpose for which an area is being used. Examples of values for the key “landuse” are “commercial”, “retail”, “vineyard”, “cemetery”, “religious”, etc. Landuse tags are more generic than amenities and are only used for area objects while amenities can also be used for point objects. In case of any confusion, refer to &lt;a href = &#34; https://wiki.openstreetmap.org/wiki/Map_Features&#34;&gt;Map features&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Various amenities, land-use, roads (e.g. key=“highway”, value = “primary”, “service”, “footway”), natural land features (key=“natural”, value = “grassland”), settlements (key = “place”, value = “suburb”), power (key=“power”, value=“line”, “pole”, “transformer”), etc. may be useful in planning applications.
&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparing-with-google-places&#34;&gt;Comparing with Google Places&lt;/h2&gt;
&lt;p&gt;It is useful to compare the output of OSM to Google Places. I am going to use &lt;code&gt;googleway&lt;/code&gt; package for this analysis.&lt;/p&gt;
&lt;div class=&#34;alert alert-Note&#34;&gt;
  &lt;div&gt;
    To make this portion of the code work, you will need an API key from Google. Instructions to get and set an API key are located here.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note the use of loops to get the next page.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleway)

str &amp;lt;- &amp;quot;restaurants in Durham, NC&amp;quot;  # Construct a search string

res &amp;lt;- google_places(search_string = str, key = YOUR_API_KEY)  #Query google servers. Do not forget to set your Google API key 

str(res)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ html_attributions: list()
##  $ next_page_token  : chr &amp;quot;AfLeUgMGT-mwxcivbE0RqYwAt129UKfk0t72RjKr4882rjVi4LsM5k4ApcVC0veEtDM2AaDtIXm_AwTDvhyNuR7dMA80Cjxs9j18FFjUQOLfgBz&amp;quot;| __truncated__
##  $ results          :&#39;data.frame&#39;:   20 obs. of  16 variables:
##   ..$ business_status      : chr [1:20] &amp;quot;OPERATIONAL&amp;quot; &amp;quot;OPERATIONAL&amp;quot; &amp;quot;OPERATIONAL&amp;quot; &amp;quot;OPERATIONAL&amp;quot; ...
##   ..$ formatted_address    : chr [1:20] &amp;quot;737 9th St #210, Durham, NC 27705, United States&amp;quot; &amp;quot;8128 Renaissance Pkwy #114, Durham, NC 27713, United States&amp;quot; &amp;quot;315 E Chapel Hill St, Durham, NC 27701, United States&amp;quot; &amp;quot;1200 W Chapel Hill St, Durham, NC 27701, United States&amp;quot; ...
##   ..$ geometry             :&#39;data.frame&#39;:    20 obs. of  2 variables:
##   .. ..$ location:&#39;data.frame&#39;:  20 obs. of  2 variables:
##   .. .. ..$ lat: num [1:20] 36 35.9 36 36 36 ...
##   .. .. ..$ lng: num [1:20] -78.9 -79 -78.9 -78.9 -78.9 ...
##   .. ..$ viewport:&#39;data.frame&#39;:  20 obs. of  2 variables:
##   .. .. ..$ northeast:&#39;data.frame&#39;:  20 obs. of  2 variables:
##   .. .. .. ..$ lat: num [1:20] 36 35.9 36 36 36 ...
##   .. .. .. ..$ lng: num [1:20] -78.9 -79 -78.9 -78.9 -78.9 ...
##   .. .. ..$ southwest:&#39;data.frame&#39;:  20 obs. of  2 variables:
##   .. .. .. ..$ lat: num [1:20] 36 35.9 36 36 36 ...
##   .. .. .. ..$ lng: num [1:20] -78.9 -79 -78.9 -78.9 -78.9 ...
##   ..$ icon                 : chr [1:20] &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/restaurant-71.png&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/restaurant-71.png&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/restaurant-71.png&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/restaurant-71.png&amp;quot; ...
##   ..$ icon_background_color: chr [1:20] &amp;quot;#FF9E67&amp;quot; &amp;quot;#FF9E67&amp;quot; &amp;quot;#FF9E67&amp;quot; &amp;quot;#FF9E67&amp;quot; ...
##   ..$ icon_mask_base_uri   : chr [1:20] &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v2/restaurant_pinlet&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v2/restaurant_pinlet&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v2/restaurant_pinlet&amp;quot; &amp;quot;https://maps.gstatic.com/mapfiles/place_api/icons/v2/restaurant_pinlet&amp;quot; ...
##   ..$ name                 : chr [1:20] &amp;quot;Juju Durham&amp;quot; &amp;quot;Harvest 18 Restaurant&amp;quot; &amp;quot;The Restaurant at The Durham&amp;quot; &amp;quot;GRUB Durham&amp;quot; ...
##   ..$ opening_hours        :&#39;data.frame&#39;:    20 obs. of  1 variable:
##   .. ..$ open_now: logi [1:20] FALSE FALSE FALSE TRUE FALSE FALSE ...
##   ..$ photos               :List of 20
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 1365
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/101715461096256363950\&amp;quot;&amp;gt;Juju Asian Tapas + Bar&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgNgzxuFSzGqNULuskoX1J1eSYN1MoW3YEQ9c7PkXcg4qWXatHFQ4Rb5cwIQr7l-ga-4lijhr69kK6U63LVpoR7ba-XZ9zzumHPUSoZkE4V&amp;quot;| __truncated__
##   .. .. ..$ width            : int 2048
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 2340
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/114770559454901883104\&amp;quot;&amp;gt;Mike Little&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPPIA99W25syCwvvcQYxFUrut0bFaPBZl5jprnhTIRyGdcr0QDfzDOikz4Hp1Cjhg7k7E4iYQCGFWk2-uZgbns_csK-P3xcrxnZebT2q70&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4160
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/103711833847435042805\&amp;quot;&amp;gt;Erik Newby&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgMcxBgjOJDODXoQaFqHrMF-yY6eEKkRn-LhM076R41KFhumuBtiv8zjuw6yXHBfaAL_APZqjefYigLmDkpicqiayu7HvtmYc7Be8rFFYY3&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/114870547795315215397\&amp;quot;&amp;gt;James Goerke&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPMD9yIH36j5iqlrH_muls2Z3d76MRXuGriZKTcRB45kzgYghXWPiiOqgiss4XovnhvEzrA1SsrkGaL2AmkuYgu5d3wlmKaXTY3p5ZF2pF&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3072
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/106155315629813478210\&amp;quot;&amp;gt;John Vinueza&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPxyiChtj0yxR6NMCSAyHyP1C0v-aw7ROgHBa4XqABXYj2euzeR9vsXSNckahZHOuk42N6U47k5-8NDCzRTqF_14y_jR1Xr8zCnfHDskpe&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4080
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 1280
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/108889804668316321834\&amp;quot;&amp;gt;Local 22 Kitchen And Bar&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPZqAgMiH1Cj_5Ml-pqlh2Bpa7eRf7hWYaC-sjNAW-qyMHYP7KBHOvmHEcOuHALI8EM5zteRzzzbD_2HuVK4rbkhJuPdPEjxhS1aJlw55w&amp;quot;| __truncated__
##   .. .. ..$ width            : int 1920
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 2268
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/116309041934196405059\&amp;quot;&amp;gt;Vishwanath Math&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgNDxdslnjs0TSAfW8xZ2G6yKznysWRwifnJ_M1O-fFl1wO-dxca1PJE50xJkk5Z9oZt42GWr5zmEuSdC218K6_TWf9RkPjQSj6YCzH6tsm&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/108963259167865212631\&amp;quot;&amp;gt;BRIAN MASSENGILL&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgMRLSDmOeN5XmRRLtenMPy7mdfSlyxTX0wgvoI0cshqMcaYS0M3_kNYu5dL_5TIAb7zvhVmgnX27K-u2NdncbLJG5flZFm181NU0a3ZI8p&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/117366860799003538591\&amp;quot;&amp;gt;A Google User&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgP_hRNFFXZGdI75PiKIxqXFXTlY8__rykRZ2aKHSlWuA_DchjC8tU63cdRXxX5ips1oiVWQCriruFHmM-E4tPxiqAPkbrQDce_fTmnqzIe&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/109300032866188423940\&amp;quot;&amp;gt;Zach Brown&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgOICZG0j3Pyh1V3uJ5kUgcFhu46mfHDQMIf-L5r4Ytv-SAm1qGlBt447IByIr57j6szD56-G-nOrs5ZDEsOjHoFdDRfZF6eRcJXtF5-XKk&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 1067
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/118359313798311826267\&amp;quot;&amp;gt;A Google User&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPASPTIaPmGSxahYbOEgFAaqXrAQHPgGzIX7KIkDKv-SfOwbY4bfoRTtQtWoNnnSLiVZa8Qr3Yy31h3Sx3OYBmjpP-HVfHJ24ZPcZS3WYI&amp;quot;| __truncated__
##   .. .. ..$ width            : int 1600
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3072
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/114917363150643087487\&amp;quot;&amp;gt;David Foresman&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgOX4w-TgQ7Gg5xc8iDY4haqOyxq1zvU5b9o2NifriF-Op-xL6HJC9exho2T8vHFG7gUnEWwxOjxfDS5NRqlOlX-rY9VbYsKdbfqojsZL2e&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4080
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/102066483967244766103\&amp;quot;&amp;gt;Marv Baker&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgMEL1DvyfjMEr3FDGmvbs2tG1txo65kjR2e6GSxoTLzHwpJfdB0wzi0kO_5MDOdc2QAF7y7CGppjXZZoshDMiUv0tK3lg8pmaGDKNUcKRG&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/115591303687864814152\&amp;quot;&amp;gt;warada shafi&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPEm3Ny2re6YXG2yFmGZ9-VZtedKU6Pdja1s4WeeTeVYhooI5jOBgqwHbSwXA5HzHNSZ-3jpdt316T1UV3AP0WQItKaRCznhzFSW07u4Nz&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3072
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/117939330467348488790\&amp;quot;&amp;gt;protonoid&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgM-f9F0veS1XgpyjuG-sk9lHguYnn-JzHApG7mHDoUKLIGv-rPaExIxvP9ngcWfdRhpcF1f4CbgqAWhDON_eojLkkMfrn__jsM3Iqm3xgU&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4080
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 1512
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/116117583444734059617\&amp;quot;&amp;gt;Mothers &amp;amp;amp; Sons Trattoria&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgNGmqVkzlzONPHWuqhkBpc9GQRuedQTY7w8lRBBcNrhkK6wjySsTUe4trkk3KPboWvwUpRGdV0KYc3biGRmuUU5TSIz5X9TREJr0Scail5&amp;quot;| __truncated__
##   .. .. ..$ width            : int 2016
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 2721
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/108832116470362120552\&amp;quot;&amp;gt;Kimberly Slentz-Kesler&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgPi22ZdFw8D5q7HgP-evVd5fvUuh_ry12PsACZ5J_7-qLHNEiLeUCuHQpAsM-Vqz1vtSjdpHnOOX0L9bxG9iMZROZGtD6LQK2Q9Sdd6-lE&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 4032
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/100123088912788695638\&amp;quot;&amp;gt;Chris Schwarz&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgNQN_h2cGNl6t3S6GBSgmI7d2CBOBHD8kRDRhBs_iJ8eSNQtf9J1nhGzr33VJhXDMCowe-xhy4-INZqzAQAnBkGiTAo31nQqmb0nnSJeph&amp;quot;| __truncated__
##   .. .. ..$ width            : int 3024
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3264
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/115121000442582864884\&amp;quot;&amp;gt;Rachael Lord&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgP6NoILAb1ve9q3phRmvXuygx6Lno87WupJQq63RxjGl2QvzKWygM-IiyM3_ve-KCx6Bt6RaRSLCcn_iJNa5HvCbPNoUjQaHwpFlIkJytG&amp;quot;| __truncated__
##   .. .. ..$ width            : int 2448
##   .. ..$ :&#39;data.frame&#39;:  1 obs. of  4 variables:
##   .. .. ..$ height           : int 3024
##   .. .. ..$ html_attributions:List of 1
##   .. .. .. ..$ : chr &amp;quot;&amp;lt;a href=\&amp;quot;https://maps.google.com/maps/contrib/114464732617145277475\&amp;quot;&amp;gt;Mauro Jeronimo Mendoza&amp;lt;/a&amp;gt;&amp;quot;
##   .. .. ..$ photo_reference  : chr &amp;quot;AfLeUgMGjK4hHrxBdRwAT6gKHVWZ7wpuFh0TV2hJ80R0JJMBx3ZmkkM6fOHze0Ry7f1tjgxkKzuFufnxrSO9UqgOT0VPmpu6FLAprlIKn4H8fCT&amp;quot;| __truncated__
##   .. .. ..$ width            : int 4032
##   ..$ place_id             : chr [1:20] &amp;quot;ChIJ608CRgfkrIkRzaPpHby3GS4&amp;quot; &amp;quot;ChIJj6foxurorIkRhlH7GB_PMo0&amp;quot; &amp;quot;ChIJNR4KG3LkrIkR2G7VfaFtyWw&amp;quot; &amp;quot;ChIJtQrhphrkrIkRwN9ZgZlJdKU&amp;quot; ...
##   ..$ plus_code            :&#39;data.frame&#39;:    20 obs. of  2 variables:
##   .. ..$ compound_code: chr [1:20] &amp;quot;235H+W2 Durham, North Carolina&amp;quot; &amp;quot;W23W+8Q Durham, North Carolina&amp;quot; &amp;quot;X3WX+VJ Durham, North Carolina&amp;quot; &amp;quot;X3WJ+QW Durham, North Carolina&amp;quot; ...
##   .. ..$ global_code  : chr [1:20] &amp;quot;8783235H+W2&amp;quot; &amp;quot;8773W23W+8Q&amp;quot; &amp;quot;8773X3WX+VJ&amp;quot; &amp;quot;8773X3WJ+QW&amp;quot; ...
##   ..$ price_level          : int [1:20] 3 2 2 2 NA 2 2 3 2 2 ...
##   ..$ rating               : num [1:20] 4.5 4.3 4.3 4.4 4.7 4.5 4.4 4.5 4.4 4.6 ...
##   ..$ reference            : chr [1:20] &amp;quot;ChIJ608CRgfkrIkRzaPpHby3GS4&amp;quot; &amp;quot;ChIJj6foxurorIkRhlH7GB_PMo0&amp;quot; &amp;quot;ChIJNR4KG3LkrIkR2G7VfaFtyWw&amp;quot; &amp;quot;ChIJtQrhphrkrIkRwN9ZgZlJdKU&amp;quot; ...
##   ..$ types                :List of 20
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:5] &amp;quot;bar&amp;quot; &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:5] &amp;quot;restaurant&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:6] &amp;quot;restaurant&amp;quot; &amp;quot;cafe&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:5] &amp;quot;restaurant&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:5] &amp;quot;bar&amp;quot; &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:5] &amp;quot;bar&amp;quot; &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; ...
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   .. ..$ : chr [1:4] &amp;quot;restaurant&amp;quot; &amp;quot;food&amp;quot; &amp;quot;point_of_interest&amp;quot; &amp;quot;establishment&amp;quot;
##   ..$ user_ratings_total   : int [1:20] 941 1053 116 1748 179 797 1095 505 607 240 ...
##  $ status           : chr &amp;quot;OK&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Notice that the result has Status code and next page token. We are going to use them to extract the restaurants.

# Initiatlise some values.
nextpage_yes_no &amp;lt;- !is.null(res$next_page_token)
token &amp;lt;- res$next_page_token
i &amp;lt;- 1
cafe_google_list &amp;lt;- NULL

# extract only when status is OK.

if(res$status == &amp;quot;OK&amp;quot;) {
    cafe_google_list[[i]] &amp;lt;- 
                        cbind (&amp;quot;id&amp;quot; = res$results$id, 
                               &amp;quot;name&amp;quot; = res$results$name,
                               &amp;quot;address&amp;quot; = res$results$formatted_address,
                               &amp;quot;longitude&amp;quot; = res$results$geometry$location$lng,  # Notice that we are going multiple levels down in the data frame.  You should really examine the structure of the res and results to understand what is going on here.
                               &amp;quot;latitude&amp;quot; = res$results$geometry$location$lat,
                               &amp;quot;plus_code&amp;quot; = res$results$plus_code$compound_code,
                               &amp;quot;price_level&amp;quot; = res$results$price_level,
                               &amp;quot;rating&amp;quot; = res$results$rating
                               ) %&amp;gt;% as_tibble()

}


# The loop begins.

while(nextpage_yes_no == TRUE){ #See if the loop will run at least once.
  i &amp;lt;- i+1 #increment i.
  res_next &amp;lt;- google_places(search_string = str,
                          page_token = token,
                          key = YOUR_API_KEY)

  if(res_next$status == &amp;quot;OK&amp;quot;) {
    cafe_google_list[[i]] &amp;lt;- 
                        cbind (&amp;quot;id&amp;quot; = res_next$results$id, 
                               &amp;quot;name&amp;quot; = res_next$results$name,
                               &amp;quot;address&amp;quot; = res_next$results$formatted_address,
                               &amp;quot;longitude&amp;quot; = res_next$results$geometry$location$lng,
                               &amp;quot;latitude&amp;quot; = res_next$results$geometry$location$lat,
                               &amp;quot;plus_code&amp;quot; = res_next$results$plus_code$compound_code,
                               &amp;quot;price_level&amp;quot; = res_next$results$price_level,
                               &amp;quot;rating&amp;quot; = res_next$results$rating
                               ) %&amp;gt;% as_tibble()

}


  token &amp;lt;- res_next$next_page_token # notice the update of the token 
  nextpage_yes_no &amp;lt;- !is.null(res_next$next_page_token) # Notice the update of nextpage_yes_no. If you don&#39;t do it, you can potentially run the loop forever (or at least till the server shuts you down.)
  rm(res_next)  # clean up the temporary objects. Good practise/
  Sys.sleep(5) # Introduce a time delay, so that you do not overwhelm the server.
}  # The loop concludes


# Convert the list to a sf object to visualise
cafe_google &amp;lt;- plyr::compact(cafe_google_list) %&amp;gt;% bind_rows 
cafe_google &amp;lt;- st_as_sf(cafe_google,  coords = c(&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;), crs=4326)

m1 &amp;lt;-
leaflet() %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;%
  addCircles(data = cafe_google)

frameWidget(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;index_files/figure-html//widgets/widget_unnamed-chunk-2.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Note that OSM report 894 entries while Google reports 20 restaurants.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why is there a difference between Google and OSM? (Hint: Read the documentation)&lt;/li&gt;
&lt;li&gt;If there is a marked difference between OSM and Google, what implication does this have for any analysis that you might do using their data?&lt;/li&gt;
&lt;li&gt;Notice that repeated application of the same query produce different results in in the same system. Why? What implication does this have for reproducibility?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The provenance of the data and the continual update of data on servers have serious implications for reproducibility. On the other hand, the updates allow for timeliness of analysis. It is important to recognise these limitations and potential.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Much of the post is written by &lt;a href=&#34;https://planning.unc.edu/student/kshitiz-khanal-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kshitiz Khanal&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Urban Form and Transportation Energy Consumption</title>
      <link>https://nkaza.github.io/talk/urban-form-and-transportation-energy-consumption/</link>
      <pubDate>Thu, 24 Oct 2019 16:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/urban-form-and-transportation-energy-consumption/</guid>
      <description>&lt;p&gt;Transportation energy is a significant portion of the energy consumption of the US economy. While various policies such as changing the fuel mix and alternative fuels are proposed to make the system more efficient, the efficacy of land use policies such as changing the urban form and densification have been subject to considerable debate. In this paper, I use a rich dataset compiled from different sources to test the effectiveness of urban form on energy consumption in the transportation sector. I proxy the consumption with retail sales from gas stations for most of the conterminous United States at a county level. Using both demographic, economic and landscape characteristics of urban form I tease out the effect of different dimensions on energy consumption. I find that compact and contiguous urban form is modestly associated with lower energy consumption and is more important than demographic concentration in explaining the variance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does Removing Federal Subsidies Discourage Development? An Evaluation of the Impact of the U.S. Coastal Barrier Resources Act</title>
      <link>https://nkaza.github.io/talk/does-removing-federal-subsidies-discourage-development-an-evaluation-of-the-impact-of-the-u.s.-coastal-barrier-resources-act/</link>
      <pubDate>Sat, 19 Oct 2019 09:30:00 +0000</pubDate>
      <guid>https://nkaza.github.io/talk/does-removing-federal-subsidies-discourage-development-an-evaluation-of-the-impact-of-the-u.s.-coastal-barrier-resources-act/</guid>
      <description>&lt;p&gt;Urban development relies on many factors to remain viable, including infrastructure, services, and
government provisions and subsidies. However, in situations involving federal or state level policy, development responds not just to one regulatory signal, but also to multiple signals from
overlapping and competing jurisdictions. The 1982 U.S. Coastal Barrier Resources Act (CoBRA)
offers an opportunity to study when and how development restrictions and economic disincentives
protect natural resources by stopping or slowing urban development in management regimes with
distributed authority and responsibility. CoBRA prohibits federal financial assistance for
infrastructure, post-storm disaster relief, and flood insurance in designated sections (system units) of coastal barriers. How has CoBRA’s removal of these subsidies affected rates and types of urban development? Using building footprint and real estate data (n=1,385,552 parcels), we compare
density of built structures, land use types, residential house size, and land values within and outside of system units in eight Southeast and Gulf Coast US states. We show that CoBRA is associated with reduced development rates in coastal barrier zones. We also demonstrate how local responses may counteract withdrawal of federal subsidies. As attention increases towards improving urban resilience in high hazard areas, this work contributes to understanding how limitations on infrastructure and insurance subsidies can affect outcomes under overlapping jurisdictions with competing goals.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does Removing Federal Infrastructure Subsidies Discourage Development</title>
      <link>https://nkaza.github.io/publication/onda-2019-ab/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/onda-2019-ab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of Street Connectivity and Duration of Exposure on Visits to Fast Food Restaurants: A GPS based case study of Atlanta Region</title>
      <link>https://nkaza.github.io/publication/peng-2019-aa/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/peng-2019-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Urban Form and Transportation Energy Consumption</title>
      <link>https://nkaza.github.io/publication/kaza-2019-ac/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2019-ac/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning for Remote Sensing</title>
      <link>https://nkaza.github.io/post/machine-learning-for-remote-sensing/</link>
      <pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/post/machine-learning-for-remote-sensing/</guid>
      <description>
&lt;script src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Machine learning (ML) is currently a buzzword in urban analytics. It is a process of automated model building that generates a predictive model that can reasonably explain not just the data that it is trained on, but generalised to other data from the same data generating process. Traditional models are rules that operate on data to produce and output. Machine learning approaches, on the other hand, usually take outputs and data to figure out the appropriate rules. While traditional models have to rely upon external justification for the rules, the promise of ML is that it discovers these rules empirically, without a theoretical basis for understanding the correlations among the different variables.One important thing to note about machine learning is that the models are restricted to the hypothesis space and the search is not among the arbitrary model specifications. For example, in machine learning, that is about logistic regression model, the features are restricted to enter the model in a linear fashion, where as in a decision tree, they behave non-linearly based on the partition. While this may be too esoteric for students who are starting out on understanding ML techniques, it is useful to temper the expectations regarding what kinds of models can we expect to be generated by the various algorithms. In other words, there is no guarantee that the ML model is the &lt;code&gt;best&lt;/code&gt; model that explains and predicts the observed data. Practical ML is as much an art as it is a science.&lt;/p&gt;
&lt;p&gt;It might be beneficial to illustrate some of the salient points about ML though a practical example that interests planners. Identifying objects and land use classes from remotely sensed images of urban areas.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stages-of-ml-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stages of ML approach&lt;/h2&gt;
&lt;p&gt;There are 5 distinct stages of Machine Learning. Let’s focus on supervised learning, a subset of ML approaches. In supervised learning, target outcome is known for a vector of features and the dataset consists of a collection of the features and target. So for example, land use class is frequently the target (dependent variable) and the features (independent variables) are various bands, indices, textures, proximity etc.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Identifying appropriate data sources, especially labelled data. Wrangle, Clean and Assemble (Data Preprocessing)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Feature Engineering. Identify the right variable combinations from the independent variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Splitting the data into training, validation and holdout.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iterating over the algorithm to fit best explain the training dataset. Use the validation data to tune the model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choosing the best model that does well (prediction) on the holdout dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ML approaches are fundamentally iterative. I cannot emphasise this enough. While there are distinct steps in the approaches, because later stages crucially depend on earlier stages, all stages, except the last one, are iterative. We usually iterate to find better fitting algorithms to the data, which necessitates changes to feature engineering and selection as shown in the figure below.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/1528719478855-image3[4].png&#34; style=&#34;width:100.0%;height:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Image credit: &lt;a href=&#34;https://www.zeolearn.com/magazine/understanding-the-human-process-in-machine-learning&#34;&gt;Goyal (2018)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the following steps, for the sake of brevity, I do not demonstrate the iterative aspects of ML.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-acquisition-and-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Acquisition and Preprocessing&lt;/h2&gt;
&lt;p&gt;For this exercise, I am going to use a 3m, 4-band Planetscope image from around Wuhan, China. You can download it from around &lt;a href=&#34;https://www.dropbox.com/s/thx2gdi7230qme2/data.zip?dl=0&#34;&gt;here&lt;/a&gt;. The 4 bands are Blue, Green, Red and Near Infra Red (NIR). These are initial set of features.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(terra)
library(here)
library(tidyverse)
library(sf)

wuhan_raster &amp;lt;- here(&amp;quot;tutorials_datasets&amp;quot;, &amp;quot;wuhanremotesensing&amp;quot;, &amp;quot;20170914_022008_0f28_3B_AnalyticMS.tif&amp;quot;) %&amp;gt;% rast()
names(wuhan_raster) &amp;lt;- c(&amp;#39;Blue&amp;#39;, &amp;quot;Green&amp;quot;, &amp;quot;Red&amp;quot;, &amp;#39;NIR&amp;#39;)
wuhan_raster
# class       : SpatRaster 
# dimensions  : 4695, 9068, 4  (nrow, ncol, nlyr)
# resolution  : 3, 3  (x, y)
# extent      : 231990, 259194, 3372027, 3386112  (xmin, xmax, ymin, ymax)
# coord. ref. : WGS 84 / UTM zone 50N (EPSG:32650) 
# source      : 20170914_022008_0f28_3B_AnalyticMS.tif 
# names       : Blue, Green, Red, NIR

plotRGB(wuhan_raster, r=3, g=2, b=1, stretch=&amp;#39;hist&amp;#39;, main=&amp;#39;True color composite&amp;#39;) #TRUE colour composite&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotRGB(wuhan_raster, r=4,g=3,b=2, stretch=&amp;#39;hist&amp;#39;, main = &amp;#39;False color composite&amp;#39;) # FALSE colour Composite&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The labels are vector data derived from Openstreetmap data. It is available as part of the zip file you downloaded earlier. In particular, the labels are in the ‘landuse’ class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; library(sf)
 shp &amp;lt;- here(&amp;quot;tutorials_datasets&amp;quot;, &amp;quot;wuhanremotesensing&amp;quot;, &amp;quot;landuse3.shp&amp;quot;) %&amp;gt;% st_read
# Reading layer `landuse3&amp;#39; from data source 
#   `/Users/kaza/Dropbox/website_new/website/tutorials_datasets/wuhanremotesensing/landuse3.shp&amp;#39; 
#   using driver `ESRI Shapefile&amp;#39;
# Simple feature collection with 629 features and 25 fields
# Geometry type: MULTIPOLYGON
# Dimension:     XY
# Bounding box:  xmin: 114.21 ymin: 30.45716 xmax: 114.4891 ymax: 30.57547
# Geodetic CRS:  WGS 84
shp &amp;lt;- st_transform(shp, crs(wuhan_raster)) 

## Note that shp was not in the same projection as raster, so transform it to make the spatial operations possible. In general, it is quicker and easier to transform vectors.

summary(shp$landuse)
#    Length     Class      Mode 
#       629 character character

library(tmap)

tmap_mode(&amp;#39;plot&amp;#39;)

m &amp;lt;- 
shp %&amp;gt;% 
tm_shape+
  tm_fill(col = &amp;quot;landuse&amp;quot;,
          style = &amp;quot;cat&amp;quot;,
          palette = &amp;#39;Dark2&amp;#39;
          ) +
  tm_layout(legend.outside = TRUE)

m&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the sake of simplicity, lets sample 15 locations from each polygon and use that as the basis for our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ptsamp &amp;lt;- shp %&amp;gt;% 
  st_sample(rep(15, nrow(shp)), type = &amp;#39;random&amp;#39;) %&amp;gt;% 
  st_sf() %&amp;gt;%
  st_join(shp, join=st_intersects)


m +
  tm_shape(ptsamp)+
  tm_dots(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will ultimately extract the raster values from the locations of these points to construct the columns in the training data. The expectation is that at each of these points, the underlying rasters have different values for different bands (or other variables) and the key is figure out what combination of of these predictor variables can usefully predict the target variable (in this case, ‘landuse’). But before we do that we need to construct appropriate predictor variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature engineering&lt;/h2&gt;
&lt;p&gt;Feature engineering is careful construction of new variables from raw data. For example, we can construct ‘Age’ from ‘Birth Date’ and ‘CurrentDate’, even when ‘CurrentDate’ is a not explicitly part of the dataset. Or combining two categorical variables into one. In this example, you already have variables ‘Blue’, ‘NIR’, ‘Red’ etc.&lt;/p&gt;
&lt;p&gt;Feature engineering is one of the critical steps in ML approaches and is often overlooked. Because the raw data can be transformed into any number of features, it is critical that we need to draw upon domain knowledge to produce a proper ‘hypothesis space’ to find the ‘best model’.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.” - Andrew Ng&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, it is common practice to construct Normalised Difference Index by doing some band math. One such indices is Normalised Difference Vegetation Index (NDVI) that is based on the ratio of NIR and Red. Normalised Difference Water Index is based on NIR and Green.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
band_math_ratio &amp;lt;- function(x, y){ (x - y ) / (x + y) } 
ndvi &amp;lt;- band_math_ratio(wuhan_raster[[&amp;#39;NIR&amp;#39;]], wuhan_raster[[&amp;#39;Red&amp;#39;]])
names(ndvi) &amp;lt;- &amp;quot;NDVI&amp;quot;
ndwi &amp;lt;- band_math_ratio(wuhan_raster[[&amp;#39;Green&amp;#39;]], wuhan_raster[[&amp;#39;NIR&amp;#39;]])
names(ndwi) &amp;lt;- &amp;quot;NDWI&amp;quot;
          
plot(ndvi, main = &amp;quot;NDVI&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ndwi, main = &amp;quot;NDWI&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Calculate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Visible Atmospherically Resistant Index &lt;span class=&#34;math inline&#34;&gt;\((Green - Red)/ (Green + Red - Blue)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modified Soil Adjusted Vegetation Index (MSAVI2): &lt;span class=&#34;math inline&#34;&gt;\(\frac{(2* NIR+1)-\sqrt{(2*NIR+1)^2-8*(NIR-Red))}}{2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look up the original references for these indices and see if they can really be applied to Planetscopse sensors. What are the limitations of each of these indices including NDVI, NDWI&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot these indices and see if the values visually distinguish different classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;It is often useful to look at correlations within the different bands in the dataset to see if different features are adding much to the information content.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wuhan_raster &amp;lt;- c(wuhan_raster, ndvi, ndwi) # Only possible because the extents, resolution and crs are same.
pairs(wuhan_raster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, Blue and Red are pairwise heavily correlated (linearly) to Green. One way to reduce the dimensions is to extract the principal components of the data that encompasses most of the information. The other is to use either Red or Green. Also notice how NDVI and NDWI are highly correlated. Perhaps you don’t need both. Make appropriate judgements as to what to keep and what to throw out and proceed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;textures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Textures&lt;/h2&gt;
&lt;p&gt;Textures describe the spatial distribution of intensities, which makes it useful in classification of similar regions in different images. Haralick textures are usually from discrete gray level images.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/GLCM.jpg&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Image credit: &lt;a href=&#34;https://doi.org/10.1016/j.cageo.2013.07.006&#34;&gt;Eichkitz et.al (2013)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The main idea is that a gray level image is discretized into n-levels. In a moving window of 3x3 or 5x5, the proportion of co-occurence of two levels is noted in a matrix. From the Gray Level Co-Occurrence Matrix (GLCM), we can derive texture features such as Variance Homogeneity, Dissimilarity etc.&lt;/p&gt;
&lt;p&gt;The following code is not evaluated because it takes a long time, but is here to demonstrate. feel free to experiment with various textures based on gray images of different layers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glcm)
library(raster)
textures &amp;lt;- glcm(raster(wuhan_raster[[&amp;#39;NDVI&amp;#39;]]), shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)))
textures &amp;lt;- textures[[-8]]&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above code calculates isotropic textures (taking the mean of all the directions). However, sometimes it might be better to calculate anisotropic textures for urban orbject detection. See &lt;a href=&#34;http://dx.doi.org/10.1109/JSTARS.2008.2002869&#34;&gt;Pesaresi et.al (2008)&lt;/a&gt;. Calculate the PanTex features from Pesaresi et.al based on maximum, instead of the mean of different directions for this image.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;constructing-the-training-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructing the training dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(wuhan_analysis_raster &amp;lt;- c(wuhan_raster[[c(&amp;quot;Blue&amp;quot;, &amp;quot;Green&amp;quot;, &amp;quot;NDVI&amp;quot;, &amp;quot;NIR&amp;quot;)]], textures))
# class       : SpatRaster 
# dimensions  : 4695, 9068, 11  (nrow, ncol, nlyr)
# resolution  : 3, 3  (x, y)
# extent      : 231990, 259194, 3372027, 3386112  (xmin, xmax, ymin, ymax)
# coord. ref. : WGS 84 / UTM zone 50N (EPSG:32650) 
# sources     : 20170914_022008_0f28_3B_AnalyticMS.tif  (2 layers) 
#               memory  
#               20170914_022008_0f28_3B_AnalyticMS.tif  
#               ... and 1 more source(s)
# names       :        Blue,       Green,        NDVI,         NIR,        mean,    variance, ... 
# min values  :          ? ,          ? , -0.53435452,          ? ,  0.03125000,  0.91175974, ... 
# max values  :          ? ,          ? ,   0.6235793,          ? ,   0.9826389, 936.7556695, ...
raster_sample &amp;lt;- terra::extract(wuhan_analysis_raster, vect(ptsamp))
raster_sample$landuse &amp;lt;- factor(ptsamp$landuse)
raster_sample &amp;lt;- raster_sample[complete.cases(raster_sample),] %&amp;gt;% as_tibble() %&amp;gt;% dplyr::select(-ID) # select every column but ID column


raster_sample
# # A tibble: 10,623 × 12
#     Blue Green     NDVI   NIR   mean variance homogeneity contrast dissimilarity
#    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
#  1  5508  5036  0.0595   4248 0.0764     5.85       0.778    0.444         0.444
#  2  5508  5036  0.0595   4248 0.0764     5.85       0.778    0.444         0.444
#  3  5381  4765  0.181    4957 0.0625     3.75       1        0             0    
#  4  5381  4765  0.181    4957 0.0625     3.75       1        0             0    
#  5  6428  5907  0.0188   5062 0.118     14.3        0.889    0.222         0.222
#  6  6428  5907  0.0188   5062 0.118     14.3        0.889    0.222         0.222
#  7  5970  5277  0.00332  4082 0.0955     9.19       0.944    0.111         0.111
#  8  5970  5277  0.00332  4082 0.0955     9.19       0.944    0.111         0.111
#  9  6135  5422 -0.0239   4199 0.0955     8.44       0.944    0.111         0.111
# 10  6135  5422 -0.0239   4199 0.0955     8.44       0.944    0.111         0.111
# # … with 10,613 more rows, and 3 more variables: entropy &amp;lt;dbl&amp;gt;,
# #   second_moment &amp;lt;dbl&amp;gt;, landuse &amp;lt;fct&amp;gt;


library(skimr)
skim(raster_sample)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;raster_sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10623&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ordered&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;top_counts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;landuse&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;res: 4838, mea: 1609, ind: 840, for: 806&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Blue&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6215.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1099.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4579.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5454.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5968.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6669.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17674.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Green&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5590.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1115.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3945.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4824.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5345.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6033.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17654.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NDVI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.52&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▆▇▆▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NIR&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5099.24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1184.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1950.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4253.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5086.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5896.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12356.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▂▇▂▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.66&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.92&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;413.87&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;homogeneity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▁▁▃▇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;contrast&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dissimilarity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;entropy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.53&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▆▅▇▂▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;second_moment&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.48&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▃▇▃▃▅&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To test the generalisability of the model, we will hold out a portion of the dataset and train the model on the remaining dataset. The following image illustrates this.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/1_4G__SV580CxFj78o9yUXuQ.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Image credit: &lt;a href=&#34;https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6&#34;&gt;Borhnstein (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# create a holdout test set
# use 80% of the original training data for training # use the remaining 20% of the original training data for testing
set.seed(12)
train_index &amp;lt;- createDataPartition(raster_sample$landuse, p=0.80, list=FALSE)
test_dataset &amp;lt;- raster_sample[-train_index,]
train_dataset &amp;lt;- raster_sample[train_index,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use repeated cross validation to fine tune each model. During each iteration, we will shuffle the dataset, so that the model is trained and tested on different datasets.
&lt;img src=&#34;img/1_J2B_bcbd1-s1kpWOu_FZrg.png&#34; alt=&#34;Image credit: Borhnstein (2017)&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fortunately the Caret library has convenience functions that automate this process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;control &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;, repeats =3, classProbs= TRUE, summaryFunction = multiClassSummary)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;build-models-using-different-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build models using different algorithms&lt;/h2&gt;
&lt;p&gt;Let’s build a simple decision tree model and see the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_tree&amp;lt;- train(landuse~., data=train_dataset, method=&amp;quot;rpart&amp;quot;, 
                trControl=control, preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;, &amp;#39;nzv&amp;#39;) )
plot(m_tree$finalModel, uniform=TRUE, main=&amp;quot;Classification Tree&amp;quot;)
text(m_tree$finalModel, cex = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
varImp(m_tree, scale=TRUE) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://nkaza.github.io/post/machine-learning-for-remote-sensing/index_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_hold_tree &amp;lt;- predict.train(m_tree,test_dataset, type=&amp;#39;raw&amp;#39;)
confusionMatrix(pred_hold_tree,test_dataset$landuse)
# Confusion Matrix and Statistics
# 
#                Reference
# Prediction      basin commercial construction forest grass industrial lake
#   basin             0          0            0      0     0          0    0
#   commercial        0          0            0      0     0          0    0
#   construction      0          0            0      0     0          0    0
#   forest            0          7            5     72    22          8    0
#   grass             0          0            0      0     0          0    0
#   industrial        0          0            6      0     0         13    0
#   lake              0          0            0      0     0          0    0
#   meadow            7          2            9     13    29         22    0
#   railway           0          0            0      0     0          0    0
#   residential      14        120          104     76    78        125    9
#   retail            0          0            0      0     0          0    0
#   river             0          0            0      0     0          0    0
#   village_green     0          0            0      0     0          0    0
#                Reference
# Prediction      meadow railway residential retail river village_green
#   basin              0       0           0      0     0             0
#   commercial         0       0           0      0     0             0
#   construction       0       0           0      0     0             0
#   forest            25       0          67      0     0             0
#   grass              0       0           0      0     0             0
#   industrial         1       0           7      1     0             0
#   lake               0       0           0      0     0             0
#   meadow            58       0          44      0     0             1
#   railway            0       0           0      0     0             0
#   residential      237       3         849     76     6             5
#   retail             0       0           0      0     0             0
#   river              0       0           0      0     0             0
#   village_green      0       0           0      0     0             0
# 
# Overall Statistics
#                                           
#                Accuracy : 0.4677          
#                  95% CI : (0.4463, 0.4892)
#     No Information Rate : 0.4559          
#     P-Value [Acc &amp;gt; NIR] : 0.1428          
#                                           
#                   Kappa : 0.131           
#                                           
#  Mcnemar&amp;#39;s Test P-Value : NA              
# 
# Statistics by Class:
# 
#                      Class: basin Class: commercial Class: construction
# Sensitivity              0.000000           0.00000             0.00000
# Specificity              1.000000           1.00000             1.00000
# Pos Pred Value                NaN               NaN                 NaN
# Neg Pred Value           0.990099           0.93918             0.94154
# Prevalence               0.009901           0.06082             0.05846
# Detection Rate           0.000000           0.00000             0.00000
# Detection Prevalence     0.000000           0.00000             0.00000
# Balanced Accuracy        0.500000           0.50000             0.50000
#                      Class: forest Class: grass Class: industrial Class: lake
# Sensitivity                0.44720      0.00000          0.077381    0.000000
# Specificity                0.93163      1.00000          0.992320    1.000000
# Pos Pred Value             0.34951          NaN          0.464286         NaN
# Neg Pred Value             0.95352      0.93918          0.925944    0.995757
# Prevalence                 0.07591      0.06082          0.079208    0.004243
# Detection Rate             0.03395      0.00000          0.006129    0.000000
# Detection Prevalence       0.09712      0.00000          0.013201    0.000000
# Balanced Accuracy          0.68942      0.50000          0.534850    0.500000
#                      Class: meadow Class: railway Class: residential
# Sensitivity                0.18069       0.000000             0.8780
# Specificity                0.92944       1.000000             0.2608
# Pos Pred Value             0.31351            NaN             0.4988
# Neg Pred Value             0.86415       0.998586             0.7184
# Prevalence                 0.15134       0.001414             0.4559
# Detection Rate             0.02735       0.000000             0.4003
# Detection Prevalence       0.08722       0.000000             0.8025
# Balanced Accuracy          0.55506       0.500000             0.5694
#                      Class: retail Class: river Class: village_green
# Sensitivity                 0.0000     0.000000             0.000000
# Specificity                 1.0000     1.000000             1.000000
# Pos Pred Value                 NaN          NaN                  NaN
# Neg Pred Value              0.9637     0.997171             0.997171
# Prevalence                  0.0363     0.002829             0.002829
# Detection Rate              0.0000     0.000000             0.000000
# Detection Prevalence        0.0000     0.000000             0.000000
# Balanced Accuracy           0.5000     0.500000             0.500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model has particularly low accuracy. Nevertheless, it is useful to predict the classes for the whole image and see where the issues might lie.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
wuhan_tree_class &amp;lt;- terra::predict(wuhan_analysis_raster, m_tree, type = &amp;#39;raw&amp;#39;, factors = levels(train_dataset$landuse))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualise this raster with appropriate colors and legend.&lt;/li&gt;
&lt;li&gt;Increase the number of random points in each class and see if your classification is better.&lt;/li&gt;
&lt;li&gt;Change the polygons for the land use and see the impact on the classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;note-on-performance-measures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Note on performance measures&lt;/h2&gt;
&lt;p&gt;Let us consider a binary classification (1, 0 classes) problem, as digression and consider the contingency table and define some terms&lt;/p&gt;
&lt;p&gt;True Positive (TP): When the algorithm results 1, when it should result 1
True Negative (TN): When the algorithm results 0, when it should be 0
False Positive (FP): When the algorithm results 1, when it should be 0
False Negative (FN): When the algorithm results 0, when it should be 1&lt;/p&gt;
&lt;p&gt;Once we define these terms, we can define&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accuracy as &lt;span class=&#34;math inline&#34;&gt;\((TP+TN)/(TP + TN + FP + FN)\)&lt;/span&gt;; Accuracy can be terribly biased if there are large number of Negatives or Positives, i.e if the data is unbalanced&lt;/li&gt;
&lt;li&gt;Precision/Positive Predictive Value as &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP +FP)\)&lt;/span&gt;; What proportion of positive identifications was actually correct?&lt;/li&gt;
&lt;li&gt;Recall/Sensitivity as &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt;; What proportion of actual positives was identified correctly?&lt;/li&gt;
&lt;li&gt;True Negative Rate/ Specificity as &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt;; What proportion of negative identifications are actually correct&lt;/li&gt;
&lt;li&gt;F1-Score as harmonic mean of Precision and Recall.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instead of overall accuracy measure, F1-scores may be a better measure.&lt;/p&gt;
&lt;p&gt;These could be extended to multi-class classifications. Kappa is a measure of agreement above random chance. Though it has been discouraged in recent literature (see &lt;a href=&#34;https://doi.org/10.1016/j.rse.2014.02.015&#34;&gt;Olofsson et.al (2014)&lt;/a&gt;), it is still widely reported.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-among-different-predictive-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing among different predictive algorithms&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(doParallel)
cl &amp;lt;- makeCluster(detectCores(), type=&amp;#39;PSOCK&amp;#39;)
registerDoParallel(cl)

algos &amp;lt;- c(&amp;#39;multinom&amp;#39;, &amp;#39;kknn&amp;#39;, &amp;#39;ranger&amp;#39;, &amp;#39;xgbTree&amp;#39;)

m_algos &amp;lt;- lapply(algos, function(x){train(landuse~., data=train_dataset, method=x, trControl=control, preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;, &amp;#39;nzv&amp;#39;)) })

names(m_algos) &amp;lt;- algos

stopCluster(cl)

# calculate resamples // exclude SIMCA and PLS
resample_results &amp;lt;- resamples(m_algos)
# print results to console
bwplot(resample_results , metric = c(&amp;quot;Kappa&amp;quot;,&amp;quot;Accuracy&amp;quot;))
summary(resample_results,metric = c(&amp;quot;Kappa&amp;quot;,&amp;quot;Accuracy&amp;quot;,&amp;quot;logLoss&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/accuracy_kappa.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite all this effort, the mean accuracy is low. Furthermore, the even the maximum Kappa statistic is less than 20%. In other words, the machine learning algorithms are at best 20% better at predicting the land use than random chance alone. Furthermore, I am checking the accuracy here on the training dataset. This is not typically kosher.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore the tuning parameters for each of the algorithms and try to optimise the performance of the model&lt;/li&gt;
&lt;li&gt;For each of these models, plot and describe the variable importance.&lt;/li&gt;
&lt;li&gt;Pick the best model of the lot and test its performance on the holdout dataset&lt;/li&gt;
&lt;li&gt;Visualise the result of the classification of the entire scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;potential-improvements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Potential Improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Get better training dataset. Reduce the number of classes, by merging similar classes.&lt;/li&gt;
&lt;li&gt;Perform image segmentation to extract objects and then use machine learning algorithms&lt;/li&gt;
&lt;li&gt;Add information from ancilliary datasets (such as distance to roads, railroads etc.)&lt;/li&gt;
&lt;li&gt;Tune the hyperparameters of model. Explore &lt;code&gt;caret&lt;/code&gt; package documentation&lt;/li&gt;
&lt;li&gt;Work on feature engineering more.&lt;/li&gt;
&lt;li&gt;Try hierarchical image classification (impervious/water/barren/park at first level; residential/commercial/industrial within urban etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;In this post, I showed how machine learning can be used to classify remote sensing images. However, these methods are more general than satellite image applications. We can use these methods to predict time series data, classify textual informtion, identify sentiments in tweets and complaints and in general find patterns in data. While ML approaches are powerful, they are not always the most useful (as this post has shown) nor can they be a substitute for careful analysis, problem framing, data assembly, feature engineering, label data construction etc. Another big critique of the ML approaches are that most of them do not give us an understanding of the correlations. Causal relationships are even more problematic to ascertain. In any case, ML approaches, just like any other tool, should be used with caution and for appropriate purposes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Parts of the code in this post is written by &lt;a href=&#34;https://planning.unc.edu/student/chenyan/&#34;&gt;Yan Chen&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Missing Millions: Undercounting Urbanisation in India</title>
      <link>https://nkaza.github.io/publication/kaza-2018-aa/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2018-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Urban form and household electricity consumption: A multilevel study</title>
      <link>https://nkaza.github.io/publication/li-2017/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/li-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Residential Energy Consumption: A Structural Analysis of Chicago</title>
      <link>https://nkaza.github.io/publication/li-2016-aa/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/li-2016-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>bibstringReview of mkbibemphPlanning Support Systems for cities and regions, bibstringby Richard Brail</title>
      <link>https://nkaza.github.io/publication/kaza-2009-review/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2009-review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>bibstringReview of mkbibemphThe Exposed City: Mapping the Urban Invisibles, bibstringby Nadia Amaroso</title>
      <link>https://nkaza.github.io/publication/kaza-2015-review/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2015-review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vain Foresight: Against Implementation</title>
      <link>https://nkaza.github.io/publication/kaza-2014-aa/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2014-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>bibstringReview of mkbibemphLand Policy: Planning and the Spatial Consequences of Property, bibstringby Benjamin Davy</title>
      <link>https://nkaza.github.io/publication/kaza-2012-review/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://nkaza.github.io/publication/kaza-2012-review/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
