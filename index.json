[{"authors":null,"categories":null,"content":"I work at the intersection of urbanization patterns, local energy policy and equity. In my research, I seek to understand the motivations, intentions and plans of multiple actors endowed with limited capabilities, imperfect foresight and distributed authority, in urban settings. Lately, I have been studying these issues in the context of local energy planning. I routinely use high cadence, large geospatial and temporal datasets and novel quantitative methods in my research. I am also a planning theorist, specialising in organisational decision-making, institutional restructuring and the role of plans. In addition to my faculty appointments in the City \u0026amp; Regional Planning at the University of North Carolina at Chapel Hill, I am also a faculty fellow at the Insitute for the Enviroment.\n Hi-res, respectable image.  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nkaza.github.io/author/nikhil-kaza/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nikhil-kaza/","section":"authors","summary":"I work at the intersection of urbanization patterns, local energy policy and equity. In my research, I seek to understand the motivations, intentions and plans of multiple actors endowed with limited capabilities, imperfect foresight and distributed authority, in urban settings.","tags":null,"title":"Nikhil Kaza","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://nkaza.github.io/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Nikhil Kaza FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://nkaza.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, \u0026#39;Hello world\u0026#39;]  Tuples\n Tuples are immutable - they can‚Äôt be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, \u0026#39;Hello world\u0026#39;)   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://nkaza.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026#34;country == \u0026#39;Canada\u0026#39;\u0026#34;) fig = px.bar(data_canada, x=\u0026#39;year\u0026#39;, y=\u0026#39;pop\u0026#39;) fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://nkaza.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://nkaza.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":[],"content":"         Getting Started Application Programming Interface (API) is a software intermediary that allows two applications to talk to each other. For example, when you use an app on your phone, it connects to the internet, queries a server in a specific way. The server responds to the query, by retrieving the data, aggregating/interpreting/managing it and sending it back to the phone. The application then interprets that data and presents you with the information you wanted in a readable way. While API are not solely based on internet based queries, these are becoming increasingly important. In this tutorial, we are going to use API to query and process US Census data via http protocol on the fly, instead of downloading them as csv files.\nTo get started you need a US census API key. Signup for one and store it in a secure place. Whenever ‚ÄúYOUR_API_KEY‚Äù appears in the code below, you should substitute this key.\nQuerying Census Servers Different APIs have different requirements, so you will have to use the documentation specific to the endpoint. To illustrate, let‚Äôs use the American Community Survey API. For example, paste the following into your web browser after suitably editing for the ‚ÄúYOUR_API_KEY‚Äù\nhttps://api.census.gov/data/2017/acs/acs5?key=YOUR_API_KEY\u0026amp;get=B01003_001E\u0026amp;for=zip%20code%20tabulation%20area:27514,27510\u0026amp;in=state:37\nThe output should look like the following in the JSON format (Different browsers may format it slightly differently)\n   The results tell you that there are 15,397 people in ZCTA 27510 and 31,659 people in ZCTA 27514. Also, this would be a good time to refresh your understanding of the differences between Zipcodes and ZCTA\nIf you get this result, from your R script (instead of the browser), it is relatively straightforward to convert them into data structures you are familiar with, such as tables, tibbles and data.frames. See this post.\nIt is useful to understand the structure of the query\n  https://api.census.gov/data/2017/acs/acs5? is the base url. It tells the census servers that it is looking for 5 year ACS data that ends in 2017 i.e ACS 2012-2017.\n  key=[YOUR_API_KEY] is the key to identify the client (i.e your browser/script) who is making the request.\n  get=B01003_001E is the variable name (Total population). For a full list, see here\n  for=zip%20code%20tabulation%20area:27514,27510 for the census geography ZCTA for Chapel Hill(27514) and Carrboro (27510). The %20 is a hexademical representation of ‚Äúunsafe‚Äù space character. There are many such special characters that need to be properly encoded, if they need to be passed to the server.\n  in=state:37 refers to the FIPS code for North Carolina.\n  Note that the all the arguments are concatenated using \u0026amp;.\n  This structure is unique to Census APIs and endpoints. For other APIs you will have to refer to their documentation. And it is also likely that these structures will change over time, which may explain why your code may stop working after a while.\n Exercise\nPractise the following in the web browser by constructing the right URL.\n Use the endpoint for the decennial census and get the population for all the zipcodes in Durham, NC. Use the ACS 2016 endpoint and get the population for all the census tracts in Orange County, NC. Use the ACS 2013-2018 and get the households under poverty in each census tracts in Orange County, NC   TidyCensus While the above is a generic way of constructing the query and getting the results using APIs, tidycensus is a package that wraps this all in different convenience functions such as get_decennial and get_acs. Note that, this package is tailored only towards US Census and not for any other Census agencies or for that matter other US government data.\n You should check out Kyle Walker‚Äôs excellent book Analyzing US Census Data: Methods, Maps, and Models in R   In the rest of the tutorial, I am going to demonstrate how to download, construct and visualise unemployment rates and poverty rates at a Census tract level. We are going to use the 5 year ACS 2012-2017 data.\nPoverty Rates using TidyCensus library(tidyverse) library(sf) library(tidycensus) census_api_key(\u0026#34;YOUR_KEY_HERE\u0026#34;, install=TRUE)  To get the poverty level, you can use B17001_01 (persons for whom poverty level is tracked) and B17001_002 (persons whose income in the last 12 months was below poverty level). Use the get_acs function for ACS.\npovrate23 \u0026lt;- get_acs(geography = \u0026#34;tract\u0026#34;, variables = \u0026#34;B17001_002\u0026#34;, summary_var = \u0026#39;B17001_001\u0026#39;,state = \u0026#34;NC\u0026#34;, geometry = TRUE, year = 2023, progress_bar = FALSE) %\u0026gt;% rename(population = summary_est) %\u0026gt;% filter(population\u0026gt;0)%\u0026gt;% mutate(pov_rate = estimate/population) %\u0026gt;% select(GEOID, NAME, population, pov_rate) povrate23 # Simple feature collection with 2643 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84.32187 ymin: 33.84232 xmax: -75.46062 ymax: 36.58812 # Geodetic CRS: NAD83 # First 10 features: # GEOID NAME population # 1 37037020600 Census Tract 206; ‚Ä¶","date":1760659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1760659200,"objectID":"472d3fd5f980f6f910088fa6b92d543b","permalink":"https://nkaza.github.io/post/using-tidycensus-package/","publishdate":"2025-10-17T00:00:00Z","relpermalink":"/post/using-tidycensus-package/","section":"post","summary":"Using tidycensus to download US Census data from the API","tags":["R","teaching"],"title":"Using TidyCensus package to explore Census Data","type":"post"},{"authors":[],"categories":["new-urban-analytics"],"content":"Introduction Drone imagery offers promising capabilities for urban planning, but its application warrants a measured and critical perspective. While drones can rapidly capture high-resolution spatial data, enabling planners to visualize land use, infrastructure, and environmental conditions with impressive detail, their use also raises concerns about data accuracy, privacy, and long-term integration. The quality of drone imagery can vary depending on weather, flight regulations, and operator expertise, potentially limiting its reliability for complex planning decisions. Moreover, the proliferation of aerial surveillance introduces ethical questions around consent and the monitoring of public and private spaces. Urban planners must also grapple with the challenge of incorporating drone data into existing workflows and regulatory frameworks, which are often not designed to accommodate such fast-evolving technologies. Nonetheless it is useful to figure out how to use them effectively.\nWhen focusing specifically on drones, the types of data they can collect are largely determined by the onboard sensors they carry. Standard RGB cameras provide high-resolution visual imagery, useful for mapping, inspections, and photogrammetry. Multispectral sensors detect light across several bands, allowing analysis of vegetation health, water pollution, and urban heat islands‚Äîespecially valuable in environmental planning. Thermal cameras capture infrared radiation, helping identify energy inefficiencies in buildings or monitor heat patterns in dense urban areas. LiDAR systems, which use laser pulses to measure distances, generate precise 3D models of terrain and infrastructure, even in areas with dense vegetation or poor lighting.\nWithin R, tools like FIELDimageR and uasim offer specialized functions for managing UAV imagery, including organizing flight data, converting formats, and preparing images for GIS integration. However, pre-processing and flight planning work is often done in other applications and R is used predominantly for processing the mosaiced raster datasets. In this tutorial we are focused on these later steps, especially on nadir/overhead images.\nMultiple ways of thinking about rasters Rasters are everywhere including pictures that may or may not have an external coordinate system, medical images (e.g. X-rays), astronomical images from telescopes, drawings etc. A raster is a grid of cells (or pixels), each holding a value that represents something about the space it covers. These values might describe elevation, temperature, color intensity, or even tissue density‚Äîdepending on the domain. Some key features:\n Grid Structure: Rasters are organized into rows and columns, forming a rectangular data. Pixel Values: Each cell contains a numeric value‚Äîthis could be brightness, elevation, or a class label. Resolution: The size of each pixel determines how detailed the raster is. Smaller pixels mean finer detail. Bands/Layers: Rasters can have multiple layers (bands), each representing different data types (e.g., RGB channels in images). Sometimes, these layers can be discretised time representing the same area at different timestamps. Spatial Context: Some rasters are tied to real-world coordinates (georeferenced), while others are not. When rasters are georeferenced within a particular coordinate system, all you need to know is the coordinates of the corners of the raster and the resolution to figure out the coordinates of every pixel in the raster.  Rasters can be thought of in a number of different ways. Chief among them are:\n They are similar to rectangular data frames where each pixel is a row and each band is a column. You can have other columns such as raster row position and raster column position. They are matrices where each cell is a pixel and each band is a matrix.  These two points are illustrated below.\n   They are a regular network/graph, where each pixel is node and the edges are the connectivity among the pixels; i.e. neighboring pixels are connected to each other. Different connectivities (such as rook, 2-nd order queen) can be defined. This is illustrated below.  Rasters are samples of a continuous surface that is discretised by the pixels.     Each of these ways of thinking may be useful for particular purposes. Sometimes it is helpful to think about rasters with their geographic locations. Sometimes what matters are internal relationships among the pixels.\nRasters in R R offers a diverse ecosystem of packages for handling rasters, images, and multidimensional arrays across domains like biology, computer vision, remote sensing, and climate science.\nRemote Sensing \u0026amp; Raster Visualization These packages are designed for geospatial raster data, satellite imagery, and thematic mapping.\n   Package Description     terra Modern package for handling spatial raster data efficiently. Supports cropping, resampling, and projection.   rasterVis Enhances raster visualization with lattice-style graphics and ‚Ä¶","date":1756944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757016577,"objectID":"04819ec4ee2090dd14cd57056a1a009e","permalink":"https://nkaza.github.io/post/working-with-drone-imagery/","publishdate":"2025-09-04T00:00:00Z","relpermalink":"/post/working-with-drone-imagery/","section":"post","summary":"Introduction Drone imagery offers promising capabilities for urban planning, but its application warrants a measured and critical perspective. While drones can rapidly capture high-resolution spatial data, enabling planners to visualize land use, infrastructure, and environmental conditions with impressive detail, their use also raises concerns about data accuracy, privacy, and long-term integration.","tags":[],"title":"Working with Drone Imagery","type":"post"},{"authors":[],"categories":["python","object detection","R","exif"],"content":"Introduction Object detection allows computers to identify and locate objects ‚Äî like cars, people, street furniture ‚Äî in images or videos. For urban planners, it offers a way to gather observational data at scale.\nIt can be used in various applications such as\n Traffic and Mobility Studies: Count vehicles, bikes, or pedestrians at intersections to inform signal timing or street design. Public Space Monitoring: Understand how parks, plazas, or transit stops are used throughout the day. Infrastructure Audits: Use aerial or street-level imagery to assess curb usage, sidewalk conditions, or construction activity.  Often these are meant to be supplement human data collection, but come with their own caveats. More on that later.\nOften object detection uses deep learning, though other computer vision tools are occasionally used. Deep learning is a techniques that uses neural networks that are ‚Äòdeep‚Äô (multiple layers) as opposed to a shallow single layer networks. Neural networks are a type of computer model inspired by how the human brain works ‚Äî they learn patterns from data and use that knowledge to make predictions or recognize things.\nThey require enormous amount of training data. One of the most widely used datasets for training these models is the COCO dataset (Common Objects in Context), which contains thousands of labeled images featuring everyday scenes with people, vehicles, animals, and more. Because training a neural network from scratch requires a lot of data and computing power, we often use pretrained models ‚Äî models that have already been trained on datasets like COCO ‚Äî and then fine-tune them for specific tasks or environments. This makes it easier and faster to apply object detection in real-world settings, including urban planning, without needing to build everything from the ground up.\nIn this tutorial, we will explore how to use pre-trained neural network models for object detection using R. We will not do any fine tuning as this is meant as an illustration.\nData For this tutorial, we will use street-level images from Rio de Janeiro, Brazil, available through Kartaview is an open-source platform for collecting and sharing street-level imagery, similar to Google Street View but built by a global community. It allows users to upload images captured from smartphones or dash cams, which can then be used for mapping, navigation, and research. You can download the images and unzip them into your InputData folder.\nlibrary(here)\rlibrary(magick)\rlibrary(tidyverse)\rimg_files \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;rio_kartaview_images\u0026#34;, \u0026#34;geotagged\u0026#34;) %\u0026gt;% #suitably modify the file path\rlist.files(full.names = TRUE, pattern = \u0026#34;\\\\.jpg$\u0026#34;) # Note the use of regular expressions to only load jpg file paths.\r Let‚Äôs view a few random images first to visually inspect some. I am going to use magick and cowplot packages to read and plot the images\nlibrary(cowplot)\rset.seed(123)\rplot_list \u0026lt;- sample(img_files, 10) %\u0026gt;% # Randomly sample 10 image file paths\rmap(function(x) {\rimage_read(x) %\u0026gt;% ## This is a function from magick package. Read the image\rimage_ggplot() + ## This is a function from magick package. Convert the image to a ggplot object\rtheme_void() ## Note the switch between %\u0026gt;% and +. The former is for chaining functions, the latter is for adding layers to ggplot. This removes the axes and background.\r})\rplot_grid(plotlist = plot_list, nrow = 2, ncol = 5) #plot_grid is a layout function from cowplot package to arrange multiple ggplot objects in a grid.\r Note the variation in lighting conditions, angles, zoom levels, focus and occlusions. This is typical of street-level imagery and can pose challenges for object detection models. Also note the differences in urban settings such as highway, commercial corridor, vegetation cover etc.\nIn addition to data within the images, it is useful to look at the metadata associated with the images. In the case of images, metadata can include details such as the date and time the photo was taken, camera type, the camera settings used, the geographic location (latitude and longitude) where the image was captured. EXIF (Exchangeable Image File Format) is a standard that stores metadata within image files. This information is automatically recorded by most digital cameras and smartphones and we can use the exiftoolr package to extract this information. exiftoolr requires exiftool to be installed on your computer . You can find instructions here or uncomment appropriate line in the code.\nlibrary(exiftoolr)\r# install_exiftool() # Uncomment this line if you don\u0026#39;t have exiftool installed already.\rimg_metadata \u0026lt;- sample(img_files, 10) %\u0026gt;%\rexif_read() %\u0026gt;%\rselect(FileName,ImageWidth, ImageHeight, Megapixels, GPSLatitude, GPSLongitude)\rhead(img_metadata)\r# FileName ImageWidth ImageHeight Megapixels\r# 1 330665659__1308095_016b5_5175.jpg 3072 1728 5.308416\r# 2 1318418389__3645437_4e431_60c0edf80bea9.jpg 3232 2424 7.834368\r# 3 346918433__1323919_5d1ab_1179.jpg 3072 1728 5.308416\r# 4 ‚Ä¶","date":1756425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756512000,"objectID":"988e9f46fb801a5c3ace5d9d6a99404e","permalink":"https://nkaza.github.io/post/object-detection-using-pre-trained-neural-network-models/","publishdate":"2025-08-29T00:00:00Z","relpermalink":"/post/object-detection-using-pre-trained-neural-network-models/","section":"post","summary":"Introduction Object detection allows computers to identify and locate objects ‚Äî like cars, people, street furniture ‚Äî in images or videos. For urban planners, it offers a way to gather observational data at scale.","tags":[],"title":"Object Detection Using Pre-trained Neural Network Models","type":"post"},{"authors":null,"categories":["teaching","R"],"content":" Course Description \u0026amp; Objectives This course is about different techniques used in assembling, managing, analysing and predicting using heterogeneous data sets in urban environments. These datasets are inherently messy and incomplete. Types of data include, point, polygon, raster, vector, text, image and network data; data sets with high cadence and high spatial resolution.\nThis is a survey course for different techniques and approaches in dealing with these data in R. The objective of these analytical techniques is to inform both short term operational decisions and long term planning in cities. As such, the emphasis is on practical urban data analytics rather than in-depth discussion about the suitability and appropriateness of techniques and their associated theoretical assumptions.\nUnlike other courses of similar vein, I put inordinate emphasis on data visualisation and communication. The point of data analysis is to tell a compelling story, not to use latest analytical techniques.\nThis is a companion course to PLAN 562: The Ethics and Politics of New Urban Analytics (Seminar), which deals with problems, opportunities and hidden agendas with data generation, analysis and visualisation in urban settings. Students are encouraged to take them both.\n Course Details  Instructor: Nikhil Kaza Classroom: New East 101 Hours: T 17:00 - 19:30 Office Hours: https://go.unc.edu/kaza Course Materials: https://nkaza.github.io/teaching/techniques-course/ HW \u0026amp; Lab submissions: Canvas Troubleshooting \u0026amp; Collaboration: https://stackoverflowteams.com/c/plan672/questions   Prerequisites \u0026amp; Preparation The course will move quickly, cover a large number of analytical techniques, data sets, use cases and disciplinary domains. It requires significant investment on the part of the students to learn the technical skills as well as to learn about substantive urban and regional analyses.\nMuch of the work in this course will be done using Open Source Software that is usually free.\nOver the summer prior to the course, you are expected to review the materials in preparation for the course.\nThe course assumes a working knowledge of R. R is a programming language and a free software environment for statistical computing and graphics. There are a number of online resources that will help you with getting up to speed with R. You will have to use extensively the documentation, help and examples that R environment provides; i.e.¬†Do not be afraid to use, for example,\n?qplot ??randomForest to seek help for specific commands.\nOne disadvantage with R is that it stores all its objects in memory. This means that your computer should have significant RAM to deal with large data sets.\nAnother disadvantage with R is that it has a shallow learning curve. And it has some quirks. In particular, please pay attention to R-Inferno. However, persistence will have long term benefits.\nYou should have an aptitude for debugging computer code, thinking through edge cases in data sets, identifying and dealing with missing data and messy data sets.\nYou should expect that the instructions and help provided may not work on your system due to different configurations, mismatched data types and differences in libraries. You should have an aptitude to troubleshoot the problems and figure out workarounds.\nIt may be helpful to go through the materials from STOR 320: Introduction to Data Science\nExternal IT Accounts While most of the work in the class will be done using R using publicly available data sets, you will need to set up accounts with the following services. Some of them might require billing enabled and most of them would require 2FA. You are responsible for monitoring them and ensuring that the charges are within your budget.\n StackOverflow for Teams. This is a private StackOverflow team for the class. You can use this to ask questions and troubleshoot issues with your peers. You should have received an invite to join the team. If you haven‚Äôt, please let me know. Github Education. Free access to Copilot in Rstudio and StackOverflow. RStudio Cloud. Free access to RStudio in the cloud. You can use this to share your code and troubleshoot issues with your peers. Claude. Free access to Claude 3.5 for R code generation and troubleshooting. US Census API. Free access to US Census data.  Others maybe required for specific portions of the course.\n  Textbooks \u0026amp; Readings The following books are used implicitly in the class. You are not required to buy any of them, but they are very useful to have on your bookshelf.\n Brewer, Cynthia A. (2015). Designing Better Maps: A Guide for GIS Users. 2 edition. Redlands, California: Esri Press. ISBN: 978-1-58948-440-5.  Few, Stephen (2015). Signal: Understanding What Matters in a World of Noise. Burlingame, California: Analytics Press. ISBN: 978-1-938377-05-1.  Tufte, E. R (2001). The Visual Display of Quantitative Information. Cheshire, CT: Graphics Press.  Wickham, Hadley (2016). Ggplot2: Elegant Graphics for Data Analysis. ‚Ä¶","date":1755388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1760054400,"objectID":"464998db36fcc7fdf17ae699b2b36b90","permalink":"https://nkaza.github.io/teaching/techniques-course/","publishdate":"2025-08-17T00:00:00Z","relpermalink":"/teaching/techniques-course/","section":"teaching","summary":"Course Description \u0026 Objectives This course is about different techniques used in assembling, managing, analysing and predicting using heterogeneous data sets in urban environments. These datasets are inherently messy and incomplete.","tags":["PLAN 672"],"title":"Urban Analytics in R","type":"teaching"},{"authors":[],"categories":["teaching"],"content":"We communicate by telling stories. In our professional world, we often use data to support our stories. But how do we tell compelling stories with data? This post explores, very briefly, the art and science of data storytelling, providing insights and techniques to help you craft narratives that resonate with your audience.\nThe Elements of a Good Data Story There are no hard rules for good writing‚Äîjust a few trusty guidelines and a lot of judgment. Consider these. Use them wisely. Break them with flair.\nWho, What, and Why Audience Every good story begins with knowing your audience. Who are they? What keeps them up at night (besides grading)? What do they already know‚Äîor think they know?\nThe story you tell your faculty advisor will not be the same one you share with policymakers, journalists, or peers. This isn‚Äôt about changing your findings‚Äîit‚Äôs about spotlighting different parts of the elephant depending on who‚Äôs asking. There‚Äôs no single ‚Äúright‚Äù story, only the right story for the this crowd at this time for that purpose. Sometimes two stories.\nTransformation Great stories are about change. What did we learn? What problem did we solve? What new questions popped up like weeds?\nIf your research confirms what everyone already suspected (but hadn‚Äôt bothered to prove), you don‚Äôt yet have a story‚Äîyou have a very polite shrug. A compelling story reveals insight, not just validation.\nPurpose A good data story has a mission. What are you trying to do‚Äîconvince, inspire, provoke, confuse and complicate? Confusion is a valid purpose. It engenders deeper understanding and sweeps away simplistic beliefs.\nWhat emotions do you want to stir? What actions do you want your audience to take? (e.g., give you money, cite your work, name their first born after you?).\nThis purpose does not have to be earth shattering, paradigm shifting. It can be as simple as ‚ÄúI want my audience to understand that X is more complicated than they thought.‚Äù Or ‚ÄúI want my audience to see that Y is a problem worth pondering about.‚Äù\nShould your audience be moved? Impressed? Outraged? A story without a purpose is just a long walk with no destination. That‚Äôs fine for Kerouac or Knausgaard, but not for you.\nThe Process Data Exploration Spend time with your data. Take it out for coffee. Ask it about its origin story. How did it evolve and mature? What is its role in the world? Understand how it was collected, what it claims to measure, and what it‚Äôs actually measuring.\nMost of the context isn‚Äôt in the data‚Äîit‚Äôs in the metadata, the instruments, the people wielding those instruments, the time spans, the units, the scales, and the deeply human messiness behind it all. Think hard about what is missing, what is biased, and what is just plain weird.\nAll data are simplified, abstracted, and mangled representation of a reality. And there are many realities, if I want to be post-modern about it. Treat data with curiosity and a healthy dose of scepticism.\nDepth A good story doesn‚Äôt try to say everything. It goes deep, not wide. Anchor your story with a thesis.\nHowever, you won‚Äôt get to a thesis in the beginning. You might want to start with 4‚Äì5 core findings that matter. These might include:\n Surprising results that defied expectations Patterns that emerged across analyses Anomalies that begged for explanation Methodological wizardry you invented (In this class, we don‚Äôt care about this as much. Maybe in your phd.) Theoretical frameworks you tested or refined Real-world applications that actually work  For each finding, ask:\n What did we believe before? What changed? What new questions arose? What became possible?  If nothing changed, toss it. Ruthlessly. Your story deserves better.\nEventually, settle on 1‚Äì2 findings that carry the most weight. These are your story‚Äôs spine. They inform your thesis.\nThe Presentation Your audience needs to know what‚Äôs at stake. Set the stage at the beginning. Make them care. Use a motivating example, a neglected concept, a spicy newspaper headline‚Äîwhatever works. Context is everything. Stakes are everything-er.\nStructure Every story has a beginning, middle, and end. The middle is where your methods and results live‚Äîand let‚Äôs be honest, only your committee cares. Much of your graduate training is about this middle. However, the important things are the other two.\nThe beginning hooks your audience. The end leaves them thinking, feeling, and maybe tweeting about it.\nExhibits Just because you can make a pretty chart doesn‚Äôt mean you should. Every exhibit‚Äîtable, figure, map, interpretive dance‚Äîshould serve a purpose. It should clarify, not confuse1. Label it clearly. Integrate it into your narrative.\nRemember, exhibits are not neutral. They are not faithful representations of your data. They are evidence to support your argument. As such, they are heavily biased towards that purpose. Lean into those biases. Use them wisely. Use them effectively. Think about what you need to foreground and what you need to background. Think about what ‚Ä¶","date":1754438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754513052,"objectID":"56ff7320b45f9ea77658b3d4472c23de","permalink":"https://nkaza.github.io/post/data-stories/","publishdate":"2025-08-06T00:00:00Z","relpermalink":"/post/data-stories/","section":"post","summary":"We communicate by telling stories. In our professional world, we often use data to support our stories. But how do we tell compelling stories with data? This post explores, very briefly, the art and science of data storytelling, providing insights and techniques to help you craft narratives that resonate with your audience.","tags":[],"title":"Telling Stories with Data","type":"post"},{"authors":null,"categories":["llm","data-munging","R"],"content":" Introduction This post was last updated on 2025-08-07\nLarge Language Models (LLMs) represent a class of artificial intelligence systems that have fundamentally transformed natural language processing and generation. These neural networks, trained on vast corpora of text data, demonstrate remarkable capabilities in mimicking the understanding context, generating coherent(ish) responses, and performing complex reasoning tasks across diverse domains.\nAt their core, most modern LLMs are built upon the transformer architecture, introduced in the seminal paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al.¬†in 2017. The transformer‚Äôs key innovation lies in its self-attention mechanism, which allows the model to weigh the importance of different words in a sequence when processing each token. This parallel processing capability offers significant computational advantages over previous sequential architectures like RNNs and LSTMs.\n(Generated by Gemini)\nThe transformer consists of encoder and decoder blocks, each containing multi-head self-attention layers and feed-forward networks. However, most contemporary LLMs adopt variations of this base architecture, leading to distinct model families with different strengths and applications.\nLLM development typically follows a multi-stage process. Pre-training involves unsupervised learning on massive text corpora, where models learn to predict next tokens and develop broad linguistic understanding. This is why many of the models are really bad at langauges other than English. Fine-tuning adapts pre-trained models to specific tasks or domains through supervised learning on curated datasets.\nArchitectural Variations Encoder-only models, such as BERT (Bidirectional Encoder Representations from Transformers), excel at understanding and encoding text representations. These models process input bidirectionally, making them particularly effective for tasks like sentiment analysis, question answering, and text classification where understanding context from both directions is crucial.\nDecoder-only models represent the dominant paradigm for generative LLMs. Models like GPT (Generative Pre-trained Transformer), Claude, and LLaMA use causal self-attention, processing text autoregressively from left to right. This architecture proves highly effective for text generation, completion, and conversational AI applications.\nEncoder-decoder models, including T5 (Text-to-Text Transfer Transformer) and BART, combine both components and excel at sequence-to-sequence tasks like translation, summarization, and text transformation.\n Scale and Emergent Capabilities The ‚Äúlarge‚Äù in Large Language Models refers to their unprecedented scale across multiple dimensions: parameter count, training data volume, and computational requirements. Modern LLMs contain billions to trillions of parameters, with models like GPT-4 and PaLM demonstrating that increased scale often leads to emergent capabilities not present in smaller models.\nThese emergent properties include where models can adapt to new tasks with minimal examples, chain-of-thought reasoning for complex problem-solving, and cross-modal understanding when trained on multimodal data. The scaling hypothesis, which I do not subscribe to, suggests that many AI capabilities may emerge naturally from increased model size and training data, though this relationship isn‚Äôt always predictable.\n  Use Case LLMs have found applications across a wide range of domains and in particular for planning purposes they can be used for extracting data from unstructured texts. In this tutorial, I am going to demonstrate it on an example dataset scraped from the North Carolina Utilities Commission website.\n‚ÄúPursuant to G.S. 62-110.1(g), any person who seeks to construct an electric generating facility in North Carolina, and is exempt from the requirement to obtain a certificate of public convenience and necessity, is required to file this [ROPC] form and a notice of completion of the construction of the facility.‚Äù\nSmall scale (\u0026lt;1 MW) solar power plants (rooftop or ground mounted) fall under this category. This dataset consists of all the unique records for ROPC until mid May 2024. Note that these do not necessarily mean that the plant is completed or operational.\nlibrary(tidyverse) library(here) ropc_data \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;llms\u0026#34;, \u0026#34;dockets_fileurls_unique.csv\u0026#34;) %\u0026gt;% read_csv(show_col_types = FALSE) %\u0026gt;% mutate(across(everything(), as.character)) set.seed(123) # For reproducibility test_data \u0026lt;- ropc_data %\u0026gt;% sample_n(5) test_data$Description # [1] \u0026#34;ROPC for 0.00609MW Solar Located at 7145 Streamhaven Drive, Harrisburg, NC 28075 in Cabarrus County\\nFiles:\u0026#34; # [2] \u0026#34;ROPC for 5.2kW Solar Located at 179 Lars Lane, Garner, NC 27529 in Johnston County\\nFiles:\u0026#34; # [3] \u0026#34;ROPC for 10.85kW Solar System Located at 8804 Amerjack Ct. Raleigh, NC 27603 in Wake County\\nFiles:\u0026#34; # [4] \u0026#34;ROPC for 10.88 kW Solar Located at 46 Sunnyfield Ct Benson NC 27504 in Johnston County\\nFiles:\u0026#34; # [5] ‚Ä¶","date":1753920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753965149,"objectID":"874b838145f372b145b6d2d8854d8574","permalink":"https://nkaza.github.io/post/possibilities-and-pitfalls-of-large-language-models/","publishdate":"2025-07-31T00:00:00Z","relpermalink":"/post/possibilities-and-pitfalls-of-large-language-models/","section":"post","summary":"Introduction This post was last updated on 2025-08-07\nLarge Language Models (LLMs) represent a class of artificial intelligence systems that have fundamentally transformed natural language processing and generation. These neural networks, trained on vast corpora of text data, demonstrate remarkable capabilities in mimicking the understanding context, generating coherent(ish) responses, and performing complex reasoning tasks across diverse domains.","tags":[],"title":"Possibilities and Pitfalls of Large Language Models.","type":"post"},{"authors":[],"categories":["R"],"content":"\n\n\n","date":1753747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753802566,"objectID":"be1526c31cc03572aa013d6ffd15c6a1","permalink":"https://nkaza.github.io/post/using-external-databases-such-as-postgresql/","publishdate":"2025-07-29T00:00:00Z","relpermalink":"/post/using-external-databases-such-as-postgresql/","section":"post","summary":"","tags":[],"title":"Using external databases such as PostGRESQL","type":"post"},{"authors":["Jordan Branham","Nikhil Kaza","Todd BenDor"],"categories":[],"content":"","date":1726392148,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392148,"objectID":"da6a583521f7bb592ccf2cbd8e67f09a","permalink":"https://nkaza.github.io/publication/branham-conservation/","publishdate":"2024-09-15T09:22:28.029371Z","relpermalink":"/publication/branham-conservation/","section":"publication","summary":"","tags":[],"title":"Do development disincentives influence land conservation activity?","type":"publication"},{"authors":["K. Khanal","Nichola Lowe","Nikhil Kaza"],"categories":[],"content":"","date":1726392147,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392148,"objectID":"777cb0560c99dd9c3a431f073bfa07be","permalink":"https://nkaza.github.io/publication/khanal-energytransition/","publishdate":"2024-09-15T09:22:27.896156Z","relpermalink":"/publication/khanal-energytransition/","section":"publication","summary":"","tags":[],"title":"Retraining for Energy Transition: A Workforce Development Approach Using Occupational Similarity and Unsupervised Clustering","type":"publication"},{"authors":["Betsy Donald","Shauna Brail","Nichola Lowe","C. DeLoyde","Kaitlin Heatwole","F. Hernandez","K. Hill-Tout","Nikhil Kaza","Kshtiz Khanal","Donald Planey","Jueyu Wang"],"categories":[],"content":"","date":1726392147,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"c343d806922222306b4acdbab312be17","permalink":"https://nkaza.github.io/publication/donald-dashboard-not-dead/","publishdate":"2024-09-15T09:22:27.743328Z","relpermalink":"/publication/donald-dashboard-not-dead/","section":"publication","summary":"","tags":[],"title":"The Dashboard Is Not Dead: Dashboards as Effective Tools in Skills Building, Sense-Making and Community Collaboration","type":"publication"},{"authors":["Julia Cardwell","Nikhil Kaza"],"categories":[],"content":"","date":1726392145,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392145,"objectID":"c720af1097bccea859fa077ee610a1e4","permalink":"https://nkaza.github.io/publication/cardwell-aa/","publishdate":"2024-09-15T09:22:25.704914Z","relpermalink":"/publication/cardwell-aa/","section":"publication","summary":"","tags":[],"title":"Extension, Densification, Dispersion and Stagnation: Patterns in Urban Spatial Development in Metropolitan United States 2001-2021","type":"publication"},{"authors":["Kshitiz Khanal","Nikhil Kaza","Miyuki Hino","Antonia Sebastian"],"categories":[],"content":"","date":1726392145,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392145,"objectID":"0861561bd8f754754d88456ed97ae7e8","permalink":"https://nkaza.github.io/publication/khanal-aa/","publishdate":"2024-09-15T09:22:25.457504Z","relpermalink":"/publication/khanal-aa/","section":"publication","summary":"","tags":[],"title":"Manufactured Housing in North Carolina: A Computer Vision Approach","type":"publication"},{"authors":[],"categories":["R","teaching"],"content":"            Introduction In this post, I am going to show how one might go about analysing the Bikeshare use in New York city. The purpose of this post is to demonstrate how to do some standard exploratory analysis and visualisation of a data set that has some spatial attributes, in open source software primarily in R.\nOne of the fundamental data structures in R is colloquially a table. A table is collection of loosely related observations (rows), where the attributes of these observations are in columns. In general, each column has values of same type, e.g.¬†text, date, number, but different columns could be of different types. One of these columns could be a geometry, another could be a time stamp. This is the philosophy behind some of the newer R packages that are using ISO standards for spatial data. Traditional GIS, such as ArcGIS or QGIS put geometry/location as the fundamental and attach attributes to locations. This requires that all geometries to be the same, which is limiting in many cases. Whereas spatial databases such as POSTGIS/POSTGRESQL, GeoJSON etc. make observations fundamental and attach geometry as one of the columns (could have missing values). This also allows for mixing different geometry types. In this tutorial, we will use the sf package that implements the simple features access (like POSTGIS) to load, analyse and visualise spatial data. The advantage with sf is that it fits with the tidyverse computation frameworks, including visualisation with ggplot.\nAs we will see in this analyses, many of the interesting questions lurk in the relationships and summarisation of the variables of the observations rather than in the spatial relationships. In many instances, spatial attributes are used for visualisation and exploration rather taking advantage of the inherent topolgical relationships. However, even when topological relationships are important, many packages in R allow us to work with them.\nAdditional resources In this post, I am assuming that the users have some familarity with spatial datasets and this includes coordinate systems, projections, basic spatial operations such as buffering, distance etc. in other contexts such as QGIS. If not, please refer other resources, including this one. Graphical User Interfaces of some software such as QGIS and ArcGIS allow for quick learning, visualisation. Using R or Python should be considered an advanced skills when point and click becomes tedious and/or overhead for constant visualisation is not essential to the task at hand.\nOther resources include\n Robert Hijmans. www.rspatial.org Edzer Pebesma. www.r-spatial.org Robin Lovelace, Jakub Nowosad, Jannes Muenchow. Geocompuation with R Oscar Perpi√±√°n Lamigueiro. Displaying time series, spatial and space-time data with R  Acquire data  Bikeshare use in New York city. Download the June 2018 dataset. 2010 Census Blocks for New York city Blockgroup demographic data from American Community Survey 2012-2016  You can also download the local copy of the above datasets.\n Todd Schneider analysed 22 million NYC bike share trips of a much larger dataset that this. Check it out.   Analyse bike stations library(tidyverse) tripdata \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;,\u0026#34;nycbikeshare\u0026#34;, \u0026#34;201806-citibike-tripdata.csv\u0026#34;) %\u0026gt;% read_csv()#Change the file path to suit your situation. tripdata \u0026lt;- rename(tripdata, #rename column names to get rid of the space Slat = `start station latitude`, Slon = `start station longitude`, Elat = `end station latitude`, Elon = `end station longitude`, Sstid = `start station id`, Estid = `end station id`, Estname = `end station name`, Sstname = `start station name` ) tripdata # # A tibble: 1,953,052 √ó 16 # tripduration starttime stoptime Sstid Sstname Slat # \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 569 2018-06-01 01:57:20 2018-06-01 02:06:50 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 2 480 2018-06-01 02:02:42 2018-06-01 02:10:43 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 3 692 2018-06-01 02:04:23 2018-06-01 02:15:55 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 4 664 2018-06-01 03:00:55 2018-06-01 03:11:59 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 5 818 2018-06-01 06:04:54 2018-06-01 06:18:32 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 6 753 2018-06-01 06:11:52 2018-06-01 06:24:26 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 7 687 2018-06-01 07:15:15 2018-06-01 07:26:42 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 8 619 2018-06-01 07:40:02 2018-06-01 07:50:22 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 9 819 2018-06-01 07:43:01 2018-06-01 07:56:41 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # 10 335 2018-06-01 07:49:47 2018-06-01 07:55:23 72 W 52 St \u0026amp; 1‚Ä¶ 40.8 # # ‚Ñπ 1,953,042 more rows # # ‚Ñπ 10 more variables: Slon \u0026lt;dbl\u0026gt;, Estid \u0026lt;dbl\u0026gt;, Estname \u0026lt;chr\u0026gt;, Elat \u0026lt;dbl\u0026gt;, # # Elon \u0026lt;dbl\u0026gt;, bikeid \u0026lt;dbl\u0026gt;, name_localizedValue0 \u0026lt;chr\u0026gt;, usertype \u0026lt;chr\u0026gt;, # # `birth year` \u0026lt;dbl\u0026gt;, gender \u0026lt;dbl\u0026gt; #Convert gender and usertype to factor tripdata$gender \u0026lt;- factor(tripdata$gender, labels=c(\u0026#39;Unknown\u0026#39;, \u0026#39;Male\u0026#39;, \u0026#39;Female\u0026#39;)) tripdata$usertype\u0026lt;- factor(tripdata$usertype) summary(tripdata) # tripduration starttime # Min. : 61 Min. :2018-06-01 00:00:02.46 # 1st Qu.: 386 1st Qu.:2018-06-08 16:40:28.67 # Median : 660 Median ‚Ä¶","date":1726272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726358400,"objectID":"9cb9248a7a31bdf05aea0bb036f28021","permalink":"https://nkaza.github.io/post/geospatial-data-in-r/","publishdate":"2024-09-14T00:00:00Z","relpermalink":"/post/geospatial-data-in-r/","section":"post","summary":"Introduction In this post, I am going to show how one might go about analysing the Bikeshare use in New York city.","tags":[],"title":"Geospatial Data in R","type":"post"},{"authors":null,"categories":["R"],"content":" Course Description \u0026amp; Objectives The aim of this bootcamp is to introduce using R, a powerful and flexible interactive environment for statistical computing and research. It is not intended to cover everything there is to know about R - that would be an impossible task. Neither is it intended to be an introductory statistics course, although you will be using some simple statistics to highlight some of R‚Äôs capabilities. The main aim of this bootcamp is to help you get over the initial learning hurdle and provide you with the basic skills and experience (and confidence!) to enable learning in PLAN 672- Urban Data Analytics in R\n Textbook We will discuss the material from Introduction to R book\n Course Policies Venue The bootcamp is conducted in New East computer lab. Please refer to email for access instructions.\n Equipment Every student should have a computer that has R and Rstudio installed. The computer should have sufficient memory and processing capacity to deal with large data sets.\n Grading We will jointly do exercises from the textbook. No submission is required; no grades are assigned.\n  Schedule Day 1 9:30 AM - 12:30 PM: R Basics  Slides\n Notes: Chapter 1 | Chapter 2\n   1:30 PM - 4:30 PM: Tidyverse  Slides\n Notes: Chapter 3 | Chapter 4\n    Day 2 9:30 AM - 12:30 PM: Visualisation  Slides Notes: Chapter 5   1:30 PM - 4:30 PM: Programming in R  Slides Notes: Chapter 6     ","date":1723852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723852800,"objectID":"2a1b8200619b671c616b9b57cbbed39a","permalink":"https://nkaza.github.io/teaching/intro2r/","publishdate":"2024-08-17T00:00:00Z","relpermalink":"/teaching/intro2r/","section":"teaching","summary":"R Bootcamp","tags":["bootcamp","teaching"],"title":"Introduction to R - 2 day bootcamp","type":"teaching"},{"authors":["M. Hino","T. K. BenDor","J. Branham","N. Kaza","A. Sebastian","S. Sweeney"],"categories":[],"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"4b69c5531988ee2e540c3f4b74b627f4","permalink":"https://nkaza.github.io/publication/hino-2023-uq/","publishdate":"2024-02-01T00:00:00Z","relpermalink":"/publication/hino-2023-uq/","section":"publication","summary":"Problem, research strategy, and findings: Limiting housing and infrastructure in flood-prone places has long been recognized as critical to managing long-term risk. However, due to the difficulty of tracking development at small spatial scales, little empirical research has been conducted to explain differences between communities? floodplain development patterns. We analyzed new construction across 5 million parcels in the state of North Carolina to develop standardized measures of floodplain development and evaluated the relationships between flood risk management effort and development outcomes. Statewide, for every property removed through buyouts from 1996 to 2017, more than 10 new residences were built in floodplains. At the community level, indicators of flood risk management effort (participation in the Community Rating System and use of buyouts) did not consistently align with floodplain development outcomes. Based on a sample of urban and suburban counties, we found more than 75,000 acres of vacant floodplain land currently zoned for development. Although we did not capture the full range of flood risk mitigation practices, results indicate that local development policies often run counter to efforts to limit long-term risk.\nTakeaway for practice: Land use planning and floodplain management have the potential to play a larger role in flood risk mitigation. Modifying federal programs to more strongly disincentivize floodplain development could enhance local regulation and minimize future flood exposure. Given extensive undeveloped floodplain land and projected climate change, additional effort to manage development is needed to limit increases in flood risk.","tags":["floods,","resilience"],"title":"Growing Safely or Building Risk?","type":"publication"},{"authors":["Nikhil Kaza"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392148,"objectID":"a47c882d8b6406f630405e7d62e6a8e1","permalink":"https://nkaza.github.io/publication/kaza-multiclass-compactness-index-2024/","publishdate":"2024-09-15T09:22:28.165322Z","relpermalink":"/publication/kaza-multiclass-compactness-index-2024/","section":"publication","summary":" Often urban compactness indexes ignore the variegated intensity of urban development in their focus on the configuration. In this article, I propose an extension to the index of moment of inertia (IMI) as a compactness measure that can account for multiple ordinal classes representing development intensity. I demonstrate this index for developed areas for each county in the contiguous United States. These counties have a range of development configurations and intensity compositions. I show that for this data set, ignoring the multiple class composition and only focusing on the configuration overestimates the urban compactness in almost all instances. I demonstrate the impact of different analytical choices on the index for this data set. ","tags":[],"title":"Multiclass Compactness Index for Urban Areas","type":"publication"},{"authors":["Nikhil Kaza"],"categories":null,"content":"","date":1683792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683792000,"objectID":"931c27e94b2690a9355923d9eadfbb1f","permalink":"https://nkaza.github.io/talk/advanced-spatial-analysis-in-r/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/advanced-spatial-analysis-in-r/","section":"event","summary":"Spatial Analysis in R","tags":[],"title":"Advanced Spatial Analysis in R","type":"event"},{"authors":["Nikhil Kaza"],"categories":null,"content":"","date":1683187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683187200,"objectID":"d0cff1acb4d3927e36a38f3d86e53242","permalink":"https://nkaza.github.io/talk/spatial-analysis-in-r/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/spatial-analysis-in-r/","section":"event","summary":"Spatial Analysis in R","tags":[],"title":"Spatial Analysis in R","type":"event"},{"authors":["Nikhil Kaza"],"categories":null,"content":"","date":1682841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682841600,"objectID":"4b7e324a74e0fc9386f67cff7469776a","permalink":"https://nkaza.github.io/talk/visualisation-principles/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/visualisation-principles/","section":"event","summary":"Principles and Practise of Data Visualisation","tags":[],"title":"Visualisation Principles","type":"event"},{"authors":["Nikhil Kaza"],"categories":null,"content":"","date":1682236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682236800,"objectID":"a3903f9a8d0e04db9ae5e0588b15c003","permalink":"https://nkaza.github.io/talk/introduction-to-r-urban-analytics/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/introduction-to-r-urban-analytics/","section":"event","summary":"Introduction to R","tags":[],"title":"Introduction to R \u0026 Urban Analytics","type":"event"},{"authors":["Nikhil Kaza"],"categories":null,"content":"","date":1677243600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677243600,"objectID":"df1cc64a28a6ccdf2ff18c8b44dcd781","permalink":"https://nkaza.github.io/talk/data-driven-approaches-to-quantifying-relationships-between-urban-form-livability/","publishdate":"2023-02-24T00:00:00Z","relpermalink":"/talk/data-driven-approaches-to-quantifying-relationships-between-urban-form-livability/","section":"event","summary":"Different Data Driven Approaches","tags":[],"title":"Data driven approaches to quantifying relationships between urban form \u0026 livability","type":"event"},{"authors":["M. DiCarlo","E. Z.Berglund","N. Kaza","A. Grieshop","L. Shealy","A. Behr"],"categories":[],"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"2f1d1e1fe8a8c722dea15f900da10476","permalink":"https://nkaza.github.io/publication/dicarlo-customer-complaint-management-2023-a/","publishdate":"2021-02-18T16:50:03.894005Z","relpermalink":"/publication/dicarlo-customer-complaint-management-2023-a/","section":"publication","summary":"Community water systems (CWSs) supply safe drinking water through pipes and other conveyances to the same population year-round. Complaint management is an important activity for CWSs and can assist efforts to monitor water quality and improve public perceptions. This research explores how CWSs receive, store, and use customer complaints. A new dataset is constructed through the distribution of an online survey. Respondents represent more than 500 CWSs across the U.S. and vary in characteristics, including the population size served. This research gives new insight about the tools that CWSs need and are willing to adopt for analyzing and reporting water quality issues.","tags":["Community water systems,","Complaint management,","Survey"],"title":"Customer Complaint Management and Smart Technology Adoption by Community Water Systems","type":"publication"},{"authors":["Kshitiz Khanal","Nichola Lowe","Nikhil Kaza"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"01f4d98b96f77b16bc41bf70abb6ed6a","permalink":"https://nkaza.github.io/publication/khanal-2023-aa/","publishdate":"2024-09-15T09:22:26.360037Z","relpermalink":"/publication/khanal-2023-aa/","section":"publication","summary":"","tags":[],"title":"Targeting Occupations to Retrain for Clean Energy Workforce Development And Implications for Labor Market Dynamics","type":"publication"},{"authors":[],"categories":["transportation","R"],"content":"            Introduction Understanding the reach and effectiveness of public transit is important to understand how urban environments are accessible for different groups of people. Analysing fixed route public transit has become relatively straightforward with the introduction of the ‚ÄúGeneral Transit Feed Specification‚Äù, a common standard for transit agencies to publish their schedules. Pioneered by TriMet of Portland, Oregon in collaboration with Google, this specification has become common for transit agencies around the world to publish their consumer facing datasets.\nGTFS is split into a schedule component that contains schedule, fare, and geographic transit information and a real-time component that contains arrival predictions, vehicle positions and service advisories. Static GTFS consists of routes, trips, stop_times, stops and calendar as mandatory tables. There are also other optional tables such as fares and frequencies. Real Time GTFS (gtfs-rt) consists of trip updates, service interruptions etc.\nReading in GTFS files Because GTFS are regularly updated (usually semi-annually) and published on transit agency‚Äôs website, it is a good idea to get the latest schedule. Fortunately, Mobility Database has a catalogue of various GTFS feeds and their source locations. In this tutorial, I am going to use Metro St.¬†Louis GTFS feed.\nIn addition, I am going to use tidytransit and gtfsrouter packages. extract_gtfs is a function in the gtfsrouter package that reads the zipfile and converts it into bunch of data tables. Similarly read_gtfs from tidytransit could also be used.\nIn the following code, I am showing how to download the latest GTFS files. However, I am going to use an archived GTFS files for consistency.\ngtfs_feeds \u0026lt;- read_csv(\u0026#34;https://bit.ly/catalogs-csv\u0026#34;) %\u0026gt;% filter(provider == \u0026#34;Metro St. Louis\u0026#34;) %\u0026gt;% filter(data_type == \u0026#34;gtfs\u0026#34;) %\u0026gt;% pull(\u0026#39;urls.direct_download\u0026#39;) gtfs_feeds ## function to download and read the gtfs feeds. Particularly helpful, if there are multiple feeds and/or multiple transit agencies in the area. download_read_gtfs \u0026lt;- function(feed_url){ zipfilename \u0026lt;- basename(feed_url) download.file(feed_url, destfile = zipfilename) # gtfsrouter::extract_gtfs(zipfilename) tidytransit::read_gtfs(zipfilename) } stlouis_gtfs \u0026lt;- gtfs_feeds %\u0026gt;% download_read_gtfs() summary(stlouis_gtfs)  library(widgetframe) library(tidytransit) library(tidyverse) library(sf) library(units) library(here) stlouis_gtfs \u0026lt;- here(\u0026#34;tutorials_datasets/gtfs/google_transit.zip\u0026#34;) %\u0026gt;%tidytransit::read_gtfs() summary(stlouis_gtfs) # tidygtfs object # files agency, stops, routes, trips, stop_times, calendar, calendar_dates, shapes # agency Metro St. Louis # service from 2022-09-26 to 2023-03-12 # uses stop_times (no frequencies) # # routes 121 # # trips 17704 # # stop_ids 5315 # # stop_names 5256 # # shapes 420  Notice that there are about 121 routes with over 5,000 stops in the St.¬†Louis region. Visualising these stops is pretty straightforward, once you convert them into sf objects.\nstlouis_gtfs \u0026lt;- gtfs_as_sf(stlouis_gtfs, skip_shapes = FALSE, crs = 4326, quiet = TRUE) stlouis_gtfs # $agency # # A tibble: 1 √ó 8 # agency_phone agency_url agenc‚Ä¶¬π agenc‚Ä¶¬≤ agenc‚Ä¶¬≥ agenc‚Ä¶‚Å¥ agenc‚Ä¶‚Åµ agenc‚Ä¶‚Å∂ # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 314-231-2345 http://www.metro‚Ä¶ EN \u0026#34;\u0026#34; Metro ‚Ä¶ Americ‚Ä¶ http:/‚Ä¶ Transi‚Ä¶ # # ‚Ä¶ with abbreviated variable names ¬π‚Äãagency_lang, ¬≤‚Äãagency_id, ¬≥‚Äãagency_name, # # ‚Å¥‚Äãagency_timezone, ‚Åµ‚Äãagency_fare_url, ‚Å∂‚Äãagency_email # # $calendar_dates # # A tibble: 6 √ó 3 # service_id date exception_type # \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;int\u0026gt; # 1 3_merged_3039412 2023-01-02 1 # 2 3_merged_3039412 2022-12-26 1 # 3 1_merged_3039411 2023-01-02 2 # 4 1_merged_3039411 2022-12-26 2 # 5 3_merged_3039415 2022-11-24 1 # 6 1_merged_3039414 2022-11-24 2 # # $calendar # # A tibble: 6 √ó 10 # service_id start_date end_date monday tuesday wedne‚Ä¶¬π thurs‚Ä¶¬≤ friday satur‚Ä¶¬≥ # \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; # 1 2_merged_‚Ä¶ 2022-09-26 2022-11-27 0 0 0 0 0 1 # 2 2_merged_‚Ä¶ 2022-11-28 2023-03-12 0 0 0 0 0 1 # 3 3_merged_‚Ä¶ 2022-11-28 2023-03-12 0 0 0 0 0 0 # 4 1_merged_‚Ä¶ 2022-11-28 2023-03-12 1 1 1 1 1 0 # 5 3_merged_‚Ä¶ 2022-09-26 2022-11-27 0 0 0 0 0 0 # 6 1_merged_‚Ä¶ 2022-09-26 2022-11-27 1 1 1 1 1 0 # # ‚Ä¶ with 1 more variable: sunday \u0026lt;int\u0026gt;, and abbreviated variable names # # ¬π‚Äãwednesday, ¬≤‚Äãthursday, ¬≥‚Äãsaturday # # $stops # Simple feature collection with 5315 features and 7 fields # Geometry type: POINT # Dimension: XY # Bounding box: xmin: -90.66209 ymin: 38.46975 xmax: -89.87466 ymax: 38.82565 # Geodetic CRS: WGS 84 # # A tibble: 5,315 √ó 8 # wheelchair_boarding zone_id stop_id stop_desc stop_‚Ä¶¬π locat‚Ä¶¬≤ stop_‚Ä¶¬≥ # * \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; # 1 2 \u0026#34;\u0026#34; 9160 FAR SIDE NATURAL‚Ä¶ NATURA‚Ä¶ 0 9160 # 2 2 \u0026#34;\u0026#34; 16079 FAR SIDE KINGSHI‚Ä¶ KINGSH‚Ä¶ 0 16079 # 3 2 \u0026#34;\u0026#34; 4446 FAR SIDE WOODSON‚Ä¶ WOODSO‚Ä¶ 0 4446 # 4 2 \u0026#34;\u0026#34; 5745 NEAR SIDE 546 NO‚Ä¶ 546 NO‚Ä¶ 0 5745 # 5 2 \u0026#34;\u0026#34; 16076 FAR SIDE CRAIG @‚Ä¶ CRAIG ‚Ä¶ 0 16076 # 6 2 \u0026#34;\u0026#34; 16075 FAR SIDE CRAIG @‚Ä¶ CRAIG ‚Ä¶ 0 16075 # ‚Ä¶","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668794076,"objectID":"b9d6829f20de50868d8a0bf102393bbb","permalink":"https://nkaza.github.io/post/transit-accessibility-using-gtfs/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/post/transit-accessibility-using-gtfs/","section":"post","summary":"Introduction Understanding the reach and effectiveness of public transit is important to understand how urban environments are accessible for different groups of people.","tags":[],"title":"Transit Accessibility Using GTFS","type":"post"},{"authors":["J. Branham","N. Kaza","T. K. BenDor","D. Salvesen","K. Onda"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636660105,"objectID":"3faf3add4dc30b20961e4c70d0c38657","permalink":"https://nkaza.github.io/publication/branham-removing-federal-subsidies-2021/","publishdate":"2021-11-11T19:48:24.822849Z","relpermalink":"/publication/branham-removing-federal-subsidies-2021/","section":"publication","summary":"The federal government has implemented a variety of policies and subsidies that help coastal development remain viable, including investments in risk reduction measures, subsidized flood insurance, and post-disaster assistance. In this study, we explore how the removal of federal subsidies impacts coastal development patterns by measuring the causal effect of the U.S. Coastal Barrier Resources Act (CBRA) on building activity. Implemented in 1982, CBRA withdrew eligibility for federal funding for infrastructure, post-disaster assistance, and subsidized flood insurance along designated sections of coastal barriers (``CBRA units''). Using a novel built structures dataset, we employ a spatial regression discontinuity design to compare development rates inside and outside CBRA units in 1980 and 2016. We demonstrate that CBRA has caused significant reductions in development activity, with development rates in CBRA more than 75% lower than areas outside CBRA. Our findings suggest that policies like CBRA can be effective at slowing development in other sensitive or hazardous areas, and could help to preserve natural environments for habitat conservation and climate adaptation purposes.","tags":[],"title":"Removing Federal Subsidies from High-Hazard Coastal Areas Slows Development","type":"publication"},{"authors":[],"categories":["transportation","new-urban-analytics"],"content":"            Introduction Some transportation networks are intuitive to understand as networks. We can easily visualize intersections as nodes and roadways as edges in a road network. A network analysis approach helps in understanding complex behaviour of systems such as transportation networks that would not be possible with other types of analyses that look at individual components as separate entities. Understanding the relationships between various components such as different types of roadways (highways, side walks, residential roads) and how they induce events likes traffic jams requires a relational systemic perspective that is not fully provided with statistical or spatial analysis on their own. Spatial networks preserves the spatial properties while allowing exploration of relations between entities represented as points, lines and polygons.\nWhile it is fairly easy to extract road networks from geospatial databases such as OpenStreetMap, using such data for network analysis isn‚Äôt so straightforward. Road network information are inherently messy because of complexities in roads and information associated with roads and transportation. In this tutorial, we will learn how to clean spatial networks in R. We take an example of a network of walkable streets in Downtown Carrboro in North Carolina.\nWe will use sfnetworks package. The package bridges network analysis functionalities of the tidygraph package with the spatial analysis functionalities of the sf package.\nAs of writing this tutorial, sfnetworks has recently been removed from CRAN, the peer-reviewed repository for R packages. We can use remotes packages to install the package from Github.\ninstall.packages(\u0026#34;remotes\u0026#34;) remotes::install_github(\u0026#34;luukvdmeer/sfnetworks\u0026#34;)  library(osmdata) library(tidyverse) library(sf) library(igraph) library(tidygraph) library(sfnetworks) library(tmap) library(dbscan) library(widgetframe)  Downloading Data \u0026amp; Sanity Checks Previously we have used ‚Äúosmdata‚Äù package to download restaurants from OpenStreetMap using Overpass Query. Let‚Äôs use the same package to download all walkable ways in Downtown Carrboro. The key-value pairs used are not exhaustive. There maybe others that are relevant.\nwe_foot \u0026lt;- opq(\u0026#34;Downtown Carrboro, North Carolina\u0026#34;) %\u0026gt;% add_osm_feature(key = \u0026#34;highway\u0026#34;, value = c(\u0026#34;bridleway\u0026#34;,\u0026#34;corridor\u0026#34;, \u0026#34;elevator\u0026#34;, \u0026#34;footway\u0026#34;, \u0026#34;living_street\u0026#34;, \u0026#34;path\u0026#34;, \u0026#34;pedestrian\u0026#34;, \u0026#34;primary\u0026#34;, \u0026#34;primary_link\u0026#34;, \u0026#34;residential\u0026#34;, \u0026#34;secondary\u0026#34;, \u0026#34;secondary_link\u0026#34;, \u0026#34;service\u0026#34;, \u0026#34;steps\u0026#34;, \u0026#34;tertiary\u0026#34;, \u0026#34;tertiary_link\u0026#34;, \u0026#34;track\u0026#34;, \u0026#34;unclassified\u0026#34;))%\u0026gt;% osmdata_sf() #convert to sf  We will look at the columns, and a summary of the components of the spatial object\nnames(we_foot)  ## [1] \u0026#34;bbox\u0026#34; \u0026#34;overpass_call\u0026#34; \u0026#34;meta\u0026#34; ## [4] \u0026#34;osm_points\u0026#34; \u0026#34;osm_lines\u0026#34; \u0026#34;osm_polygons\u0026#34; ## [7] \u0026#34;osm_multilines\u0026#34; \u0026#34;osm_multipolygons\u0026#34;  we_foot  ## Object of class \u0026#39;osmdata\u0026#39; with: ## $bbox : 35.8899859,-79.0952611,35.9299859,-79.0552611 ## $overpass_call : The call submitted to the overpass API ## $meta : metadata including timestamp and version numbers ## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 17707 points ## $osm_lines : \u0026#39;sf\u0026#39; Simple Features Collection with 2793 linestrings ## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 44 polygons ## $osm_multilines : NULL ## $osm_multipolygons : NULL  We can see that there are points, lines and polygons but no multi-lines and multipolygons. First we need to change polygons to lines. We do not need to worry about the points. The points are the points that make up the lines and polygons.\nWe can look at the location of those polygons.\nwe_foot$osm_polygons %\u0026gt;% st_geometry() %\u0026gt;% plot()  We are only considering points and lines for this spatial network analysis. We can cast the polygons to lines.\n# cast polygons to lines poly_to_lines \u0026lt;- st_cast(we_foot$osm_polygons, \u0026#34;LINESTRING\u0026#34;)  ## Warning in st_cast.sf(we_foot$osm_polygons, \u0026#34;LINESTRING\u0026#34;): repeating attributes ## for all sub-geometries for which they may not be constant  # bind all lines together we_foot_lines \u0026lt;- bind_rows(we_foot$osm_lines, poly_to_lines)  # plot it we_foot_lines %\u0026gt;% st_geometry() %\u0026gt;% plot()  Cleaning the Network Mathematically a network/graph is a collection of links and nodes. For many applications, what really matters is the connections and relationships among these links and nodes, but not the spatial attributes these links and nodes. In other words, topological relationships are more important than spatial relationships. See for example, the Input Output matrix of various industries, or social network of different individuals or networks of neurons in brain.\nHowever, in spatial networks, the location of the roads and street intersection matter. Furthermore, the representation of streets as lines (straight, or multilines, curved roads), their directionality (one-way, two-way), the precision (e.g.¬†dangling, snaps), crossings (at grade, overpass etc.), road type (limited access, pedestrian etc.) all matter for how we construct what are links and which nodes ‚Ä¶","date":1666742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666807773,"objectID":"c4012cbe25ad5fdbf9dd85b696b3142d","permalink":"https://nkaza.github.io/post/cleaning-using-spatial-networks/","publishdate":"2022-10-26T00:00:00Z","relpermalink":"/post/cleaning-using-spatial-networks/","section":"post","summary":"Introduction Some transportation networks are intuitive to understand as networks. We can easily visualize intersections as nodes and roadways as edges in a road network.","tags":["R","teaching"],"title":"Cleaning Spatial Networks","type":"post"},{"authors":["J. Wang","N. Kaza","N. McDonald","K. Khanal"],"categories":[],"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"eb9ef935ca39ed8d6d137fcbb8056071","permalink":"https://nkaza.github.io/publication/wang-socioeconomic-disparities-activitytravel-2022/","publishdate":"2021-02-18T16:49:57.936915Z","relpermalink":"/publication/wang-socioeconomic-disparities-activitytravel-2022/","section":"publication","summary":"The COVID-19 pandemic significantly affected human mobility. This study examines the changes in activity-travel behavior over 23 months (from Jan 2020 to Nov 2021) and how these changes are associated with the socio-economic status (SES) at the block group level in North Carolina. We identified 5 pandemic stages with different restriction regimes: the pre-pandemic, lockdown, reopening stage, restriction, and complete opening stage. Using the block-group mobility data from SafeGraph, we quantify visits to 8 types of destinations during the 5 stages. We construct regression models with interaction terms between SES and stages and find that visit patterns during the pandemic vary for different types of destinations and SES areas. Specifically, we show that visits to retail stores have a slight decrease for low and medium SES areas, and visits to retail stores and restaurants and bars bounced back immediately after the lockdown for all SES areas. The results suggest that people in low SES areas continued traveling during the pandemic. Transportation planners and policymakers should carefully design the transportation system to satisfy travel needs of those residents. Furthermore, the results also highlight the importance of designing mitigation policies that recognize the immediate recovery of visits to retail locations, restaurants, and bars.","tags":["COVID-19","Equity","Mobile device data","Mobility","Travel behavior"],"title":"Socio-Economic Disparities in Activity-Travel Behavior Adaptation during the COVID-19 Pandemic in North Carolina","type":"publication"},{"authors":["J. Becker","N. Kaza"],"categories":["energy","urban-form"],"content":"","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"7009ecda26daee1d547282a3c3ddb4f6","permalink":"https://nkaza.github.io/publication/becker-2020/","publishdate":"2022-02-18T16:49:55.893749Z","relpermalink":"/publication/becker-2020/","section":"publication","summary":"Smart growth principles have focused on land use and transportation connections while ignoring the underlying energy infrastructure. In this chapter we explore ways to decrease energy consumption and increase renewable generation while upholding the goals of Smart Growth. Smart Growth policies promoting compact development can lessen transportation and building energy use. However, decarbonisation of the energy systems has the potential to create its own sprawl due to low energy density of renewables. In this chapter, we explore the challenges and opportunities associated with integrating energy planning principles into regional urban growth management. Land use guidelines can be used to lessen the institutional and physical barriers to reducing fossil fuel consumption and energy sprawl.","tags":[],"title":"Tale of Two Sprawls: Energy planning and challenges for Smart Growth 2.0","type":"publication"},{"authors":[],"categories":["transportation","new-urban-analytics"],"content":"Introduction In other posts, I have shown how to use external servers (google, craigslist etc.) to access information. You can also query a local (on your computer) server and your R session acts like a client (e.g. browser).\nWe can demonstrate this using (Open Source Routing Machine) OSRM server and constructing Isocrhrones. Isochrones are area you can reach from a point within a specified time. Often isochrones are used to identify gaps in the service area boundaries (e.g. 15 min distance from fire stations).\nWe can plot isochornes of every 2 min biking, around some random points in Orange County, NC. While we use OSRM, though any other API works as well (e.g. Google, Mapbox etc. see this blog for example.) . If you want to set other types of local routing engine, Valhalla and associated R packages might be a good option. Use whichever one suits your needs.\n I am using a OSRM server. T   Setting up a OSRM server We are going to set up a OSRM server on your computer.\n  Different OS require different instructions, so please follow the website to construct the backend for your OS.\n  Download North Carolina road network from Openstreetmap from Geofabrik for example (use the pbf file).\n  Construct a graph that can be used for finding shortest routes. Use suitably modified versions of (mind the file locations)\n   - osrm-extract nc.osm.pbf -p ./profiles/bicycle.lua - osrm-partition nc.osrm - osrm-customize nc.osrm - osrm-routed --algorithm mld nc.osrm  in the command window/terminal (outside R). If all goes well, this sequence of steps will create local server ready to accept your requests. The server will be ready to be queried at http://localhost:5000/ or http://127.0.0.1:5000\nTo get other modes, simply change the lua profile.\nThe following code is here for the sake of completeness and is not evaluated. it can only be evaluated with the server is runnign in the background at localhost:5000\nlibrary(osrm) library(tidyverse) library(sf) library(leaflet) library(widgetframe) # Ideally set these options up options(osrm.server = \u0026#34;http://localhost:5000/\u0026#34;) options(osrm.profile = \u0026#39;bike\u0026#39;) randompoints \u0026lt;- matrix(c( -79.065443, 35.924787, -79.087353, 35.914525, -79.066203, 35.881521), ncol=2, byrow =TRUE) %\u0026gt;% data.frame() names(randompoints) \u0026lt;- c(\u0026#39;lng\u0026#39;, \u0026#39;lat\u0026#39;) randompoints$name \u0026lt;- c(\u0026#39;pt1\u0026#39;, \u0026#39;pt2\u0026#39;, \u0026#39;pt3\u0026#39;) rt \u0026lt;- osrmRoute(src = randompoints[1,c(\u0026#39;name\u0026#39;, \u0026#39;lng\u0026#39;,\u0026#39;lat\u0026#39;)], dst = randompoints[2,c(\u0026#39;name\u0026#39;,\u0026#39;lng\u0026#39;,\u0026#39;lat\u0026#39;)], returnclass = \u0026#34;sf\u0026#34;) m1 \u0026lt;- rt %\u0026gt;% leaflet() %\u0026gt;% addProviderTiles(providers$Stamen.TonerLines, group = \u0026#34;Basemap\u0026#34;) %\u0026gt;% addProviderTiles(providers$Stamen.TonerLite, group = \u0026#34;Basemap\u0026#34;) %\u0026gt;% addMarkers(data=randompoints[1:2,], ~lng, ~lat) %\u0026gt;% addPolylines(weight =5, smoothFactor = .5, color=\u0026#39;red\u0026#39;) frameWidget(m1)  OSRM is a convenience package that is wrapping the calls to the server and parsing the output into sf classes. For example, the curl query in the backend looks like\nhttp://localhost:5000/route/v1/biking/-78.901330,36.002806,-78.909020,36.040266\nand should result in the following image\n   You can always shut the server down by closing the terminal/command window, after you are done with your analysis.\nTo get a matrix of distances and durations between multiple points,\nttmatrix \u0026lt;- osrmTable(loc=randompoints[, c(\u0026#39;name\u0026#39;, \u0026#39;lng\u0026#39;, \u0026#39;lat\u0026#39;)], measure = c(\u0026#39;duration\u0026#39;, \u0026#39;distance\u0026#39;))  ttmatrix$durations should return a table similar to\n    pt1 pt2 pt3     pt1 0.0 22.8 32.5   pt2 22.8 0.0 25.1   pt3 32.9 25.8 0.0    Querying a OSRM server iso \u0026lt;- list() for (i in 1:nrow(randompoints)){ iso[[i]] \u0026lt;- osrmIsochrone(loc = randompoints[i,c(\u0026#39;lng\u0026#39;,\u0026#39;lat\u0026#39;)], breaks = seq(from = 0,to = 15, by = 2)) %\u0026gt;% st_as_sf() } iso \u0026lt;- do.call(\u0026#39;rbind\u0026#39;, iso) Npal \u0026lt;- colorNumeric( palette = \u0026#34;Reds\u0026#34;, n = 5, domain = iso$center ) iso %\u0026gt;% leaflet() %\u0026gt;% addProviderTiles(providers$Stamen.TonerLines, group = \u0026#34;Basemap\u0026#34;) %\u0026gt;% addProviderTiles(providers$Stamen.TonerLite, group = \u0026#34;Basemap\u0026#34;) %\u0026gt;% addMarkers(data=randompoints, ~lng, ~lat) %\u0026gt;% addPolygons(color = \u0026#34;#444444\u0026#34;, weight = 1, smoothFactor = 0.5, opacity = 1.0, fillOpacity = 0.5, fillColor = ~Npal(iso$center), group = \u0026#34;Isochrone\u0026#34;) %\u0026gt;% addLegend(\u0026#34;topleft\u0026#34;, pal = Npal, values = ~iso$center, title = \u0026#34;Biking Time (min)\u0026#34;,opacity = 1 )  The result should looks similar to the following.\n    Exercise\n Change the profile of the OSRM server to car and notice the differences in the Isochrones (shapes and extents). Repeat the entire exercise for all the major shopping areas in the Triangle.   Conclusions It should be obvious that having a local server might reduce latency and allow for quicker turnaround. However, there are some distinct disadvantages compared to commercial option, including but not limited to incorporating real time information, setup costs and maintenance costs. Nonetheless, it is useful to think about how these might be beneficial within an organisation.\n","date":1643068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643145367,"objectID":"f1ba9d1199c9abf2ce8287c60ad6ced1","permalink":"https://nkaza.github.io/post/isochrones-from-routing-engines-osrm/","publishdate":"2022-01-25T00:00:00Z","relpermalink":"/post/isochrones-from-routing-engines-osrm/","section":"post","summary":"Introduction In other posts, I have shown how to use external servers (google, craigslist etc.) to access information. You can also query a local (on your computer) server and your R session acts like a client (e.","tags":["R","teaching"],"title":"Isochrones from Routing Engines (OSRM)","type":"post"},{"authors":[],"categories":["new-urban-analytics","air-quality"],"content":"                        Updated on 2025-08-14.\nGetting started with real data In this tutorial, we are going to summarise Air Quality Index for about 4,000 stations in the US in 2017. These datasets are downloaded from the EPA website. A local copy of the dataset is here\nlibrary(tidyverse) ozone_airq_df \u0026lt;- here(\u0026#39;tutorials_datasets\u0026#39;, \u0026#34;aqi\u0026#34;, \u0026#34;daily_44201_2017.csv\u0026#34;) %\u0026gt;% read_csv() #Change file paths accordingly dim(ozone_airq_df) # [1] 402501 29 names(ozone_airq_df) # [1] \u0026#34;State Code\u0026#34; \u0026#34;County Code\u0026#34; \u0026#34;Site Num\u0026#34; # [4] \u0026#34;Parameter Code\u0026#34; \u0026#34;POC\u0026#34; \u0026#34;Latitude\u0026#34; # [7] \u0026#34;Longitude\u0026#34; \u0026#34;Datum\u0026#34; \u0026#34;Parameter Name\u0026#34; # [10] \u0026#34;Sample Duration\u0026#34; \u0026#34;Pollutant Standard\u0026#34; \u0026#34;Date Local\u0026#34; # [13] \u0026#34;Units of Measure\u0026#34; \u0026#34;Event Type\u0026#34; \u0026#34;Observation Count\u0026#34; # [16] \u0026#34;Observation Percent\u0026#34; \u0026#34;Arithmetic Mean\u0026#34; \u0026#34;1st Max Value\u0026#34; # [19] \u0026#34;1st Max Hour\u0026#34; \u0026#34;AQI\u0026#34; \u0026#34;Method Code\u0026#34; # [22] \u0026#34;Method Name\u0026#34; \u0026#34;Local Site Name\u0026#34; \u0026#34;Address\u0026#34; # [25] \u0026#34;State Name\u0026#34; \u0026#34;County Name\u0026#34; \u0026#34;City Name\u0026#34; # [28] \u0026#34;CBSA Name\u0026#34; \u0026#34;Date of Last Change\u0026#34;  If library(.) command returns an error, it is likely that you do not have the package installed on your computer. Install them by e.g.¬†install.packages(tidyverse). You only need to install packages once. But you need to call them into your library in every R session.\n Zhu Wang: I am trying to create a library which uses some Fortran source files [‚Ä¶]\nDouglas Bates: Someone named Martin Maechler will shortly be sending you email regarding the distinction between ‚Äòlibrary‚Äô and ‚Äòpackage‚Äô :-)\n -- Zhu Wang and Douglas Bates: R-help (May 2004)   If read_csv results in an error, it is very likely that you got the path to the file wrong. Make sure that the data file exists at the path specified. File paths are tricky, especially when you want your code to be portable among different operating systems or even different computers. Three rules of thumb will help you with some headaches, later on.\n Use file.path() function to create platform independent file paths. Use relative paths Assign a variable to the string (e.g.¬†input_file_path) and use it in the functions (such as read_csv(input_file_path)) Better still use the here function from the here package and use R projects framework.   Exercise\n Use the above rules to rewrite the code for reading the input data file. If you have not used R project to start with, go back and follow the instructions about Rprojects and directory structure. Rewrite the code to read in the data from appropriate file path.   In the US, State and Counties are uniquely identified by a 2 and 3 digit FIPS (Federal Information Processing System) Code. Lets create it so that we can merge datasets together.\nozone_airq_df$stcofips \u0026lt;- paste(formatC(ozone_airq_df$`State Code`, width=2,flag=\u0026#34;0\u0026#34;),formatC(ozone_airq_df$`County Code`, width=3,flag=\u0026#34;0\u0026#34;), sep=\u0026#34;\u0026#34; ) head(ozone_airq_df$stcofips) #Inspect the outcome # [1] \u0026#34;01003\u0026#34; \u0026#34;01003\u0026#34; \u0026#34;01003\u0026#34; \u0026#34;01003\u0026#34; \u0026#34;01003\u0026#34; \u0026#34;01003\u0026#34; tail(ozone_airq_df$stcofips) # [1] \u0026#34;80026\u0026#34; \u0026#34;80026\u0026#34; \u0026#34;80026\u0026#34; \u0026#34;80026\u0026#34; \u0026#34;80026\u0026#34; \u0026#34;80026\u0026#34; unique(nchar(ozone_airq_df$stcofips)) # Making sure all fips codes are of length 5 characters # [1] 5  What we really need is a few columns. So we extract them\nozone_airq_df2 \u0026lt;- ozone_airq_df[,c(\u0026#39;stcofips\u0026#39;, \u0026#39;Site Num\u0026#39;, \u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;, \u0026#39;Date Local\u0026#39;, \u0026#39;State Name\u0026#39;, \u0026#39;County Name\u0026#39;, \u0026#39;CBSA Name\u0026#39;, \u0026#39;AQI\u0026#39;)] summary(ozone_airq_df2) # stcofips Site Num Latitude Longitude # Length:402501 Length:402501 Min. :18.18 Min. :-158.09 # Class :character Class :character 1st Qu.:34.10 1st Qu.:-110.10 # Mode :character Mode :character Median :38.34 Median : -91.53 # Mean :37.64 Mean : -95.33 # 3rd Qu.:41.06 3rd Qu.: -82.17 # Max. :64.85 Max. : -65.92 # Date Local State Name County Name CBSA Name # Min. :2017-01-01 Length:402501 Length:402501 Length:402501 # 1st Qu.:2017-04-12 Class :character Class :character Class :character # Median :2017-06-30 Mode :character Mode :character Mode :character # Mean :2017-06-30 # 3rd Qu.:2017-09-19 # Max. :2017-12-31 # AQI # Min. : 0.00 # 1st Qu.: 31.00 # Median : 38.00 # Mean : 40.13 # 3rd Qu.: 45.00 # Max. :233.00 nrow(unique(ozone_airq_df2[,c(\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;)])) # [1] 1265 length(unique(ozone_airq_df2$stcofips)) # [1] 786  i.e.¬†There are 1,265 unique monitoring station locations in 786 counties. Roughly, on average 1.61 per county. However, there are about ~3100 counties in the US, suggesting that large parts of the US are not covered by the monitoring stations.\nIt is useful to rewrite the last command as following, using a pipe operator %\u0026gt;%\nozone_airq_df2$stcofips %\u0026gt;% unique() %\u0026gt;% length() # [1] 786  Following this pipe operator mode helps avoid a lot of headaches associated with orphan brackets and make the code cleaner and easier to read.\nYou can also follow the tidyverse syntax.\nozone_airq_df3 \u0026lt;- select(ozone_airq_df, `stcofips`, `Site Num`, `Latitude`, `Longitude`, `Date Local`, `State Name`, `County Name`, `CBSA Name`, `AQI`) #W e only needed `` because of spaces in column names. all.equal(ozone_airq_df2, ozone_airq_df3) # [1] TRUE rm(ozone_airq_df3)  select is more powerful than [ ‚Ä¶","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1755129600,"objectID":"aa4c96dbae01e8485671be3bfcce3d03","permalink":"https://nkaza.github.io/post/exploratory-data-analysis-and-visualisation/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/post/exploratory-data-analysis-and-visualisation/","section":"post","summary":"Some exploratory graphics using air pollution data in the US.","tags":["R","teaching"],"title":"Exploratory Data Analysis and Visualisation","type":"post"},{"authors":[],"categories":["new-urban-analytics","urban-form"],"content":"Introduction  This post is inspired by Geoff Boeing‚Äôs OSMnx python package. In addition, I drew some notes from Ujaval Gandhi‚Äôs QGIS tutorial.   Often one of the key indicators for measuring walkability of a place is the intersection density of road network. In grid networks, roads intersect at regular intervals and are likely to result in more walkable paths than cul-de-sac patterns. Thus planners often use this metric to to measure urban form (see e.g.¬†EPA‚Äôs EnviroAtlas). In this post, I am going to demonstrate how to calculate this measure for any place in the world where Open Street Map has a reasonably detailed and accurate street network.\n I am going to demonstrate how to use QGIS and R together, though there is no real need to use QGIS for this particular task. It could all be done in R. However, there might be some situations where combining the GUI of QGIS and extensive code base and algorithms in R might helpful. Hence this approach.   Plugins \u0026amp; Packages In this post, I am going to use plugins for QGIS called QuickOSM and Processing R Provider. They can be downloaded and installed from Plugins \u0026gt; Manage and Install Plugins. As of writing, both these plugins are available in the official QGIS repository.\nYou will need to make sure that path to the R executables and R libraries are known to the Processing framework. Please follow the instructions located on the plugin‚Äôs website\nWithin R, in addition to tidyverse and sf suite of packages, we are going to use tidygraph and sfnetworks. Please download and install them.\nWhat is Processing? Since QGIS 2.0, the Processing Framework provides an environment within QGIS to run native and third-party algorithms for processing data. For example, you can run powerful hydrological algorithms from GRASS GIS or remote sensing algorithms from Orfeo Toolbox within QGIS. In addition to individual tools, you can also ability to batch process them and create graphical models to build a workflow, e.g.¬†outputs of one algorithm as inputs to another. A good introduction to Processing is from Ujaval‚Äôs website.\nDownload Data from OSM OpenStreetMap (OSM) is a collaborative project to create a free editable geographic database of the world. For the purposes of this tutorial, we are simply going to use the data from this project rather than contributing to it.\n While the OSM data is comparable to proprietary data sources (e.g.Google Maps and Bing) in its coverage and accuracy, it should be noted that as a collaborative project it has its own unique biases. See for example,\n Das, Maitraye, Brent Hecht, and Darren Gergle. 2019. ‚ÄúThe Gendered Geography of Contributions to OpenStreetMap: Complexities in Self-Focus Bias.‚Äù In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 1‚Äì14. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3290605.3300793.    In the following short video I demonstrate how to download the street network for Accra, Ghana using the QuickOSM plugin. QuickOSM allows us to download OSM data for small areas. To quickly figure out an extent of a small area, I am going to use the subnational administrative boundaries of Ghana downloaded from the Humanitarian Data Exchange. I am primarily focused on Admin Level 2 boundaries. Add them to the project.\nOnce you added the polygons, select some polygons around Accra to specify the extent of the OSM download. Note that, selecting a large area will overwhelm the OSM servers and will not result in an output. Be judicious.\n If you are unable to read the menu items, please turn on HD in the video quality settings for YouTube.     Once you have downloaded and added the road network into QGIS, you are ready to use Processing framework to run an R script that counts the intersections. In the following video I give a quick overview of how you might run it and in the rest of the tutorial, I am going to explain the script in detail.\n  The script that is used in the video is the following.\n##QgsProcessingParameterFeatureSource|INPUT|Lines|1 ##Use_Spatial_Subdivision=boolean TRUE ##Use_Spatial_Smooth=boolean TRUE ##Use_Spatial_Contraction=boolean FALSE ##QgsProcessingParameterNumber|EPS|Clustering Parameter (Only used when Contraction is selected) |QgsProcessingParameterNumber.Integer|30 ##Grid_Type=enum literal Fishnet;Hexabin ##intersections=output vector ##Output_Grid=output vector library(sf) library(tidyverse) library(tidygraph) library(sfnetworks) net \u0026lt;- INPUT%\u0026gt;%st_as_sf()%\u0026gt;% as_sfnetwork(directed = FALSE) node_coords \u0026lt;- net %\u0026gt;% activate(\u0026#34;nodes\u0026#34;) %\u0026gt;% st_coordinates() node_coords_x_mean \u0026lt;- mean(node_coords[,1], na.rm=T) node_coords_y_mean \u0026lt;- mean(node_coords[,2], na.rm=T) UTMzone \u0026lt;- floor((node_coords_x_mean + 180)/6 %% 60) + 1 EPSGcode \u0026lt;- ifelse (node_coords_y_mean\u0026gt;=0, 32600+UTMzone, 32700+UTMzone) if(Use_Spatial_Subdivision){ net \u0026lt;- tidygraph::convert(net, to_spatial_subdivision) } if(Use_Spatial_Smooth){ net \u0026lt;- tidygraph::convert(net, to_spatial_smooth) } ‚Ä¶","date":1642723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642807991,"objectID":"44c3b39a1c234911b1526022c13d2850","permalink":"https://nkaza.github.io/post/intersection-density-from-osm-using-qgis-r/","publishdate":"2022-01-21T00:00:00Z","relpermalink":"/post/intersection-density-from-osm-using-qgis-r/","section":"post","summary":"Intersection Density is often used as an indicator of urban form in travel behaviour and urban design. In this post, I detail a way to calculate this indicator by leveraging OpenStreetMap data. I also focus on how to link R \u0026 QGIS as an illustration. ","tags":["R","teaching"],"title":"Intersection Density From OSM using QGIS \u0026 R","type":"post"},{"authors":["Kshitiz Khanal","Nikhil Kaza","Noah Kittner"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"98122870069762bf2ccfeda3c82a657b","permalink":"https://nkaza.github.io/publication/khanal-2022-aa/","publishdate":"2024-09-15T09:22:26.233203Z","relpermalink":"/publication/khanal-2022-aa/","section":"publication","summary":"","tags":[],"title":"Evaluating generalisation of deep learning computer vision models for satellite data and global development","type":"publication"},{"authors":["Nikhil Kaza"],"categories":null,"content":"Why are many plans not implemented? Common explanations are planners have little power, they fail to account for political or environmental uncertainty in the plans or they failed to include enough voices during the planning process. The theoretical frameworks on which we base our understanding of plans focus on implementation as a key evaluative mechanism. I challenge the premise that plans realise their potential only when they are implemented. Monitoring implementation of plans presupposes that we know what plans there are to monitor. Such monitoring privileges published plans and ignores all the other plans that guide urban development. It assumes that the decision situations in which plans are used are observable. By jettisoning implementation as a key criterion by which to evaluate the effectiveness of plans, we can begin to focus on the myriad ways in which plan makers and others use plans. We can instead ask, ‚ÄòHow are these plans used? Who uses them? When are they useful? How to make useful plans?‚Äô With these questions, we can create different evaluative frameworks for different types of plans. Some unimplementable plans are worth making.\n","date":1638968400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638968400,"objectID":"9c975d8f2800c665dc97dee9904f6701","permalink":"https://nkaza.github.io/talk/against-the-idea-of-implementation-in-planning/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/talk/against-the-idea-of-implementation-in-planning/","section":"event","summary":"We need to move beyond implementation to measure the usefulness of plans","tags":["planning-theory"],"title":"Against the Idea of Implementation in Planning","type":"event"},{"authors":["Chaosu Li","Yan Song","Nikhil Kaza","Rene Burghardt"],"categories":[],"content":"","date":1613666997,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"2d8a7aa89df4838803afacd19392886e","permalink":"https://nkaza.github.io/publication/li-2018/","publishdate":"2021-02-18T16:49:57.083693Z","relpermalink":"/publication/li-2018/","section":"publication","summary":"While urban form affects building energy consumption, the pathways, direction and magnitude of the effect are disputed in the literature. This paper uses a unique dataset to examine the effect of urban form on residential electricity consumption in Ningbo, China. Using survey and utility bill data of 534 households in 46 neighborhoods in the city, we model the electricity use of households using a multi-level regression model. We find that neighborhood street configuration and tree shade are important in controlling residential electricity consumption and, consequently, greenhouse gas emissions. Our results suggest that seasonality and dwelling type condition the effect of neighborhood densities on electricity consumption. Neighborhood density is associated with household electricity consumption in summer months, while there is no such association in the winter months. As neighborhood density increases, households in slab and tower apartments in dense urban neighborhoods consume more electricity in summer months, which can be partly explained by exacerbated heat island effect. Interestingly, the neighborhood density is negatively associated with electricity consumption for single-family houses, suggesting that the effect of neighborhood density is different for different types of dwelling units.","tags":[],"title":"Explaining spatial variations in residential energy usage intensity in Chicago: The role of urban form and geomorphometry","type":"publication"},{"authors":["N. Kaza","K. Nesse"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666995,"objectID":"025a425edf8c225a22235feedb02eef9","permalink":"https://nkaza.github.io/publication/kaza-2019-aa/","publishdate":"2021-02-18T16:49:55.613951Z","relpermalink":"/publication/kaza-2019-aa/","section":"publication","summary":"Categorizing places based on their network connections to other places in the regiom reveals not only population concentration but also economic dynamics that are missed in other typologies. The US Office of Management and Budget categorization of counties into metropolitan/micropolitan central/outlying is widely seen as insufficient for many analytic purposes. In this paper we use coreness index from network analysis to identify labor market centrality of a county. We use county-to-county commute flows, including internal commuting, to identify regional hierarchies. Indicators broken down by this typology reveal counterintuitive results in many cases. Population size and level of urbanisation is largely irrelevant to strong core counties. Employment in these strong core counties grew faster in the post-recession (2008-2015) than other types, that is missed by other typologies, suggesting that this categorization may be useful for regional analysis and policy. ","tags":[],"title":"Characterizing the Regional Structure in United States: A county-based analysis of labor market centrality","type":"publication"},{"authors":["Jordan Branham","Nikhil Kaza","Todd K BenDor","Kyle Onda"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"f7f29aa4d7a99141f10c7cebc2322fae","permalink":"https://nkaza.github.io/publication/branham-2021-aa/","publishdate":"2024-09-15T09:22:26.492203Z","relpermalink":"/publication/branham-2021-aa/","section":"publication","summary":"","tags":[],"title":"Does Subsidy Removal Reduce Coastal Development? Measuring the Effect of US Coastal Barrier Resources Act","type":"publication"},{"authors":["J. Branham","K. Onda","N. Kaza","T. K. BenDor","D. Salvesen"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666995,"objectID":"70047d073b6b54a96c15520f098e23c7","permalink":"https://nkaza.github.io/publication/branham-2021-cb/","publishdate":"2021-02-18T16:49:55.746634Z","relpermalink":"/publication/branham-2021-cb/","section":"publication","summary":"Shoreline armoring, which involves the installation of hardened structures to protect coastal property, dramatically alters shoreline composition and resulting ecological functions. Accelerating hazard threats to growing coastal communities compounds this problem, creating demand for more armoring. We examine whether designation by the U.S. Coastal Barrier Resources Act (CBRA) -- enacted to disincentivize urban development on hazardous coastal barriers -- is associated with lower propensities to armor shorelines. In designated areas, CBRA removes access to federally-subsidized flood insurance, infrastructure subsidies, and disaster assistance. Using logistic regression modeling, we examine armoring at the parcel scale across the State of Florida (USA), controlling for CBRA designation, land use, and local population density. Our findings reveal a significant negative relationship between CBRA designation and the odds of armoring, particularly for residential and vacant properties. As coastal areas grapple with increasing impacts from coastal hazards, removal of public subsidies may be an effective non-regulatory method for maintaining the ecological and protective benefits of natural shorelines.","tags":[],"title":"How does the removal of federal subsidies affect investment in coastal protection infrastructure?","type":"publication"},{"authors":["Miyuki Hino","Todd K BenDor","Nikhil Kaza","Antonia Sebastian","Jordan Branham","Shane Sweeney"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"d644a90ad264fe169add50f1e1e6c4c3","permalink":"https://nkaza.github.io/publication/hino-2021-ab/","publishdate":"2024-09-15T09:22:26.783615Z","relpermalink":"/publication/hino-2021-ab/","section":"publication","summary":"","tags":[],"title":"One Step Forward, Two Steps Back: Managing Floodplain Development in North Carolina","type":"publication"},{"authors":["Miyuki Hino","Todd K BenDor","Nikhil Kaza","Antonia Sebastian","Jordan Branham","Shane Sweeney"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"8d6d9d5afe8373f06f9470fddc7a2982","permalink":"https://nkaza.github.io/publication/hino-2021-aa/","publishdate":"2024-09-15T09:22:26.639591Z","relpermalink":"/publication/hino-2021-aa/","section":"publication","summary":"","tags":[],"title":"One Step Forward, Two Steps Back: Managing Floodplain Development in North Carolina (Invited)","type":"publication"},{"authors":["Will Curran-Groome","Nikhil Kaza"],"categories":["new-urban-analytics"],"content":"      Introduction In this post, I am going to introduce basic methods for analysing free form (i.e., unstructured) text. In a different post, we analysed some [free form text that is somewhat semi-structured] (/post/matching-messy-texts/) (e.g., addresses, firm names, etc.). In this post, we will expand on some of those techniques and explicitly focus on quantifying frequencies of words, topics, and sentiments. Free form texts are ubiquitous in the planning field. For example, you may want to understand public sentiment around a proposed project by analysing tweets or public comments. Or you may want to analyse newspaper articles and blogs to understand trending topics. The possibilities are endless.\nHowever, by its very nature, unstructured text and natural language is very hard to pin down in numbers. Even things that are amenable to quantification (e.g., frequencies of words) lose context‚Äìand thus meaning and nuance‚Äìwhen quantified. Qualitative analysis of text is always more meaningful than reductive quantitative analyses. Nonetheless, quantitative approaches are useful for examinining large bodies of text (also referred to as a ‚Äúcorpus‚Äù of texts), and they can provide some advantages over qualitative analyses, including replicability and scalability.\nAcquire Data and Packages This post draws heavily from Will Curran-Groome‚Äôs final project for the Urban Analytics course in Spring 2020. We are going to analyse emails from a listserv (Cohousing-L) that focuses on cohousing. Cohousing is an intentional community of private homes clustered around shared space with some shared norms about voluntary contributions, management, and governance structures. US cohousing communities often comprise both rental and owner-occupied units; they frequently are multi-generational; they leverage existing legal structures, most often the home owner association (HOA), but the lived experience is often very different from that of conventional HOAs; and they also reflect a diversity of housing types, including apartment buildings, side-by-side duplexes and row homes, and detached single-family units.\nWeb-scraped emails (~45,000) and community characteristics are available here.\nIn this tutorial, we are going to use packages such as tidytext, textclean, and sentimentr in addition to other packages we have used previously. Please install them and call them into your library appropriately.\n I am using the library calls to packages where I need them for pedagogical purposes. In general, you want to put all your library calls at the top of the script. Please pay particular attention to conflicts in the functions of the same name in different packages. Packages that are loaded later will take precedence over ones that are loaded earlier. If you want use the function from an earlier loaded package you can use packagename::function().   library(ids) library(lubridate) library(textclean) msgs \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;cohousingemails\u0026#34;, \u0026#34;cohousing_emails.csv\u0026#34;) %\u0026gt;% read_csv() str(msgs) # spec_tbl_df [45,000 √ó 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) # $ subject : chr [1:45000] \u0026#34;Test message 10/22/92 6:37 pm\u0026#34; \u0026#34;test message 10/22/92 6:47 pm\u0026#34; \u0026#34;Places to announce COHOUSING-L\u0026#34; \u0026#34;Discussion style\u0026#34; ... # $ author : chr [1:45000] \u0026#34;Fred H Olson -- WB0YQM\u0026#34; \u0026#34;STAN\u0026#34; \u0026#34;Fred H Olson -- WB0YQM\u0026#34; \u0026#34;Fred H Olson -- WB0YQM\u0026#34; ... # $ email : chr [1:45000] NA NA NA NA ... # $ date : POSIXct[1:45000], format: NA NA ... # $ msg_body: chr [1:45000] \u0026#34;This is a test message. Fred\u0026#34; \u0026#34;Thes message was set to the list with the address in all caps: COHOUSING-L [at] UCI.COM Fred\u0026#34; \u0026#34;Judy, below is my list of places to announce the list including the Inovative Housing newsletter. Can you dig u\u0026#34;| __truncated__ \u0026#34;Another topic is what tone, style or whatever should we encourage on this list. Some discussions people introdu\u0026#34;| __truncated__ ... # $ thread : chr [1:45000] NA NA NA NA ... # $ content : chr [1:45000] \u0026#34;\\n\\n\\n\\n\\n\\n\\n\\n\\nTest message 10/22/92 6:37 pm\\n\\t¬†\u0026lt;‚Äì¬†Date¬†‚Äì\u0026gt;¬†\u0026lt;‚Äì¬†Thread¬†‚Äì\u0026gt;\\n\\n\\tFrom: Fred H Olson -- WB0Y\u0026#34;| __truncated__ \u0026#34;\\n\\n\\n\\n\\n\\n\\n\\n\\ntest message 10/22/92 6:47 pm\\n\\t¬†\u0026lt;‚Äì¬†Date¬†‚Äì\u0026gt;¬†\u0026lt;‚Äì¬†Thread¬†‚Äì\u0026gt;\\n\\n\\tFrom: STAN (STAN%MNHEPvx.c\u0026#34;| __truncated__ \u0026#34;\\n\\n\\n\\n\\n\\n\\n\\n\\nPlaces to announce COHOUSING-L\\n\\t¬†\u0026lt;‚Äì¬†Date¬†‚Äì\u0026gt;¬†\u0026lt;‚Äì¬†Thread¬†‚Äì\u0026gt;\\n\\n\\tFrom: Fred H Olson -- WB0\u0026#34;| __truncated__ \u0026#34;\\n\\n\\n\\n\\n\\n\\n\\n\\nDiscussion style\\n\\t¬†\u0026lt;‚Äì¬†Date¬†‚Äì\u0026gt;¬†\u0026lt;‚Äì¬†Thread¬†‚Äì\u0026gt;\\n\\n\\tFrom: Fred H Olson -- WB0YQM (FRED%JWHv\u0026#34;| __truncated__ ... # - attr(*, \u0026#34;spec\u0026#34;)= # .. cols( # .. subject = col_character(), # .. author = col_character(), # .. email = col_character(), # .. date = col_datetime(format = \u0026#34;\u0026#34;), # .. msg_body = col_character(), # .. thread = col_character(), # .. content = col_character() # .. ) # - attr(*, \u0026#34;problems\u0026#34;)=\u0026lt;externalptr\u0026gt; msgs2 \u0026lt;- msgs %\u0026gt;% mutate_all(as.character) %\u0026gt;% ## I (Will) added this because I was getting an error stemming from read_csv() returning all factor variables. filter(!is.na(content)) %\u0026gt;% mutate( msg_id = random_id(n = nrow(.)), # Create a random ID email = case_when( ‚Ä¶","date":1598832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598878102,"objectID":"b7878f89c29ffda967e09e9a7b9880d0","permalink":"https://nkaza.github.io/post/analysing-free-form-text/","publishdate":"2020-08-31T00:00:00Z","relpermalink":"/post/analysing-free-form-text/","section":"post","summary":"Free form text is ubiquitous in planning, including plans, regulations, meeting minutes, and transcripts, etc. This is a short introduction to analysing such text quantitatively.","tags":["R","teaching"],"title":"Analysing Free Form Text","type":"post"},{"authors":null,"categories":["new-urban-analytics","data-munging"],"content":"The Problems of Free Form Text One of the joys of language is that there is an infinite variety of ways in which we can express ourselves. Conventions vary. People are likely to run afoul of grammatical and spelling conventions depending on their background, attention to detail and adherence to convention. This expressive variety becomes a disadvantage for a data analyst who wants to analyse text. Urban datasets frequently contain free form language; names, addresses, surveys, regulations, complaints, tweets, posts, newspaper reports, plans, interviews, ordinances etc. When the text is not standardised and restricted, they pose problems for a data analyst who is trying to reduce and distill the information, often ignoring the context and conventions that the gives meaning to the text. The text is then wrangled, standardised or ignored. In this blog, I will show how one might attempt to wrangle text. I also want to highlight the decisions that the analyst makes that severely distort the meaning and introduce errors.\n All clean datasets are alike. But each messy dataset is messy in its own way ‚Äî Prener paraphrasing Wickham paraprhasing Tolstoy\n Data \u0026amp; Required Packages The Worker Adjustment and Retraining Notification Act (WARN) requires employers with 100 or more employees (generally not counting those who have worked less than six months in the last 12 months and those who work an average of less than 20 hours a week) to provide at least 60 calendar days advance written notice of a plant closing and mass lay-off affecting 50 or more employees at a single site of employment. In North Carolina, the Department of Commerce is in charge of collecting and archiving the notices. Research by Cleveland Federal Reserve Bank suggests that these notices are useful bellwethers for economic conditions in the state.\nIn this blog, I am going to use another business listings dataset from Infogroup called ReferenceUSA to establish additional information about the business that is listed in the WARN database. We are limiting our analysis to Mecklenberg county in North Carolina. ReferenceUSA data for the US can be obtained from UNC library. I obtained the WARN dataset from Cleveland Federal Reserve. Citation for the data is\n Krolikowski, Pawel M. and Kurt G. Lunsford. 2020. ‚ÄúAdvance Layoff Notices and Labor Market Forecasting.‚Äù Federal Reserve Bank of Cleveland, Working Paper no. 20-03. https://doi.org/10.26509/frbc-wp-202003  In this tutorial, I am going to use stringr, stringdist, postmastr and censusxy packages in addition to other packages in R. postmastr is not yet available on CRAN, but can be installed from github using\nremotes::install_github(\u0026#34;slu-openGIS/postmastr\u0026#34;)   At the time of writing, postmastr is still in beta stages. So it may be buggy. Use it with caution.   library(tidyverse) library(skimr) warn \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;textmatching\u0026#34;, \u0026#34;WARN_Mecklenburg.csv\u0026#34;) %\u0026gt;% read_csv() names(warn) # [1] \u0026#34;county\u0026#34; \u0026#34;notice_date\u0026#34; \u0026#34;received_date\u0026#34; \u0026#34;effective_date\u0026#34; # [5] \u0026#34;company\u0026#34; \u0026#34;type\u0026#34; \u0026#34;number_affected\u0026#34; \u0026#34;zipcode_city\u0026#34; # [9] \u0026#34;warn_no\u0026#34; \u0026#34;address\u0026#34;  Parsing the Addresses Most geocoders expect an address that is properly standardised (e.g. South - S, Bvld - Boulevard ) and spelling errors corrected. This is important as user entered text about addresses rely on personal and local convention rather than standardisation.\n The following sections refer to addresses within the US, following its conventions. While some of the methods will transfer to other locations, use caution to interpret and adapt .   postmastr works by following the precise sequence of the following steps.\nPreparation First there is a need to create a unique ID for unique addresses, so that only unique addresses are parsed. A quick look at the address column reveals that it has newlines \\n in some of the entries. Let‚Äôs replace it with a space. We are going to use Regular Expressions to search and replace.\nIn the following code, I am replacing all the diacritics, e.g.≈ô, √º, √©, √® by transliterating to Latin alphabet. This is not strictly necessary but is useful to remember that some place names are in Spanish in the US. Some databases store the diacritics and some don‚Äôt. Apologies to speakers of other languages.\nIt is always a good idea to have one case. We are going to use the upper case.\nwarn \u0026lt;- warn %\u0026gt;% mutate(address = str_replace_all(address, \u0026#34;[[:space:]]\u0026#34;, \u0026#34; \u0026#34;), address = stringi::stri_trans_general(address, \u0026#34;Latin-ASCII\u0026#34;), address = str_remove_all(address, \u0026#34;[[:punct:]]\u0026#34;), address = str_to_upper(address) )  library(postmastr) warn \u0026lt;- pm_identify(warn, var = \u0026#34;address\u0026#34;) warn_min \u0026lt;- pm_prep(warn, var = \u0026#34;address\u0026#34;, type =\u0026#39;street\u0026#39;) # There do not seem to be any addresses that are based on intersections. So we are using the type=street. nrow(warn) # [1] 111 nrow(warn_min) # [1] 91  You should notice the difference in the number of rows. 20 observations are dropped.\nExtract Zipcodes and States Zipcodes come in two formats. A 5 digit variety and 5-4 digit variety. ‚Ä¶","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598280078,"objectID":"16c56724db237a7d8228be50d1f47d03","permalink":"https://nkaza.github.io/post/matching-messy-texts/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/matching-messy-texts/","section":"post","summary":"Using messy text fields to create some structure out of the datasets.","tags":["R","teaching"],"title":"Matching Messy Texts","type":"post"},{"authors":null,"categories":[],"content":"The following is a transcript for a podcast as part of the series called ‚ÄúViewpoints on Resilient and Equitable Responses to the Pandemic\u0026#34; produced by Center for Urban and Regional Studies.\nIn March 2020, the New York Times reported on a smartphone application that is being rolled out in Hangzhou, China in response to the COVID-19 pandemic. The Alipay Health Code, as China‚Äôs official news media has called the system, is a project by the local government with the help of e-commerce giant Alibaba. The app displays a color code ‚Äî Green, Yellow or Red ‚Äî that indicates the health status of the owner. It is not transparent how these statuses are generated. Furthermore, the app automatically reports the information on the location to the police, who then enforce restrictions such as quarantine, access to public spaces such as transit. In other cases, drones are used to identify and chastise people not wearing face masks or are not following lockdown protocols.\nThese surveillance methods are not unique to China. In Tiruvallur, India, police require people who are subject to quarantine orders to download an app called CoBuddy onto their smartphones. This app queries the GPS location and requires the user to upload selfies at random time of the day. Facial recognition software is used to verify that the user is indeed the quarantined person. Israel‚Äôs cabinet passed a law without parliamentary approval during an emergency sitting that authorises Shin Bet, its security service, to mine the massive location database that it collects (the phone‚Äôs past movements) on an ongoing basis to trace potential contacts. The coronavirus economic relief bill, passed by US congress also included a $500 million for the Centers for Disease Control to build a ‚Äúsurveillance and data collection system‚Äù.\nThe use of surveillance has been credited with limiting the spread of coronavirus in Singapore and South Korea. And this epidemiological surveillance has a long history ranging from Spanish flu, Tuberculosis, SARS, MERS and AIDS. Much of this surveillance, however, requires complicated recall during focused interviews and painstaking sleuthing that interprets digital and non-digital traces to identify who the infected person is contact with, as they went about their daily activities. In this iteration of surveillance, we are abdicating such thick data collection for thin and large-scale data collection based on proximity of cell phones. We should heed the warnings of James Scott who vividly characterised authoritarian high-modernist tendency to create legibility from illegibility and its penchant to simplify complexity.\nThis surveillance state is built in conjunction with the surveillance economy. The advent and widespread adoption of smartphones has resulted in a platform for myriad sensors (bluetooth, microphone, GPS, radio, accelerometer to name a few). In combination with the phones‚Äô data storage and communication capabilities, these devices have become continuous and ubiquitous surveillance devices that can soak up enormous amounts of data exhaust from users. These data exhausts have routinely been used by firms to target advertisements, reframe information from search results and reprioritise relevance of news. Shosahana Zuboff argues in Surveillance Capitalism that human experience monitored, quantified through these ubiquitous sensor networks is treated as proprietary behavioural surplus over which behavioural future markets are built. She also characterises ‚Äúdispossession by surveillance‚Äù which challenges the psychological and political bases of self-determination.\nI will address the issue of the probity of this surveillance state later on, but first I want to address the issue of the effectiveness of the data. GPS traces from cellphones are notoriously unreliable for precise and persistent tracking. Location traces emitted by smartphones are best suited to identify location, on average over time, rather than a precise path ‚Äì which is required for contact tracing in an epidemiological event. For example, in a recent study, Merry and Bettinger found that horizontal position recorded by smartphones is off by as much as 100m and is highly dependent on both the phone settings, environmental and built form features and time of day. Location accuracy in indoor settings is even more challenging. Recent attempts by Apple and Google to create a decentralized and automated tracing system based on Bluetooth has been criticised by security experts for potential to reveal identities, faulty signals, and the potential for trolls to render the system in useless by inundating it with false positives.\nFurthermore, the accuracy of the location data is highly dependent on the make and model of the sensor and the technology used to infer the location. Since these are highly correlated with socio-economic structures and classes the burden of false inference is disproportionate on lower income people, migrants and other marginalised groups. Any ‚Ä¶","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587254400,"objectID":"d4b15655a5dda25cecfec0469ef44f9c","permalink":"https://nkaza.github.io/post/surveillance-covid19/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/post/surveillance-covid19/","section":"post","summary":"The following is a transcript for a podcast as part of the series called ‚ÄúViewpoints on Resilient and Equitable Responses to the Pandemic\" produced by Center for Urban and Regional Studies.","tags":["data\u0026society"],"title":"Surveillance, Sensor Networks and COVID-19 Pandemic","type":"post"},{"authors":null,"categories":["machine-learning","urban-change"],"content":"   What is the point?   Introduction In an earlier post, I used location information to identify spatial patterns/clusters. Recall that space is, often simply 2 dimensional dataset (X,Y) coordinates. There is no reason to presume that clusters are of interest only in geographic space. Higher dimensional and non spatial datasets are of interest to planners as well. In this post, I am going to demonstrate few tried and tested methods to identify patterns in higher dimensional data about urban indicators.\nUnsupervised learning works by identifying patterns in the data. The following graphic vividly illustrates the point.\nSource: Booz Allen Hamilton\n  Data \u0026amp; Required Packages In this tutorial, I am going to use different urban landscape metrics that were found using methods described in other posts, at a county level for all the 3,100 counties in the conteminous United States for the 2011 land cover data. In additon, I am going restrict my attention few of the counties classified as Large Fringe Metro counties and Medium Metro counties, by the National Center for Health Statistics using the 2013 datasets.\nFor this tutorial, in addition to other packages for visualisation, you will need to install and initalise factoExtra and cluster packages.\nPrepare the dataset as follows. It is often necessary to scale the variables in machine learning. Conventional scaling is to subtract the mean and divide by the standard deviation. But, any appropriate scaling techniques can be used as shown below.\n library(tidyverse) library(skimr) library(openxlsx) library(tigris) library(cluster) library(factoextra) library(here) county_df \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;,\u0026#34;unsupervisedclustering\u0026#34;,\u0026#34;2k11_metrics_2019.csv\u0026#34;) %\u0026gt;% read_csv() %\u0026gt;% mutate(FIPS = cntyfips %\u0026gt;% as.integer %\u0026gt;% formatC(width=5, flag=\u0026#34;0\u0026#34;)) nchsurc_codes \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;,\u0026#34;unsupervisedclustering\u0026#34;,\u0026#34;NCHSURCodes2013.xlsx\u0026#34;) %\u0026gt;% read.xlsx() %\u0026gt;% as_tibble() %\u0026gt;% mutate(FIPS = `FIPS.code` %\u0026gt;% as.integer %\u0026gt;% formatC(width=5, flag=\u0026#34;0\u0026#34;)) %\u0026gt;% dplyr::select(FIPS, CountyName = County.name, CBSA = CBSA.title, NCHSURC_code = Code2013) %\u0026gt;% mutate(NCHSURC_code = NCHSURC_code %\u0026gt;% as.factor) county_df \u0026lt;- county_df %\u0026gt;% left_join(nchsurc_codes) county_df_scaled \u0026lt;- county_df %\u0026gt;% mutate_if(is.numeric, scale) # county_df_scaled %\u0026gt;% skim() scale2 \u0026lt;- function(x, na.rm = TRUE) ((x) / (max(x, na.rm=na.rm) - min(x,na.rm=na.rm))) county_df_scaled2 \u0026lt;- county_df %\u0026gt;% mutate_if(is.numeric, scale2) # county_df_scaled2 %\u0026gt;% skim()  Different clustering techniques All clustering techniques rely on the notion of distance. It is fairly straightforward to think of geographic (eulcidean) distance \\(d(i,j)\\) between two points \\(i:= (x_i,y_i, z_i), j:= (x_j,y_j, z_i)\\) as\n\\[\\sqrt{(x_i-x_j)^2 + (y_i -y_j)^2 + (z_i -z_j)^2}\\]\nBut there are other types of distances that are possible as well. For example, Manhattan distance is\n\\[|x_i-x_j| + |y_i-y_j| + |z_i-z_j|\\]\nIn fact distance can be any metric that satisfies\n\\(d(i,j) = 0\\) if and only if \\(i = j\\) \\(d(i,j) \\leq d(i,k) + d(j,k)\\)  There are others criteria, but the above two are most important. Note that \\(i\\),\\(j\\),\\(k\\) can be of any dimensions. Some popular distances are Manhattan distance, \\(L_p\\) norm, Minkowski distance, Hamming distance, Cosine Simiarlity etc.\nPartition based clustering The K-Means algorithm is perhaps the most well known, clusters data by trying to separate samples into \\(K\\) groups, minimizing a criterion known as within-cluster sum-of-squares, by focusing on euclidean distance. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields. This algorithm is susceptible to outliers that skew the euclidean norm. The following animation provides some indication of how the algorithm works.\nThe Partition around Medoids (PAM) on the other hand, is bit more robust than k-means. Instead of finding the centroids (white xs in the above animation), the algorithm finds a ‚Äòmedoid‚Äô (a representative point that is a member of the set). Note that centroids may not be members of the cluster. As such, PAM can accept any distance metric or dissimilarity matrix, instead of calculating the distance on the fly.\nIn the rest of the tutorial, I am going to restrict my attention to a subset of counties (large fringe and medium metro) and a subset of variables (number of patches, mean and variance in patch areas). The following code divides the dataset into 5 clusters using K-means and PAM.\n county_df_scaled_sel \u0026lt;- county_df_scaled %\u0026gt;% filter(NCHSURC_code %in% c(2,3)) %\u0026gt;% select(FIPS, n.patches, mean.patch.area, sd.patch.area)%\u0026gt;% na.omit() km.res \u0026lt;- county_df_scaled_sel %\u0026gt;% select(-FIPS) %\u0026gt;% kmeans(5, nstart = 25, iter.max=30) pm.res \u0026lt;- county_df_scaled_sel %\u0026gt;% select(-FIPS) %\u0026gt;% pam(k = 5) as_tibble(cbind(pam =pm.res$cluster %\u0026gt;% as.factor(), kmeans = km.res$cluster %\u0026gt;% as.factor)) %\u0026gt;% table() # kmeans # pam 1 2 3 4 5 # 1 0 0 286 ‚Ä¶","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"b6adf427a6f642da0f161f2379f703f9","permalink":"https://nkaza.github.io/post/unsupervised-clustering/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/unsupervised-clustering/","section":"post","summary":"Pattern recognition","tags":["R","teaching"],"title":"Unsupervised Clustering ","type":"post"},{"authors":null,"categories":["land-suitability"],"content":"In 2018, as I was setting up this website, I had a conversation about land suitabilty analysis over email with Lew Hopkins, who was my doctoral advisor when I was at Illinois. I want to capture this conversation on this site, as a caveat, instead of sitting in my Inbox. The emails are slightly edited.\n Lew,\nI am in the process of getting some blog posts up regarding planning methods and I was imagining writing one about land suitability analysis.\nI have not done much on this before but the more I thought about it, its problems are becoming more apparent.\n conversion of real values (e.g. slopes) to categorical (even when it is 1 -100) rescaling categorical values (e.g. soil types) to same scale as others arbitrary assignment of weights. (some times these are inconsistent e.g. A\u0026gt;B\u0026gt;C\u0026gt;A). Probably can use AHP to get at inconsistency issues. Global assignment of weights Independence of variables assumed when adding up. This is related to #4. Conversion to a single scale (to create order). This masks the ‚Äòtradeoffs‚Äô implicit in the weighting scheme.  I know that you have done some work on this before. Anything else I should be thinking about? and how to correct for some of these issues?\nMy regards to Susan.\nNikhil\n Nikhil,\nWhere to begin? Or, perhaps more appropriately, where to end? I have been pointing out these problems, literally, for 50 years, first published in\n______, ‚ÄúMethods for Generating Land Suitability Maps: A Comparative Evaluation,‚Äù Journal of the American Institute of Planners, 43:4 (1977) pp. 386-400.\nEven then, this was a review article based on well established knowledge.\nAlthough still widely cited, I could count on one hand the applications, much less the methods books, that explicitly follow the recommendation to focus on what I then called ‚Äúrules of combination‚Äù. That approach addresses many of the problems you mention by avoiding numerical manipulations of doubtful validity and retaining substantive meaning and interaction among variables in expressing relationships. I developed various better examples later in teaching, but the basic premise remains: it is better to keep the meaning of data as salient as possible throughout a process of multiattribute decision making.\nThere are even more fundamental flaws in many applications. There is often confusion about what meaning of ‚Äúsuitability‚Äù is being used, confounding predictions of where development will occur versus where it should occur, confounding absolutes with factors fungible as costs, focusing only on site attributes without considering spatial situation, failing to distinguish among factors subject to expertise from factors subject to preference, etc.\nAs Gold, JAIP, 1974, 284-286 pointed out, suitability analysis characterizes only supply, then implies that suitability determines use, completely ignoring demand.\nThese problems led to lots of what I did in the 1970s, trying to figure out how to use actual models of spatial relationships of air pollution, water resources, and accessibility while also considering demand and supply with explicit measures of cost or flooding or whatever and a presumption that complete definition of a single scale or model is impossible.\nI don‚Äôt think AHP resolves anything; it mostly adds obfuscation. My technical point of view best explained in\nShih-Kung Lai and ______, ‚ÄúThe Meanings of Tradeoffs in Multi-attribute Evaluation Methods: A Comparison,‚Äù Environment and Planning B: Planning and Design 16:2, (1989). pp. 155-170.\nMy attitude about how to provide tools for helping people deal with multi attribute decisions is still probably best expressed in these:\nE. Downey Brill, Jr., John M. Flach, ______, and S. Ranjithan, ‚ÄúMGA: A Decision Support System for Complex, Incompletely Defined Problems,‚Äù IEEE Transactions on Systems, Man and Cybernetics, 20:4 (1990), 745-757.\nShih-Kung Lai and ______, ‚ÄúCan Decision Makers Express Multiattribute Preferences Using AHP and MUT? An Experiment,‚Äù Environment and Planning B: Planning and Design 22:1 (1995) pp. 21.34.\nInsung Lee and ______, ‚ÄúProcedural Expertise for Efficient Multiattribute Evaluation: A Procedural Support Strategy for CEA,‚Äù Journal of Planning Education and Research, 14:4 (1995) pp. 255-268. (Chester Rapkin Award, best paper in volume 14 of JPER)\nThese obviously get far from what most people think of as suitability analysis, but that is perhaps the point. If the task is an initial screening of site factors for site development, then rules of combination should suffice. If it is more than that, then the task should be reframed under some other label. Put differently, my impression from the last 50 years is that attempts to resolve the methodological flaws of numerical suitability analysis just lead to greater obfuscation.\nIf I were sufficiently motivated to work on this, I think tools and protocols to implement something akin to rules of combination, ideally with hooks to models, would be the most promising way to contribute to teaching about suitability ‚Ä¶","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"ddcfd37220eb94bb60523579711c4172","permalink":"https://nkaza.github.io/post/some-troubles-with-land-suitability-analysis-a-conversation-with-lew/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/post/some-troubles-with-land-suitability-analysis-a-conversation-with-lew/","section":"post","summary":"In 2018, as I was setting up this website, I had a conversation about land suitabilty analysis over email with Lew Hopkins, who was my doctoral advisor when I was at Illinois.","tags":null,"title":"Some Issues with Land Suitability Analysis","type":"post"},{"authors":null,"categories":["R","teaching"],"content":"  pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .sourceCode { overflow: visible; } code.sourceCode  span { color: inherit; text-decoration: inherit; } div.sourceCode { margin: 1em 0; } pre.sourceCode { margin: 0; } @media screen { div.sourceCode { overflow: auto; } } @media print { pre  code.sourceCode { white-space: pre-wrap; } pre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; } } pre.numberSource code { counter-reset: source-line 0; } pre.numberSource code  span { position: relative; left: -4em; counter-increment: source-line; } pre.numberSource code  span  a:first-child::before { content: counter(source-line); position: relative; left: -1em; text-align: right; vertical-align: baseline; border: none; display: inline-block; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; padding: 0 4px; width: 4em; color: #aaaaaa; } pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; } div.sourceCode { background-color: #f8f8f8; } @media screen { pre  code.sourceCode  span  a:first-child::before { text-decoration: underline; } } code span.al { color: #ef2929; } /* Alert */ code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */ code span.at { color: #c4a000; } /* Attribute */ code span.bn { color: #0000cf; } /* BaseN */ code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */ code span.ch { color: #4e9a06; } /* Char */ code span.cn { color: #000000; } /* Constant */ code span.co { color: #8f5902; font-style: italic; } /* Comment */ code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */ code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */ code span.dt { color: #204a87; } /* DataType */ code span.dv { color: #0000cf; } /* DecVal */ code span.er { color: #a40000; font-weight: bold; } /* Error */ code span.ex { } /* Extension */ code span.fl { color: #0000cf; } /* Float */ code span.fu { color: #000000; } /* Function */ code span.im { } /* Import */ code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */ code span.kw { color: #204a87; font-weight: bold; } /* Keyword */ code span.op { color: #ce5c00; font-weight: bold; } /* Operator */ code span.ot { color: #8f5902; } /* Other */ code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */ code span.sc { color: #000000; } /* SpecialChar */ code span.ss { color: #4e9a06; } /* SpecialString */ code span.st { color: #4e9a06; } /* String */ code span.va { color: #000000; } /* Variable */ code span.vs { color: #4e9a06; } /* VerbatimString */ code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */  Introduction In this post, I am going to show how to characterise urban form using urban landscape metrics. Landscape metrics are relatively common on ecology to understand the configuration and composition of natural landscapes. Their application to urban landscape is relatively novel, though is becoming more widespread.\nMost of the methods and results are discussed in Kaza (2013) and McCarty \u0026amp; Kaza (2015). Please refer to those articles.\n Requirements This requires R, the many libraries including raster, terra, mmand and landscapemetrics.\nI strongly recommend that you read through R spatial by Robert Hijmans, the author of the raster and terra packages. terra is a newer implementation of raster. Usually it is faster, but it does not have all the functionality of raster. So I am going to switch between the two as necessary.\n Acquire data I am going to use a small section of the US National Land Cover Dataset for 2011 from United States Geological Survey. You are welcome to use any categorical raster data. If you are interested in automatically downloading the data with the script try FedData package from github.\n Explore the data raster::raster creates a raster object, while terra::rast creates as SpatRaster object. Both packages have writeRaster function, but I strongly recommend using the terra implementation, by using terra::writeRaster. In fact, it is a good idea to append the package name when dealing with rasters, especially since select in rasters conflict with select in dplyr. Jakub Nowosad‚Äôs blog explains how to convert between various classes of objects.\nIt is also useful to learn about the differences between rasterLayers, rasterStack and rasterBrick, if you are using raster package. terra on the other hand, only has a single class spatRaster and you can append layers using the traditional c function.\nInitialise the packages and read them into R along with the raster.\nlibrary(raster) library(rasterVis) library(sf) library(tidyverse)  lc \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;lmetrics\u0026#34;, \u0026#34;c11_37063.img\u0026#34;) %\u0026gt;% ‚Ä¶","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"41f4150c5a3b810299fad4fb588433be","permalink":"https://nkaza.github.io/post/urban-morphology-landscape-metrics/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/urban-morphology-landscape-metrics/","section":"post","summary":"Creating urban landscape metrics","tags":["urban-landscape-metrics"],"title":"Urban Morphology \u0026 Landscape Metrics","type":"post"},{"authors":null,"categories":["teaching","R"],"content":"  Asynchronous Communications and Troubleshooting We will continue to use Microsoft Teams for asynchronous communication and troubleshooting. Troubleshooting code errors without the cushion of in-person interactions forces us to communicate and act differently. Fortunately, this is an standard practice that has matured a lot in the last two decades. We can follow guidelines like these:\n[1] https://stackoverflow.com/help/how-to-ask\n[2] https://codereview.stackexchange.com/help/how-to-ask\nUsing RStudio Cloud We will use RStudio Cloud for troubleshooting in this course. Think of RStudio Cloud as an instance of RStudio in the cloud where you can share not only your script but also the whole environment. This increases the likelihood that others can replicate your results or troubles.\n First, create an account in https://rstudio.cloud like you would in any other website. Or login using your Github account. Here is the workflow of how this would work.   Organize your work in Projects. Everything related to a particular exercise (script, data, markdowns, etc.) should be part of one project. To create a project, go to your RStudio Cloud homepage and click New Project.  Start a project\n  This opens a RStudio interface in your browser. You can use this familiar interface to rename project, upload files, add new R script and more. See highlighted buttons in the following picture.  The fundamentals\n  The only caveat with RStudio Cloud is the limited [1 GB] space for each project in the free accounts. Because the datasets we work with are often large, it is probably a good idea to subset the data and post it to the project.\n    After adding your scripts and files, you can share the project by sharing its URL. It looks like: https://rstudio.cloud/project/1034476. Before sharing the project, click the gear sign on top-right of your project page, and change Access for Who can view this project to Everyone from the drop-down.  Sharing your project\n  When you open a project link that your colleagues have shared on Microsoft Teams, it will create a temporary version of the project in your account. No changes made to the temporary version will change the original. After you have a clear solution, save a permanent copy. This will create a new URL that can be shared back.  The 1 GB limit includes the space for the libraries from the Cloud console. In some instance installing libraries from the source (such as dplyr) might require more than 1 GB in memory. There are no easy solutions to that.\n In case your space becomes full, download projects that you do not require in the cloud and delete them from your RStudio Cloud account. Check the box left to Cloud, click More and select Export.\n  Exporting your project\n   ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"7d2cb644e8a37ec4b37955240768669f","permalink":"https://nkaza.github.io/teaching/remote-instruction-672/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/teaching/remote-instruction-672/","section":"teaching","summary":"COVID-19 actions","tags":["PLAN 672"],"title":"Using Rstudio.cloud for Troubleshooting","type":"teaching"},{"authors":null,"categories":["R"],"content":"Introduction Land suitability analysis has a long tradition in planning. Ian McHarg at Penn, pioneered the method of overlaying transparencies about suitability criteria, which then got translated into GIS with the advent of computers. Suitability analysis has many uses in planning. It can be used, for example, for\n Retail site selection Determining best locations for future land use (such as residential, agriculture, commercial etc..) Ecological planning for protecting natural habitats Prioritising investments (such as flood protection) Post-Disaster housing relocation (see Christian Karmath‚Äôs MP report)  These analyses have a long tradition in planning. In fact, my former doctoral advisor, Lew Hopkins, wrote a paper in 1977, on this topic and is still considered a classic. Dick Klosterman‚Äôs Planning Support System, WhatIf?‚Ñ¢ relies heavily on land suitability analysis.\nIn this tutorial, I give a contrived example of finding suitable site for locating a landfill using rasters. Every cell is treated as potential alternative site for the landfill and is given a score based on different criteria (such as distance to schools, population centers etc.). The trick is to figure out a right combination of these scores to order the cells (alternatives). The analytical techniques used are rather straightforward and rely on simple raster algebra. As we will see, the skill is about picking the right kinds of criteria and making appropriate judgments about how to evaluate alternatives with respect to the criteria. Subjectivity and analyst‚Äôs bias is omnipresent in these steps. In particular, it is also subject to all sorts of other theoretical problems that the insistence on quantification obfuscates.\nRequirements Many new packages are introduced in this analysis including units (to systematically think and capture units such as km^2), fasterize (fast rasterization) and others such as rasterVis, terra, sf and tidycensus.\nThe datasets are available here\nAdditional resources I strongly recommend that you read through R spatial by Robert Hijmans, the author of the raster and terra packages. In addition, you can also check out the R-exercises tagged as geospatial.\nLand Suitability for Locating a Landfill There are no accepted conventions for identifying the appropriate locations of landfills. Locating one, is a contentious topic, as it is a Locally Unwanted Land Use (LULU). Furthermore, there are many regulatory agencies that are involved in regulating the landfill locations including the Environmental Protection Agency.\n Nothing in this tutorial is an endorsement of particular criteria used for site selection purposes. This is to be treated as an in-class exercise and caution should be used to interpret the results.   In this example, I am going to use 4 arbitrary criteria\n Distance to Schools. (Farther the better) Distance to Parks. (Farther the better) Slope (Flatter the better) Distance to population centres (Sufficiently far, but no further)  But first set up the project crs by setting it to the crs of the land use raster and also create a template raster, by taking the extent and projection from a land use raster and setting every value to 0.\nlibrary(terra)\rlibrary(rasterVis)\rlibrary(here)\rlibrary(tidyverse)\rlibrary(fasterize)\rlibrary(sf)\rlu_raster \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;landsuitability\u0026#34;, \u0026#34;c11_37063.img\u0026#34;) %\u0026gt;% rast template_raster \u0026lt;- classify(lu_raster, cbind(0, 100, 0), right=FALSE) %\u0026gt;% raster::raster() #fasterize works with raster object instead of spatRaster object. Hence the conversion\rproject_crs \u0026lt;- crs(lu_raster)\rvector_read_fn \u0026lt;- function(x){\rif(file.exists(x)){\rtemp1 \u0026lt;- st_read(x, quiet=TRUE) %\u0026gt;% st_transform(project_crs)\rreturn(temp1)\r} }\r Distance to Schools \u0026amp; Parks The workhorse functions are gridDistance and classify both from the terra package.\ngridDistance is a function that calculates the distance to cells of a SpatRaster when the path has to go through the centers of neighboring raster cells. This is effectively like buffering at multiple distances.\nXlassify is a function that (re)classifies groups of values to other values. For example, all values between 0 and 1000 become 1, and all values between 1000 and 2000 become 2 in the following code. In particular, see the rcl matrix argument in ?classify\nschools \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;landsuitability\u0026#34;, \u0026#34;NCDurhamSchools\u0026#34;, \u0026#34;SchoolPts.shp\u0026#34;) %\u0026gt;% vector_read_fn()\rschools_raster_dist \u0026lt;- schools %\u0026gt;% st_buffer(10) %\u0026gt;% # Fasterize only works with polygons, so we create tiny buffers around the points\rfasterize(raster= template_raster, background = 0) %\u0026gt;%\rrast %\u0026gt;% # Converting to SpRaster.\rgridDistance(target=1) %\u0026gt;% # Check ?fasterize especially the `field argument on why target is 1.\rmask(lu_raster) %\u0026gt;%\rclassify(rcl = matrix(c(0,1000,1, 1000,2000,2, 2000,4000,3, 4000,8000,4, 8000,Inf,5), ncol=3, byrow = T), include.lowest =T, right = F)\r Similar approach can be taken to distance to parks. Here we have three different types of parks, including easement. ‚Ä¶","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"417b73d75a19ae55c811569f9f56b881","permalink":"https://nkaza.github.io/post/land-suitabilty-with-ahp-wlc/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/land-suitabilty-with-ahp-wlc/","section":"post","summary":"Land Suitability Analysis using Multi Criteria Decision Making","tags":["PLAN 672","techniques"],"title":"Land Suitability Analysis","type":"post"},{"authors":null,"categories":[],"content":"   Introduction While much of urban data systems moved to vector object based data structures, raster datasets are ubiquitous and ever present. Raster datasets are simply an array of pixels/cells organized into rows and columns (or a grid) where each cell contains a value representing information, such as temperature, soil type, land use, water level. Rasters are aerial photographs, imagery from satellites, google street view images.\nFew things to note:\n Raster datasets are always rectangular (rows x col) similar to matrices. Irregular boundaries and holes are created by using NAs. Rasters have to contain values of the same type (int, float, boolean) throughout the raster, just like matrices and unlike data.frames The size of the raster depends on the resolution and the extent of the raster. As such many rasters are large and often cannot be held in memory completely. In general, rasters we would interested in have a coordinate system (projected or unprojected). Because of it regular nature of the data structure, all we need to store is the Origin coordinates and the cell resolution (square or rectangular).  Rasters in R The workhorse packages for working with rasters in R is raster and terra packages by Robert Hijmans. In particular see Spatial Data Science with R. terra is better and faster in many instances, but is newer and does not have all the functionality and support associated with raster. I will switch back and forth between the two packages as necessary. Most functions are available in both packages, but it is useful to learn the differences\nEventually, if you get into the weeds in Remote Sensing, you should use RStoolbox package that has support for parallel processing. If you are working in rasters that are not geographically located (such as digital pictures) magick wrapper around ImageMagick may be a good option.\nDownload data You can download the datasets used in this tutorial here\nReading in Rasters Let‚Äôs start with toy examples and examine the use of functions in the raster and terra packages.\nlibrary(raster) library(rasterVis) library(sf) library(tidyverse) library(tigris) library(leaflet) library(skimr) library(terra) # Let\u0026#39;s create a raster of size 5x5. #creates an empty raster object raster_object \u0026lt;- raster(ncol=5, nrow=5, xmx=-100, xmn=-500, ymn=100, ymx=500, crs=\u0026#39;+proj=longlat +datum=WGS84\u0026#39;) #In terra, you can create an object with \u0026#39;rast\u0026#39;. Notice the difference between xmx and xmax argument names. raster_object_terra \u0026lt;- rast(ncol=5, nrow=5, xmax=-100, xmin=-500, ymin=100, ymax=500, crs=\u0026#39;+proj=longlat +datum=WGS84\u0026#39;) #view its properties raster_object # class : RasterLayer # dimensions : 5, 5, 25 (nrow, ncol, ncell) # resolution : 80, 80 (x, y) # extent : -500, -100, 100, 500 (xmin, xmax, ymin, ymax) # crs : +proj=longlat +datum=WGS84 +no_defs raster_object_terra # class : SpatRaster # size : 5, 5, 1 (nrow, ncol, nlyr) # resolution : 80, 80 (x, y) # extent : -500, -100, 100, 500 (xmin, xmax, ymin, ymax) # coord. ref. : +proj=longlat +datum=WGS84 +no_defs #access the resolution of the object res(raster_object) # [1] 80 80 #we can also set the resolution of the object res(raster_object) \u0026lt;- 100 res(raster_object) # [1] 100 100 # We can assign values to the object by supplying the right number of values. values(raster_object) \u0026lt;- runif(ncell(raster_object)) plot(raster_object)  # Basic airthmetic is treated as raster_object2 \u0026lt;- raster_object * 2   Exercise\n Notice the difference in the format between objects created by rast and raster. Read up on those differences.   Of course, the real advantage is to reading in rasters that we can work with in the urban policy arena. Rasters come in many formats.\nSay for example, we want to read in tree cover data in the ERDAS Imagine (*.img) format.\nlibrary(here) (durham_treecover_raster \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;basicraster\u0026#34;, \u0026#34;durham_treecover.img\u0026#34;) %\u0026gt;% raster) # class : RasterLayer # dimensions : 1483, 888, 1316904 (nrow, ncol, ncell) # resolution : 30, 30 (x, y) # extent : 1510245, 1536885, 1558875, 1603365 (xmin, xmax, ymin, ymax) # crs : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs # source : durham_treecover.img # names : Layer_1 # values : 0, 100 (min, max) # Download the boundary of the county for visualisation purposes. durhamcty_sf \u0026lt;- counties(state = 37, refresh=TRUE, cb=TRUE, progress_bar = FALSE) %\u0026gt;% st_as_sf() %\u0026gt;% filter(GEOID == \u0026#34;37063\u0026#34;) %\u0026gt;% st_transform(crs = crs(durham_treecover_raster))  Note that the durham_treecover_raster only is a reference to a file and not loaded fully into memory. Also note the resolution, origin and dimensions. The tree cover is also an integer raster with values ranging from 0 to 100. Integers take up fewer bytes than floats and doubles, in general. The raster can be visualised as a static plot or in leaflet.\nrasterVis::levelplot(durham_treecover_raster, ylab=NULL, xlab=NULL, scales=list(y=list(draw=FALSE), x=list(draw=FALSE)), col.regions = ‚Ä¶","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582761600,"objectID":"066a95f2a9278da1d39d769e03669a5c","permalink":"https://nkaza.github.io/post/basic-raster-operations-in-r/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/basic-raster-operations-in-r/","section":"post","summary":"Basic Raster Operations in R","tags":["R","teaching"],"title":"Basic Raster Operations in R","type":"post"},{"authors":["Kshitiz Khanal","Nikhil Kaza"],"categories":["new-urban-analytics","open-data"],"content":"      Introduction This tutorial covers obtaining free OpenStreetMap (OSM) data. OpenstreetMap (OSM) is a free and open map of the world created largely by voluntary contribution of millions of people around the world. Much like Wikipedia. Since the data is free and open, there are much less restrictions to obtaining and using the data. The only condition of using OSM data is proper attribution to OSM contributors.\nComparison between OSM and Google Availability of data is reasonably good in both Goolge and OSM in most parts of the world. However, the availability of Google data is better in places where there is more commercial interest and that in OSM where there is more humanitarian interst. You can use Map Compare to compare OSM and Google in particular locations.\nOSM data is free to download. Overpass can be used free of cost to download small amount of data. For large datasets, use Geofabrik. Google requires users to pay based on volume of data served after a limited daily quota. Policies of Google change frequently, so note that your code will eventually and frequently break.\nDownloading data OSM serves two APIs, namely Main API for editing OSM, and Overpass API for providing OSM data. We will use Overpass API to gather data in this tutorial just like in the tutorial about OSM and QGIS\nData can be queried for download using a combination of search criteria lke location and type of objects. It helps to understand how OSM data is structured. OSM data is stored as a list of attributes tagged in key - value pairs of geospatial objects (points, lines or polygons). For example, for an architect‚Äôs office, the key is ‚Äúoffice‚Äù, and the value is ‚Äúarchitect.‚Äù For the name of the office, key is ‚Äúname‚Äù and value is ‚ÄúABC Design Studio.‚Äù Access an extensive list of key-value pairs through OSM Wiki Map features.\nObtaining point locations of restaurants in Durham from OSM Restaurants are tagged under amenities. Amenities, according to OSM Wiki are facilities used by visitors and residents. Here, ‚Äòkey‚Äô is ‚Äúamenity‚Äù and ‚Äòvalue‚Äô is ‚Äúrestaurant.‚Äù Do not forget to look for related amenities such as ‚Äúpub‚Äù, ‚Äúfood court‚Äù, ‚Äúcafe‚Äù, ‚Äúfast food‚Äù, etc. Other amenities include: ‚Äúuniversity‚Äù, ‚Äúmusic school‚Äù, ‚Äúkindergarten‚Äù and the likes in education, ‚Äúbus station‚Äù, ‚Äúfuel‚Äù, ‚Äúparking‚Äù and others in transportation, and much more.\nlibrary(osmdata) library(sf) library(tidyverse) library(leaflet) library(widgetframe) data_from_osm_df \u0026lt;- opq (getbb (\u0026#34;Durham, North carolina\u0026#34;)) %\u0026gt;% #gets bounding box add_osm_feature(key = \u0026#34;amenity\u0026#34;, value = \u0026#34;restaurant\u0026#34;) %\u0026gt;% #searches for restaurants osmdata_sf() #download OSM data as sf #select name and geometry from point data for restaurants cafe_osm \u0026lt;- data_from_osm_df$osm_points %\u0026gt;% #select point data from downloaded OSM data select(name, geometry) #for now just selecting the name and geometry to plot #create a plot in leaflet m1 \u0026lt;- leaflet() %\u0026gt;% addProviderTiles(\u0026#34;CartoDB.Positron\u0026#34;) %\u0026gt;% addCircles(data = cafe_osm) frameWidget(m1)   {\u0026#34;x\u0026#34;:{\u0026#34;url\u0026#34;:\u0026#34;index_files/figure-html//widgets/widget_unnamed-chunk-1.html\u0026#34;,\u0026#34;options\u0026#34;:{\u0026#34;xdomain\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;allowfullscreen\u0026#34;:false,\u0026#34;lazyload\u0026#34;:false}},\u0026#34;evals\u0026#34;:[],\u0026#34;jsHooks\u0026#34;:[]} It is helpful to learn about the distinctions between different tags (key-value pairs). For example, the key ‚Äúlanduse‚Äù is used to describe the purpose for which an area is being used. Examples of values for the key ‚Äúlanduse‚Äù are ‚Äúcommercial‚Äù, ‚Äúretail‚Äù, ‚Äúvineyard‚Äù, ‚Äúcemetery‚Äù, ‚Äúreligious‚Äù, etc. Landuse tags are more generic than amenities and are only used for area objects while amenities can also be used for point objects. In case of any confusion, refer to Map features.\nVarious amenities, land-use, roads (e.g.¬†key=‚Äúhighway‚Äù, value = ‚Äúprimary‚Äù, ‚Äúservice‚Äù, ‚Äúfootway‚Äù), natural land features (key=‚Äúnatural‚Äù, value = ‚Äúgrassland‚Äù), settlements (key = ‚Äúplace‚Äù, value = ‚Äúsuburb‚Äù), power (key=‚Äúpower‚Äù, value=‚Äúline‚Äù, ‚Äúpole‚Äù, ‚Äútransformer‚Äù), etc. may be useful in planning applications. Comparing with Google Places It is useful to compare the output of OSM to Google Places. I am going to use googleway package for this analysis.\n To make this portion of the code work, you will need an API key from Google. Instructions to get and set an API key are located here.   Note the use of loops to get the next page.\nlibrary(googleway) str \u0026lt;- \u0026#34;restaurants in Durham, NC\u0026#34; # Construct a search string res \u0026lt;- google_places(search_string = str, key = YOUR_API_KEY) #Query google servers. Do not forget to set your Google API key str(res)  ## List of 4 ## $ html_attributions: list() ## $ next_page_token : chr \u0026#34;AfLeUgMGT-mwxcivbE0RqYwAt129UKfk0t72RjKr4882rjVi4LsM5k4ApcVC0veEtDM2AaDtIXm_AwTDvhyNuR7dMA80Cjxs9j18FFjUQOLfgBz\u0026#34;| __truncated__ ## $ results :\u0026#39;data.frame\u0026#39;: 20 obs. of 16 variables: ## ..$ business_status : chr [1:20] \u0026#34;OPERATIONAL\u0026#34; \u0026#34;OPERATIONAL\u0026#34; \u0026#34;OPERATIONAL\u0026#34; \u0026#34;OPERATIONAL\u0026#34; ... ## ..$ formatted_address : chr [1:20] \u0026#34;737 9th St #210, Durham, NC 27705, United States\u0026#34; \u0026#34;8128 Renaissance Pkwy #114, Durham, NC 27713, United States\u0026#34; \u0026#34;315 E Chapel ‚Ä¶","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"f386031eff6f09ec3e5995f699f8f70b","permalink":"https://nkaza.github.io/post/extracting-data-from-osm/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/post/extracting-data-from-osm/","section":"post","summary":"Using R to scrape points of interest from OpenStreetMap","tags":["R","teaching"],"title":"Extracting Data from OpenStreetMap","type":"post"},{"authors":null,"categories":["new-urban-analytics"],"content":"   Getting Started When the data is unstructured or seemingly unstructured, we can still use R to create some structure. In this post I am going to demonstrate how to scrape a html page to extract the relevant information and convert them into a table for further analysis. In this particular example, we want to use Craigslist. Usual disclaimers about data retrieval and storage apply. Please consult a lawyer, especially if you use the data for non-research purposes. Also you can follow along the evolving landscape of the media law on this topic.\nAdditional Resources   Munzert, Simon, Christian Rubba, Peter Mei√üner, and Dominic Nyhuis. 2014. Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining. 1 edition. Chichester, West Sussex, United Kingdom: Wiley.\n  Pittard, Steve. 2019. Web Scraping with R.\n  Understanding the structure of the query In this post, I am going to demonstrate how to scrape and assemble the rental listings in Boston. To start, it is useful to use your browser to access the webpage.\nhttps://boston.craigslist.org/d/apts-housing-for-rent/search/apa\nThe results should look like this\n   On the left hand corner, you will notice that there are forms that you can use limit the search results. For example,\nhttps://boston.craigslist.org/search/apa?postedToday=1\u0026amp;max_price=2000\u0026amp;availabilityMode=0\u0026amp;broker_fee=1\u0026amp;sale_date=all+dates\nrefers to a search with\n ‚Äúposted today‚Äù is checked ‚Äúmaximum price‚Äù is ‚Äú2000 usd‚Äù ‚Äúavailability‚Äù is set to ‚Äúall dates‚Äù ‚Äúno broker fee‚Äù is checked ‚Äúopen house date‚Äù is set to ‚Äúall dates‚Äù  Few things to note.\n Cities are subdomains. i.e.¬†if you need to information about Raleigh, you need to use https://raleigh.craigslist.org/ The second bit of it /d/apts-housing-for-rent/search/apa is same for all cities but will change if you want to scrape something other than apartments. The browser uses a https protocol instead of a http protocol. This is more secure, but occasionally poses problems for accessing the web using curl instead of a browser. cURL is a command-line tool for getting or sending data including files using URL syntax often used explicitly within scripts that are used for web scraping. We are going to use rvest which takes care of this issue. The arguments start after ?. Match the options checked on the browser to the variables in the url and the values they take. For example, postedToday and broker_fee are set to 1 when corresponding boxes are checked. Variables are concatenated using \u0026amp; in the url string. It looks like the spaces in all dates are replaced with a +. Usually they are replaced by %20 a hexadecimal code for space. It is useful to understand the hex codes for special characters.  Based on the understanding of the above url, we can use conventional string manipulation to construct the url in R as follows.\nlibrary(tidyverse) library(rvest) library(tmap) location \u0026lt;- \u0026#39;boston\u0026#39; bedrooms \u0026lt;- 2 bathrooms \u0026lt;- 2 min_sqft \u0026lt;- 900 baseurl \u0026lt;- paste0(\u0026#34;https://\u0026#34;, location, \u0026#34;.craigslist.org/search/apa\u0026#34;) # Build out the query queries \u0026lt;- vector(\u0026#34;character\u0026#34;, 0) # Initialise a vector queries[1] \u0026lt;- paste0(\u0026#34;bedrooms=\u0026#34;, bedrooms) queries[2] \u0026lt;- paste0(\u0026#34;bathrooms=\u0026#34;, bathrooms) queries[3] \u0026lt;- paste0(\u0026#34;minSqft=\u0026#34;, min_sqft) (query_url \u0026lt;- paste0(baseurl,\u0026#34;?\u0026#34;, paste(queries, collapse = \u0026#34;\u0026amp;\u0026#34;))) # [1] \u0026#34;https://boston.craigslist.org/search/apa?bedrooms=2\u0026amp;bathrooms=2\u0026amp;minSqft=900\u0026#34;   Exercise\nTry out the different urls and understand how the query to the server works. i.e.¬†try different cities, different types of ads and different arguments to the queries. It is useful to construct different urls by typing them out so that you understand the syntax.\n Figuring out the structure of the results An url request to the server usually results in an HTML page that is rendered in the browser. In other posts, we have seen how the URL may result in JSON objects which are much easier to deal with to create a structured dataset. But HTML pages are often with some structure that are used to render the page. Sometimes, we can take advantage of that structure to infer and extract what we need.\nTo examine the structure, we will have to use the developer tools in the web browser. In the rest of the tutorial, I am going to use Firefox as a browser, though analogous tools can be found for other browsers. You can open the Firefox Developer Tools from the menu by selecting Tools \u0026gt; Web Developer \u0026gt; Toggle Tools or use the keyboard shortcut Ctrl + Shift + I or F12 on Windows and Linux, or Cmd + Opt + I on macOS.\n   The most useful thing for this exercise is the page inspector, usually in the bottom left. You can right click on any post in the craigslist result page and use inspect element to understand what the structure looks like. Or alternately, you can point to various elements in the inspector and see different elements highlighted in the browser window.\n     In this particular instance each posted ad seems to be within a \u0026lt;div\u0026gt; with a class cl-search-result, with a data-pid associated with ‚Ä¶","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580774400,"objectID":"1faf978562afc1df82aede1f8a895be6","permalink":"https://nkaza.github.io/post/scraping-craigslist-posts/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/scraping-craigslist-posts/","section":"post","summary":"Using R to scrape rental listings from Craigslist","tags":["R","teaching"],"title":"Scraping Craigslist Posts","type":"post"},{"authors":["K. Peng","N. Kaza"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"9a7e8ba7bfde6bdaeaa33966519a912b","permalink":"https://nkaza.github.io/publication/peng-2020-aa/","publishdate":"2021-02-18T16:49:56.404789Z","relpermalink":"/publication/peng-2020-aa/","section":"publication","summary":"Considerable research on the risks of obesity and chronic diseases has focused on the relationships between where people live, where they shop, and the types of food they purchase. Rarely have investigators used a national sample and explicitly addressed whether people use neighborhood food stores to purchase food. Even more rarely have previous studies accounted for the broader built environment characteristics in which people live, access resources, socialize, and shop for food. We examined whether the cross-sectional associations between availability of neighborhood convenience stores and supermarkets and self-reported household annual expenditures for snacks and beverages still held after including regional destination accessibility, availability and diversity of neighborhood destinations, and neighborhood street connectivity. Our study included 49,048 households in 2010, located in 378 metropolitan areas in the continental United States. Households purchased more snacks if they lived in neighborhoods with many convenience stores. Access to unhealthy food includes a consideration of geographic proximity. The broader built environment characteristics was associated with food purchase, although the magnitude was small. ","tags":[],"title":"Association between Neighborhood Food Access, Household Income, and Purchase of Snacks and Beverages in the United States","type":"publication"},{"authors":["M. He","J. Glasser","N. Pritchard","S. Bhamidi","N. Kaza"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"8464b7925f329da51d82f0792be61830","permalink":"https://nkaza.github.io/publication/he-2019-aa/","publishdate":"2021-02-18T16:49:56.671142Z","relpermalink":"/publication/he-2019-aa/","section":"publication","summary":"We aim to find agglomerations of U.S. counties that are partitioned by commuting patterns by representing inter-county commuting patterns as a weighted network. We develop and use a community detection method based on the  configuration model to identify significant clusters of nodes in a weighted network that prominently feature self-loops which represent same-county commuting.   After we apply this method to county level commuting data from 2010, we find regions that are significantly different from existing delineations such as Metropolitian Statistical Areas and Megaregions. Our method identifies regions with varying sizes as well as highly overlapping regions. Some counties belong to as many as six different statistically significant clusters but some do not belong to any. Our results offer an alternative way of categorizing economic regions from existing methods and suggest that geographical delineations should be rethought.","tags":[],"title":"Demarcating Regions using Community Detection in Commuting Networks","type":"publication"},{"authors":["K. Onda","J. Branham","T. K. BenDor","N. Kaza","D. Salvesen"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"f451795738812fe9f4dffc7f12efbb0d","permalink":"https://nkaza.github.io/publication/onda-2019-aa/","publishdate":"2021-02-18T16:49:56.956667Z","relpermalink":"/publication/onda-2019-aa/","section":"publication","summary":"Urban development relies on many factors to remain viable, including federal assistance with infrastructure development. The 1982 US Coastal Barrier Resources Act (CoBRA) prohibits federal financial assistance for infrastructure, post-storm disaster relief, and flood insurance in designated sections (system units) of coastal barriers.  How has CoBRA's removal of these subsidies affected rates and types of urban development? Using building footprint and real estate data (n=1,385,552 parcels), we compare density of built structures, land use types, and land values within and outside of system units in eight southeastern US states. Here we show that CoBRA is associated with reduced development rates in coastal barrier zones, but also reveal how withdrawal of federal subsidies might be counteracted by local responses. As attention increases towards improving development policy in high hazard areas, this work contributes to understanding how limitations on infrastructure subsidies can affect outcomes under overlapping jurisdictions with competing goals.","tags":[],"title":"Does removal of federal subsidies discourage development? An evaluation of the US Coastal Barrier Resources Act","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"cb0c1049816257f67afb16e399813813","permalink":"https://nkaza.github.io/publication/kaza-2019-ab/","publishdate":"2021-02-18T16:49:56.277788Z","relpermalink":"/publication/kaza-2019-ab/","section":"publication","summary":"Scattered urban development leads to non-compact urban form. In this paper, I demonstrate that Index of Moment of Inertia is a useful metric to measure compactness. However, elongated political boundaries and natural restrictions severely distort the metric, rendering it less useful for monitoring urban development. I propose a landscape shape adjustment of this metric that retains some of the useful properties of the Index.","tags":[],"title":"Landscape Shape Adjusted Compactness Index for Urban Areas","type":"publication"},{"authors":["J. E Pesantez","E. Z. Berglund","N. Kaza"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"3bb4c7446f8edf4360d61c8e3d0c3d3a","permalink":"https://nkaza.github.io/publication/pesantez-berglund-kaza/","publishdate":"2021-02-18T16:49:56.536845Z","relpermalink":"/publication/pesantez-berglund-kaza/","section":"publication","summary":" Smart meters installed at the user-level provides a new data source for managing water infrastructure. This research explores the use of machine learning methods to forecast hourly water demand using smart meter data at the user-level. Random Forests (RF), Artificial Neural Network (ANN), and Support Vector Regression (SVR) are applied to forecast demand using predictors that include lagged demand, seasonality, weather, and household characteristics. Time-series clustering is applied to differentiate the time of day and days of the week, which improves model performance. Two modeling approaches are compared. Individual models are developed separately for each smart meter, and a group model is trained using a dataset of multiple meters. The approaches perform similarly well. Individual models predict demands at specific meters with lower error, while the group model predicts demands at new meters with lower error. Results demonstrate that RF and ANN perform better than SVR across all scenarios.","tags":[],"title":"Smart Meters Data for Modeling and Forecasting Water Demand at the User-level","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"020aff961c9057a44b9b82ba056b90b5","permalink":"https://nkaza.github.io/publication/kaza-2019-transport/","publishdate":"2021-02-18T16:49:56.819061Z","relpermalink":"/publication/kaza-2019-transport/","section":"publication","summary":"Transportation energy is a significant portion of the energy consumption of the US economy. While various policies such as changing the fuel mix and alternative fuels are proposed to make the system more efficient, the efficacy of land use policies such as changing the urban form and densification have been subject to considerable debate. In this paper, I use a rich dataset compiled from different sources to test the effectiveness of urban form on energy consumption in the transportation sector. I proxy the consumption with retail sales from gas stations for most of the conterminous United States at a county level. Using both demographic, economic and landscape characteristics of urban form I tease out the effect of different dimensions on energy consumption. I find that compact and contiguous urban form is modestly associated with lower energy consumption and is more important than demographic concentration in explaining the variance.","tags":[],"title":"Urban Form and Transportation Energy Consumption","type":"publication"},{"authors":[],"categories":[],"content":"Preliminaries To get started with this course, you will need R. I strongly recommend Rstudio. To function correctly, RStudio needs R and therefore both need to be installed on your computer. If you already have these installed, please see this page to make sure you have up to date versions of the software. We will talk about other required software as we proceed further.\nWhy R?  Great for reproducibility Great for statistics Great for data analysis and visualization Interdisciplinary \u0026amp; Extensible Open source \u0026amp; Cross Platform Most GIS operations can be done in R  See R-ecology for more details.\n Actually, I see it as part of my job to inflict R on people who are perfectly happy to have never heard of it. Happiness doesn‚Äôt equal proficient and efficient. In some cases the proficiency of a person serves a greater good than their momentary happiness. ‚Äì Patrick Burns: R-help (April 2005)\n Why not Python? Or Julia? Or C++? Mostly because I don‚Äôt know them well/or at all. They are probably great, but R works for me, for the most part. See Sebastian‚Äôs post on Why Python?.\nGood Practices  Keep track of who wrote what code at the top of the file and its purpose. Use comments liberally using ‚Äú#‚Äù. Avoid setwd() and use only relative paths. Always use R projects Distinct components of the code should ideally be separate and be accessed using source(). This improves legibility. Use a consistent style within your code. For example, name all data frames to something ending in _df. Keep all of your source files for a project in the same directory, then use relative paths as necessary to access them. Memory is a problem for R, because it stores all objects in memory (mostly true). Follow good practice of deleting objects that you don‚Äôt need using rm(). It is rarely necessary for you to use garbage collection gc(). If you are running to into problems constantly  Consider upgrading RAM (It is becoming cheaper by the day). Consider if you can break up the data set and independently process them and reassemble them. Find alternative libraries and methods.    Basic R Once you have R \u0026amp; Rstudio installed, run the following to see if it is working correctly.\n1 + 1 #add 1 + 1 # [1] 2 a \u0026lt;- 2:5 #Assign vector 2:5 to a variable a a #print a to console # [1] 2 3 4 5 runif(5) #Generate 5 random numbers # [1] 0.08783858 0.60768902 0.46983531 0.21926932 0.99419249 sessionInfo() #What is my computer environment? (Mac, Windows, packages etc.) # R version 4.4.1 (2024-06-14) # Platform: aarch64-apple-darwin20 # Running under: macOS 15.6 # # Matrix products: default # BLAS: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib # LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib; LAPACK version 3.12.0 # # locale: # [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 # # time zone: America/New_York # tzcode source: internal # # attached base packages: # [1] stats graphics grDevices utils datasets methods base # # loaded via a namespace (and not attached): # [1] digest_0.6.37 R6_2.6.1 bookdown_0.43 fastmap_1.2.0 # [5] xfun_0.52 blogdown_1.21 cachem_1.1.0 knitr_1.50 # [9] htmltools_0.5.8.1 rmarkdown_2.29 lifecycle_1.0.4 cli_3.6.5 # [13] sass_0.4.10 jquerylib_0.1.4 compiler_4.4.1 rstudioapi_0.17.1 # [17] tools_4.4.1 evaluate_1.0.4 bslib_0.9.0 yaml_2.3.10 # [21] jsonlite_2.0.0 rlang_1.1.6  The output of the last two commands will be different on your computer than what is being shown.\n Exercise\n If a vector a is (4,6,5,7, 10, 9, 4, 15), how many elements are over 7? What position in the vector are they? if b is 3, what is a+b? If b is (2,5), what is a+b?   Literate programming and RMarkdown Literate Programming is a concept introduced by Donald Knuth. The fundamental premis is that one should write code that communicates primarily to hummans, not computers. Here are some examples:\n Introduction to Literate Programming in R Jane Austen and R  R Markdown allows you easily write documents that contain: R code, text, images, links, etc. You are required to use it for this course and you should effectively use it as a lab notebook for your data analysis everywhere. This and other tutorials in this sequence are written in Rmarkdown.\nGetting started with Markdown To start a new Markdown file, open up Rstudio and follow the following picture.   The created template is quite informative. Use Knit on the top to see the output of the template.   The parameters between the two --- is a YAML header. YAML(YAML Ain‚Äôt Markup Language) is a human-readable data serialization language. Most of the document rendering settings are there. Experiment with them. By simply changing the output format, we can get a doc file or a html file.\nThe rest of the document is formatted according to markdown syntax. Below are some useful syntax to structure your documents.\n# Heading 1 ## Heading 2 *italics* **bold** ~~strikethrough~~ ~subscript~ ^superscript^ ![](pictures/notebook.png) [link](www.link.com) ‚Ä¶","date":1577750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1755648000,"objectID":"37ed5eee6df161ee3f156dccc693e30a","permalink":"https://nkaza.github.io/post/introduction-to-r-rmarkdown/","publishdate":"2019-12-31T00:00:00Z","relpermalink":"/post/introduction-to-r-rmarkdown/","section":"post","summary":"Preliminaries To get started with this course, you will need R. I strongly recommend Rstudio. To function correctly, RStudio needs R and therefore both need to be installed on your computer.","tags":["R","teaching"],"title":"Introduction to R \u0026 Rmarkdown","type":"post"},{"authors":[],"categories":[],"content":"Preliminaries Strictly speaking, we won‚Äôt use QGIS much in this course. However, there are occasions where it may be useful to use a GIS package with a GUI to quickly visualise spatial data and troubleshoot some issues. ESRI‚Äôs products work fine if you are used to them, and you can skip this tutorial. On the other hand, QGIS is not limited to Windows and is free.\nYou should download and install QGIS on your computer, with instructions depending on the configuration. Unless you prefer bleeding edge version, I recommend that you install the stable version.\nDownload Data Download the latest building violations data from building violations data from the Chicago Open Data Portal. For convenience, a local copy downloaded in 2019 is stored here.\nGetting Started with QGIS Once you open QGIS, the windows and toolbars that show up depend on your computer configuration. But in general, it should look like the following and you can use the add new layers toolbar to add the downloaded dataset.\n   Should you have trouble locating the right panels and toolbars explore toggling some of them from the View menu as below.\n   Because, the building violations are a text delimted file, pick the button to add csv file and select the right paths.\nMake sure to check the right X and Y coordinates. Longitude is the X coordinate, despite the convention of writing the Latitude first. Please note that Lat/Long are usually in a coordinate system called WGS84. Please confirm with the documentation on the data source that this is indeed the case. Otherwise, geographic operations will result in erroneous outputs.\n   Adding this layer, results in adding ~1.6 million points in the city of Chicago, where violations are recorded.\nOne of the key features of a GUI based GIS is to allow for styling of layers. Use the following video to get some sense of how to style the violations layer based on the status of the violation.\n If the videos appear blurry, your video playback on YouTube might be set too low. Increase the playback resolution to 720p or 1080p in the Settings with in the YouTube player.      Exercise\n This is a particularly bad example of a map. List out the ways it can be improved. Add other layers to the map and style them, to give some context to Chicago (such as Roads)   Print Composer To create a proper map with legend, north arrow and scale, you need to use a print composer. You can use the following video to get started.\n  Additional Resources There are many resources on the web that will get you familiar with QGIS. If you have some familarity with ESRI products, there are many analogous functions and workflows in QGIS and should be relatively straightforward to figure out.\n Ujaval Ghandi‚Äôs QGIS tutorials and tips The official training manual of QGIS  ","date":1577750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642909252,"objectID":"d633665538bdeb86a0267e25c85e0c49","permalink":"https://nkaza.github.io/post/starting-with-qgis/","publishdate":"2019-12-31T00:00:00Z","relpermalink":"/post/starting-with-qgis/","section":"post","summary":"A Quick Introduction to QGIS","tags":["QGIS"],"title":"Starting with QGIS","type":"post"},{"authors":["Nikhil Kaza"],"categories":null,"content":"Transportation energy is a significant portion of the energy consumption of the US economy. While various policies such as changing the fuel mix and alternative fuels are proposed to make the system more efficient, the efficacy of land use policies such as changing the urban form and densification have been subject to considerable debate. In this paper, I use a rich dataset compiled from different sources to test the effectiveness of urban form on energy consumption in the transportation sector. I proxy the consumption with retail sales from gas stations for most of the conterminous United States at a county level. Using both demographic, economic and landscape characteristics of urban form I tease out the effect of different dimensions on energy consumption. I find that compact and contiguous urban form is modestly associated with lower energy consumption and is more important than demographic concentration in explaining the variance.\n","date":1571932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571932800,"objectID":"d3e20d99f55d6adc3e8ad536355caa91","permalink":"https://nkaza.github.io/talk/urban-form-and-transportation-energy-consumption/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/urban-form-and-transportation-energy-consumption/","section":"event","summary":"Transportation energy consumption in the United States and its relation to urban form","tags":["energy-policy","urban-form","transportation"],"title":"Urban Form and Transportation Energy Consumption","type":"event"},{"authors":["Nikhil Kaza"],"categories":null,"content":"Urban development relies on many factors to remain viable, including infrastructure, services, and government provisions and subsidies. However, in situations involving federal or state level policy, development responds not just to one regulatory signal, but also to multiple signals from overlapping and competing jurisdictions. The 1982 U.S. Coastal Barrier Resources Act (CoBRA) offers an opportunity to study when and how development restrictions and economic disincentives protect natural resources by stopping or slowing urban development in management regimes with distributed authority and responsibility. CoBRA prohibits federal financial assistance for infrastructure, post-storm disaster relief, and flood insurance in designated sections (system units) of coastal barriers. How has CoBRA‚Äôs removal of these subsidies affected rates and types of urban development? Using building footprint and real estate data (n=1,385,552 parcels), we compare density of built structures, land use types, residential house size, and land values within and outside of system units in eight Southeast and Gulf Coast US states. We show that CoBRA is associated with reduced development rates in coastal barrier zones. We also demonstrate how local responses may counteract withdrawal of federal subsidies. As attention increases towards improving urban resilience in high hazard areas, this work contributes to understanding how limitations on infrastructure and insurance subsidies can affect outcomes under overlapping jurisdictions with competing goals.\n","date":1571477400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571477400,"objectID":"20e229e9c59b36b6fdf58a56b42631c2","permalink":"https://nkaza.github.io/talk/does-removing-federal-subsidies-discourage-development-an-evaluation-of-the-impact-of-the-u.s.-coastal-barrier-resources-act/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/does-removing-federal-subsidies-discourage-development-an-evaluation-of-the-impact-of-the-u.s.-coastal-barrier-resources-act/","section":"event","summary":"The role of federalism in urban development","tags":["urban-form","urban-change","coastal-development"],"title":"Does Removing Federal Subsidies Discourage Development? An Evaluation of the Impact of the U.S. Coastal Barrier Resources Act","type":"event"},{"authors":null,"categories":["new-urban-analytics","regional-science"],"content":"     This post is based on joint work with Dr.¬†Kate Nesse, and is published in the Internation Regional Science Review.\nIntroduction Office of Managment and Budget (OMB) identifies Core Based Statistical Areas (CBSA) as collections of counties. These CBSA can be Metropolitan (MSA) or Micropolitan \\(\\mu\\)SA based on the population of the ‚Äòcore‚Äô (\\(\\ge\\) 50,000 or not). Within these Statistical Areas, counties can either be Central (contain all or a substantial portion of the urbanised area) or Outlying (employment interchange measure with the Central counties above 25%). In other words, the centrality of the county is defined by the urban population attributes of the county rather than its relative location in the commuting network, whereas Outlying defined in relation to the Central counties. In 2015, a vast majority of the counties within CBSA are considered Central; only 29% of the counties are Peripheral/Outlying. This is even more stark within \\(\\mu\\)SAs where only 14% are considered Peripheral. CBSAs are predominantly dominated by the Central counties; they account for 92.5% of the CBSA population. These Central counties are crucial to the delineation of these statistical regions and encompass the economic core of the country.\nTable 1 Types of CBSAs and Counties in Conterminous United States. Source: OMB (2015)\n   County Type      CBSA Type Central Outlying Total  MSA 785 451 1,236  \\(\\mu\\)SA 569 94 663  Total 1,354 545 1,899    These definitions are built upon the assumption that a labor market is built around a central node. While this may have been the case historically, with our increasingly complex cities and multiplicity of transportation networks, contemporary cities have much more complex labor market networks and regional structures. In this work, we examine if a different approach to defining the core would affect our understanding of the economic geography of metropolitan United States. In particular, we are interested in understanding how the positionality of the nodes in the network illuminates our understanding of the regions.\n Labor Market Centrality Index A k‚Äìcore of an unweighted simple binary graph is its subgraph where all the nodes have at least degree \\(k\\). This subgraph is obtained by iteratively removing nodes from the network whose degree is less than k until a stable set of vertices with the minimum degree is reached. A node in a network has a coreness index \\(k\\), if it belongs to a \\(k\\)‚Äìcore but not a \\(k+1\\)‚Äìcore.\nThis can be generalized to a directed network by focusing on the indegree; i.e.¬†a \\(k\\)‚Äìcore is the subgraph, where all nodes have an in-degree \\(k\\). We can also generalize this concept to a weighted graph by using \\(s\\)‚Äìcore decomposition, where degree of the vertex is replaced by strength of the vertices (Eidsaa and Almaas 2013). If, the edge weight between nodes \\(i\\) and \\(j\\) is denoted by a non-negative \\(w_{ij}\\), then the strength of the vertex \\(i\\)$is defined as\n\\[ s_i = \\sum_{j \\in N_i^-} w_{ij} \\]\nwhere \\(N_i^-\\) is the in-neighborhood of \\(i\\) . The \\(s\\)-core is a subgraph where the nodes have at least strength \\(s\\). As long as \\(w_{ij} \\in Z^+\\), we can replace an edge in the graph with \\(w_{ij}\\) multi-edges, and the decomposition of the graph by strength and degree are equivalent. We call this coreness index, the labor market centrality index when applied to commuting networks.\nIllustration of network decomposition into core and periphery. Vertices are sized based on in-degree.\n The s‚Äìcore decomposition is illustrated in the above figure for directed graph with multiple edges including loops. The entire graph in the figure is part of 0-core. Nodes A and F have in-degree 0, and therefore are not part of the 1-core of the graph (subgraph induced by nodes B, C, D, E, G). Thus, the coreness of A and F is 0. In that 1-core of the subgraph, nodes D and G have in-degree 1. While they are not part of the 2-core of the graph, deleting them also renders B ineligible for 2-core. Thus, the coreness index of nodes B, D and G is 1. This process continues, till all nodes are assigned a coreness index. The vertices in the top 10 percentile of the index is classified as strong core and in the upper quartile, but not in the upper decile as weak core. The rest are periphery.\nWe use the 2011‚Äì2015 county-to-county commuting flow data from the American Community Survey. For the sake of exposition, we limit our analysis to the conterminous United States consisting of 3,109 counties. 134,869 pairs of counties have non-zero commuters, representing 1.4% of the possible links; the network is relatively sparse, a testament to the continuing importance of geographic distance for economic integration. These links represent 142.5 million commuters, of which 72% commuted within the same county. When the above coreness index is computed on this dataset, we call it Labor Market Centrality Index.\n Core \u0026amp; Periphery Structure of the United States ‚Ä¶","date":1566604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566604800,"objectID":"c4e72297e179410b4a412659f0611008","permalink":"https://nkaza.github.io/post/regional-employment-structure-using-labor-market-centrality-index/","publishdate":"2019-08-24T00:00:00Z","relpermalink":"/post/regional-employment-structure-using-labor-market-centrality-index/","section":"post","summary":"Network Coreness Index can be used to measure the structure of regional economy identified through a commuting network","tags":["research"],"title":"Regional Employment Structure Using Labor Market Centrality Index","type":"post"},{"authors":null,"categories":["scholarship"],"content":"There was a recent email on PLANET Google Groups asking about what the ‚Äòtop journals‚Äô were in the field. I wrote a response, while not directly answering the question. This post is an elaboration/edit of that post.\n Do we need journals any more? I say this as/has been an associate editor for journals and appreciating the full scope of publication infrastructure that publishers provide. I also appreciate the need for cognitive shortcuts to quickly evaluate the value of research by looking to the reputation of the journal. Perhaps these shortcuts are useful to promotion and tenure committees.\nBut I wonder what the point of a journal is, from the perspective of the reader. If we take a journal to be a thematically coherent collection of published research that validates and fixes particular approaches, methods, questions and evidentiary standards for a field, then a quick peek at the some of recent issues of the ‚Äòtop‚Äô journals invalidates this view (save for some special issues). The methodological and theoretical diversity is quite astounding and should rightly be celebrated. Perhaps, coherence is overrated.\nOne might argue that this diversity is precisely the point. The diversity in the bundle/collection allows for serendipitous discoveries, that the readers would not otherwise find when they are engaged in ‚Äòproject-directed‚Äô reading. However, it has been a minute since I have engaged in a cover-cover reading/skimming of a journal. It is almost never the case, when the journal is largely available through electronic databases through the library and not a paper copy that arrives in my mailbox (forget about going to the stacks). And it is certainly never the case, when I use search engines such as Google Scholar or ResearchGate. In fact, it is because of the imprecise nature of the search results that I found the most useful things in the unlikeliest places, not because of curated collections.\nWe are in an age of the great unbundling. We no longer have to buy cable package subscriptions; we consume individual clips on YouTube. Yet the publishers still insist on selling bundles of bundles to libraries. But the dam is breaking. Universities are requiring self-archiving (largely open access) of journal articles (see http://roarmap.eprints.org/ for a list of policy adopters) . Many authors publish their works on sites such as ResearchGate and Mendeley. Funders are now prohibiting publications in journals that do not provide open access. Incentives are aligned as well; authors want wide distribution of their articles rather than bundled behind restricted access. Web, preprint servers (arXiv, SSRN etc.), social networks (e.g. ResearchGate) coupled with reasonably good search engines solve the ‚Äòdistribution‚Äô problem. These new(ish) modes of distribution also solve the matching problem; matching authors to readers. We no longer have to rely on small number of contributors/content creators that published in particular venues (because disciplinary traditions, peer networks etc.). Nor do we have to rely on small audiences within the field, who subscribe to these journals. Journal as a distribution channel is largely passe.\nSo, why publish in journals at all? This is a different question than why publish journals. Ultimately, this circles back to Tej‚Äôs inquiry about the reputation of the journal. Journals/editors act as gate keepers, for better or for worse. Just the process of writing for a journal, often sharpens arguments. Revisions demanded by editors and reviewers often make the manuscript better. Being published in a journal, through algorithmic magic, places a work in a more visible position in the search results. These advantages are invisible to the reader. If this is the primary role for a journal then there are number of other models that can be adopted. See peer voting in StackOverflow, or SciRate, for example. Of course, I am not advocating unvetted comments section that is open to all.\nUltimately, we publish in journals not to reach the right audiences, but because that is what our professional standards, funding bodies and hiring committees demand. It is telling that both the lists Andrew forwarded, contain no open access journals (maybe this has to do with publication dates for these articles). It is also telling that these are rankings by Western academics with access to resources to access these journal bundles. These demands have become ends in and of themselves, rather than facilitating knowledge creation and standing on each other‚Äôs shoulders. It is time to rethink some of these.\n","date":1556928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556928000,"objectID":"e27fd0817467a8cc8491a57f2607584c","permalink":"https://nkaza.github.io/post/why-do-we-need-journals/","publishdate":"2019-05-04T00:00:00Z","relpermalink":"/post/why-do-we-need-journals/","section":"post","summary":"There was a recent email on PLANET Google Groups asking about what the ‚Äòtop journals‚Äô were in the field. I wrote a response, while not directly answering the question. This post is an elaboration/edit of that post.","tags":[],"title":"Why do we need journals anymore?","type":"post"},{"authors":null,"categories":["new-urban-analytics","urban-change"],"content":"      Introduction Extent Shape Adjustment Urban Compactness Scores Redux Significant changes in the compactness score Conclusions   Introduction  An updated method is published in\n Kaza, N. Landscape shape adjusted compactness index for urban areas. GeoJournal (2020). https://doi-org./10.1007/s10708-020-10262-9  The indices can be downloaded from UNC dataverse   In an earlier post, I demonstrated some of the problems associated with the Index of Moment of Inertia to characterise the compactness of urban form. One of the chief problems is related to the shape of the unit of analysis. In our case, it is the County. If the shape of the county is elongated, urban areas within these counties have conform to this shape. Because moment of inertia is calculated with respect to the circle, these areas are `unfairly‚Äô penalised.\nIllustration of the effect of landscape shape on IMI. County boundaries are in red, while black represents urban areas\n In the above figures, IMI for Manhattan (New York, NY) is 0.27. It is one of the densest places in the US and among the most compact by many other measures. Yet IMI puts in the middle of the pack of all the counties in the continental US. Similarly, Portland (Multanomah, OR) has IMI of 0.53, while somewhat high, is not indicative of the reputation of the city and the region as a icon of growth management in the country. Examples like these are abound.\nIn this post, I will provide an characterisation of a index that will mitigate this issue to some extent.\n Extent Shape Adjustment Recall that IMI was defined as the ratio of MIs of the urban area and corresponding disk.\n\\[IMI_{undaj} := \\frac{A^2}{2 \\pi \\sum_{i \\in S} s^2 (\\frac{s^2}{6} + d_i^2) \\unicode{x1D7D9}_i } \\]\nwhere \\(A\\) is the area of the urban area and \\(s\\) is the raster resolution. IMI for the boundary/landscape shape can also be similarly defined, as long as the raster resolution is the same;call it \\(IMI_p\\). Define adjusted IMI as\n\\[IMI_{adj} := IMI_{unadj}^{IMI_p}\\]\nRecall that IMI is \\((0,1)\\). \\(IMI_p\\) for elongated shapes is closer to 0 and for more compact shapes like square and circles are closer to 1. Urban areas within landscapes that are ‚Äòcompact‚Äô will have IMI similar to unadjusted IMI, while urban areas in elongated or in disconnected and distant (multi) shapes receive a much higher score than before. Since \\(IMI_p \u0026lt; 1\\), \\(IMI_{adj} \u0026gt; IMI_{unadj}\\). Thus, the adjustment is non-linear, monotonic and is bounded.\nThe ratios of MI of various polygons in the following figure illustrate the adjustment. Black represents urban areas, while the black circle is the corresponding circle of the same area. The red polygon is the county boundary, while the red circle is the corresponding circle of the same area as the boundary.\n Urban Compactness Scores Redux It should be noted that the raster data comes from the National Land cover dataset and is pre-processed using various procedures described elsewhere. The boundary files for the US counties are downloaded from the US Census and include water. Future indices could correct these boundaries to exclude non-developable land (including high slopes). At the moment, this version of the index only accounts for shape of the county, but these other extensions are relatively straightforward.\n {\u0026#34;x\u0026#34;:{\u0026#34;url\u0026#34;:\u0026#34;index_files/figure-html//widgets/widget_unnamed-chunk-1.html\u0026#34;,\u0026#34;options\u0026#34;:{\u0026#34;xdomain\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;allowfullscreen\u0026#34;:false,\u0026#34;lazyload\u0026#34;:false}},\u0026#34;evals\u0026#34;:[],\u0026#34;jsHooks\u0026#34;:[]} At the first glance, the map above is not that different from the map of unadjusted IMI compactness scores. This is partly because many of the counties whose scores are likely to change because of the county shape are small and choropleth maps are a terrible way to visualise the information. However, the stability of the compactness score is apparent in the table below, which shows the to most compact county in each Census Division, especially when compared against the unadjusted IMIs. Only in Pacific (San Francisco) and New England (Suffolk) the most compact county has changed from the previous list.\n Table 1: Most Compact Counties according to IMI    Division  County  IMI  IMI_unadj      East North Central  Marion County, Indiana  0.85  0.84    East South Central  Shelby County, Tennessee  0.70  0.67    Middle Atlantic  Kings County, New York  0.83  0.82    Mountain  Salt Lake County, Utah  0.77  0.76    New England  Suffolk County, Massachusetts  0.71  0.41    Pacific  San Francisco County, California  0.91  0.49    South Atlantic  Roanoke city, Virginia  0.85  0.84    West North Central  St.¬†Louis city, Missouri  0.80  0.73    West South Central  Dallas County, Texas  0.74  0.73      Significant changes in the compactness score It is thus useful to explicitly note where adjustment has changed the compact score most. Since percentage changes are susceptible to small numbers and because many of the counties have extraordinarily low scores (\u0026lt;.01), I only report the absolute changes.\n Table 2: Counties that ‚Ä¶","date":1556755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556755200,"objectID":"e49251e37ce53d3065586b8a690a53d8","permalink":"https://nkaza.github.io/post/landscape-shape-adjusted-compactness-index/","publishdate":"2019-05-02T00:00:00Z","relpermalink":"/post/landscape-shape-adjusted-compactness-index/","section":"post","summary":"A correction to Index of Moment of Inertia  that reduces the penalty of being in an elongated landscape","tags":["landscape-metrics"],"title":"Landscape Shape Adjusted Compactness Index","type":"post"},{"authors":null,"categories":["urban-change","new-urban-analytics","landscape-metrics"],"content":"            Introduction Shape Compactness Multiple Polygons, Holes and Other Realities Moment of Inertia Urban Compactness Scores Limitations Conclusions   Introduction Urban sprawl is a multi-dimensional phenomena. Chief among its characteristics are whether or not an urban area is fragmented (or pockmarked development) and if the urban area is contiguous but of uniformly low-density. In this post, I want to demonstrate the use of a relatively novel (but in some ways an age-old) indicator of urban form at a landscape level; Area Moment of Inertia.\nQuite a bit of the literature on compactness comes from Political Science, where folks are concerned the most about gerrymandering and how to create compact electoral districts. Chief among them are Polsby and Popper (1991), Schwartzberg (1965) and Reock (1961).\n Shape Compactness There are many ways we can describe the compactness of a shape. But we need a standard against which we can measure it. For that, we can assume that in 2D space, circle is the most compact shape for a given area. Note the important points here 1) keeping the same dimensions (otherwise a point is the most compact) 2) we need to fix an attribute, area in this case (again if we didn‚Äôt, point would be the most compact).\nOne way to define compactness of a shape is to divide the perimeter of the shape \\(P\\) to the area \\(A\\) (Perimeter-Area ratio) and see how closely it matches up with similar indicator for a circle However, since units of \\(P\\) and \\(A\\) differ, it is often useful to use \\(P^2/A\\). For a circle this quantity is \\(4 \\pi\\). So an index of compactness that normalised to a circle might be\n\\[ \\frac{4 \\pi A}{P^2} \\]\nThis is known Polsby-Popper index, though the origins of this idea are much older. See Haggett et.al (1977) for some older references. The index is \\((0,1]\\) with the least compact areas taking on values closer to 0.\nsource: https://www.azavea.com/blog/2016/07/11/measuring-district-compactness-postgis/\n Another way to recover the above indicator is to take the ratio of perimeters, with the numerator being the perimeter of the most compact shape possible for the given area \\(A\\) and the denominator being the perimeter of the shape in question (focal shape). The perimeter of the circle is \\(2 \\sqrt{\\pi A}\\). It is straightforward to notice that this is related to the above perimeter-area ratio and differ only by a square root (a non-linear but monotonic transformation). This also how Schwartzberg‚Äôs index is related to Polsby-Popper index.\nOne disadvantage of the above method is perimeter is very sensitive to the errors in the boundary. In particular, if the boundary becomes more rugged (technical term), the perimeter dramatically increases without concomitant increase in area.\nThe famous Koch curve. Perimeter increases dramatically with each iteration, but only marginal changes in the area\n Reock index is slightly different. It relies on the notion of circumscribing polygon of the focal shape. We can use a convex hull or a minimum bounding circle as this polygon. The idea is to take the ratio between the area of the focal shape and the circumscribing polygon, because clearly perimeter is susceptible to fractalisation issues.\nSource: https://fisherzachary.github.io/public/r-output.html\nThere area of circumscribing circle is proportional to\n\\[\\max_{p_i, p_j \\in P} \\|p_i - p_j\\|\\] where \\(p_i\\) and \\(p_j\\) are points on the perimeter \\(P\\)\nThis property makes the Reock index very sensitive to the elongated shapes and the direction of elongation. Think about the circumscribing circle of the following three shapes.\nSource: Polsby and Popper (1991)\n This is a particular problem for geospatial datasets because shapes are distorted by different projections differently and in different parts of the world based on underlying assumptions. The convex hull approach slightly mitigates this problem.\n Multiple Polygons, Holes and Other Realities Until now, we have primarily dealt with single shapes, albeit with fuzzy boundaries. Often, we encounter disjointed polygons that belong to the ‚Äòsame feature‚Äô, polygons with holes and other esoterica. These shapes are often a result of complicated geoprocessing operations such as unions, dissolves and intersections or mapping/surveying errors. None of the above metrics fare particularly well in describing the compactness, when confronted with these realities.There is little agreement about simple concepts like perimeters, areas and circumscribing polygons.\nA partial solution One way to mitigate against these issues and also to reduce computational burden would be to deal with the rasters. While fuzziness of boundaries are still an issue, perimeters of ‚Äòraster shapes‚Äô cannot get arbitrarily large for a given resolution (cell size). The other is to rely on Moment of Inertia (MI) to account for multiple polygons and holes. Tightrope walkers rely on this idea to prevent rotating around (falling from) the rope. See Tatiana-Mosio-Bongonga in Paris ‚Ä¶","date":1556064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556064000,"objectID":"66d67bd1387e4beff4d28b2ca213ab3a","permalink":"https://nkaza.github.io/post/a-compactness-index-for-urban-patterns/","publishdate":"2019-04-24T00:00:00Z","relpermalink":"/post/a-compactness-index-for-urban-patterns/","section":"post","summary":"Index of Moment of Inertia can be used to measure both the fragmentation levels and extent of fragmentation. This is a complementary measure to density to measure compactness of an urban landscape","tags":["research"],"title":"A compactness index for urban patterns","type":"post"},{"authors":null,"categories":["energy-policy","regional-science"],"content":"Introduction In this post, I am going to briefly talk about how the transportation energy consumption varies across United States. Much of the energy in that sector is from liquid fuels and the transmission and distribution network of these fuels have enormous land use and transportation implications. If we were to think about changing the fuel mix of the transportation fleet (say for example making hydrogen fuel cells or electricity more prominent), we need to think through its implications.\nThis work is published as Kaza, N. 2020 Urban Form and Transportation Energy Consumption, Energy Policy\nIn the United States (US), the transportation sector consumes about 29% of the total energy in 2017, rising from 23.5% in the 1960s even while the energy efficiency of the economy increased. Much of this energy comes from liquid carbon-based fuels contributing to greenhouse gas emissions and bad air quality. While we know a bit about how much energy is consumed in the transportation sector nationally, we don‚Äôt really have a grasp on how various local and state policies (incluidng the design, form and function of cities and regions) are affecting transportation energy consumption.\nThree components of energy consumption The three main components that explain energy consumption in the sector is the total volume of travel, mode split and average energy efficiencies of the fleet mix. Volume of travel depends on the distances between origins and destinations, network effects and trip frequencies. Mode split, the transportation mode that trip uses, is dependent on distances that need to be travelled, the availaibility and ease of mode not just for the particular trip, but for subsequent and prior chained trips and trip purposes (e.g. delivery of goods, shuttling kids). The energy efficiency of the fleet mix depends on adoption rates of newer vehicles, organisational/household acquisition rules and procedures and government policies on pollution control mechanisms, tax subsidies, depreciation rules etc.\nIn addition to these complicated mechanisms through which we can influence energy consumption, we should also be mindful of the different components of energy consumption. We need to account for transportation consumption both in freight and in person travel. Light duty vehicles with short wheel bases (passenger cars, vans, SUVs etc.) only accounted for 52% of highway transportation energy in 2016 (Bureau of Transportation Statistics 2018 Table 4.06). Assuming other vehicle types are largely associated with non-household travel, relying simply on household travel underestimates the total energy consumption in the system and the impact of urban form.\nSales in Gas stations Sub national information is not easily forthcoming for transportation sector. Since we can not simply proxy total energy consumption from VMT derived from household travel surveys, we need an alternative strategy. Because most of energy is purchased at retail gas stations in the US, we can study the geographic differences in energy consumption, by studying the variation in gas station sales.\nI use the 2012 Economic Census by the US Census Bureau to construct a proxy for transportation energy consumption, by analyzing the sales at gas stations in each county (or equivalent areas) of the United States. The Bureau collects extensive data on businesses every 5 years. I use the data is reported at a county level for the retail sector (North American Industrial Classification System (NAICS) code 44-45), in particular, the total sales receipts from the gas stations (NAICS 447). Due to confidentiality concerns, data for 344 counties are not reported.\nFrom the above map, it should be clear that there are some outliers (e.g. Culberson County, TX for per capita consumption) that might skew our understanding. It is also entirely possible that there might be data quality issues associated with those outliers.\nEnergy Consumption by County Character Of all the states, Wyoming and North Dakota are among the top of the per capita expenditure in gasoline stations, followed by the states in the Midwest. These states are characterized by low population and vast open spaces. The populous regions in the US, the Northeast and the Pacific are at the bottom of the per capita expenditures. District of Columbia has only $418 expenditures per capita suggesting potential explanations of large commuting population from nearby states (Virginia and Maryland in particular), extensive public transportation infrastructure and high population density.\nCounties outside metropolitan statistical areas have 41.2% more per capita sales than those within them. While these counties account for only 16.2% of the total population, this suggests that urbanization is associated with lower per capita consumption due to proximity of destinations and increased economic development. Finer urban type classification of the counties from National Center for Health Statistics (NCHS) reveal an even starker ‚Ä¶","date":1554163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554163200,"objectID":"e294b3e17a2c20c74c418cb26a12584a","permalink":"https://nkaza.github.io/post/transportation-energy-consumption/","publishdate":"2019-04-02T00:00:00Z","relpermalink":"/post/transportation-energy-consumption/","section":"post","summary":"While urban counties consume substantially more gasoline/diesel, rural counties are thrice as profligate as urban counties on per capita basis.","tags":["research"],"title":"Transportation Energy Consumption in the US","type":"post"},{"authors":["K. Peng","N. Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"affd6c07668f592bfeda73955ba30040","permalink":"https://nkaza.github.io/publication/peng-kaza-fruits/","publishdate":"2021-02-18T16:49:57.444077Z","relpermalink":"/publication/peng-kaza-fruits/","section":"publication","summary":"Objective: To determine whether neighbourhood supermarket and convenience store availability and broader built environment context are associated with food purchasing behaviour in a national population. Design: We used observational data to perform a cross-sectional study of food purchases for US households in 2010. We used three-level mixed-effect regression models to determine whether the associations between the number of neighbourhood supermarkets and convenience stores and the self-reported annual household expenditures for fruits and vegetables were affected by regional destination accessibility, neighbourhood destination diversity, availability of neighbourhood destinations and neighbourhood street connectivity. Setting: Metropolitan statistical areas (n 378) in the USA. Participants: Households (n 22 448). Results: Controlling for broader built environment context, there was no significant association between availability of neighbourhood supermarkets and expenditures on fruits and vegetables; instead, we observed an inverse association between the number of convenience stores and expenditures for fruits (P = 0¬∑001). The broader built environment context was associated with food purchase, although the magnitude was small: (i) higher regional destination accessibility was associated with higher expenditures for fruits (P ","tags":[],"title":"Built environment and the purchase of fruits and vegetables in United States households","type":"publication"},{"authors":["Kyle Onda","Jordan Branham","Todd BenDor","Nikhil Kaza","David Salvesen"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"d83e5d194253aef376472b65075d8e91","permalink":"https://nkaza.github.io/publication/onda-2019-ab/","publishdate":"2024-09-15T09:22:27.335914Z","relpermalink":"/publication/onda-2019-ab/","section":"publication","summary":"","tags":[],"title":"Does Removing Federal Infrastructure Subsidies Discourage Development","type":"publication"},{"authors":["Ke Peng","Nikhil Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"a3978ad9ef8d26809d3d7b8137685a30","permalink":"https://nkaza.github.io/publication/peng-2019-aa/","publishdate":"2024-09-15T09:22:27.467576Z","relpermalink":"/publication/peng-2019-aa/","section":"publication","summary":"","tags":[],"title":"Effects of Street Connectivity and Duration of Exposure on Visits to Fast Food Restaurants: A GPS based case study of Atlanta Region","type":"publication"},{"authors":["M. He","S. Bhamidi","N. Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"c7822cfbc77121512a743b9197924ab8","permalink":"https://nkaza.github.io/publication/he-2019-ab/","publishdate":"2021-02-18T16:49:56.145148Z","relpermalink":"/publication/he-2019-ab/","section":"publication","summary":"","tags":[],"title":"Intertemporal Community Detection in Bikeshare Networks","type":"publication"},{"authors":["K. Onda","P. Sinha","F. Stevens","A.E. Gaughan","N. Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"05dcb392f1877ae8e47c2a35ae9b11f3","permalink":"https://nkaza.github.io/publication/kaza-magpie-india/","publishdate":"2021-02-18T16:49:57.204903Z","relpermalink":"/publication/kaza-magpie-india/","section":"publication","summary":"The measurement and characterization of urbanization crucially depends upon defining what counts as urban. According to The Indian Planning Commission, less than a third of the Indian population lives in urban areas, and while Indian cities are increasingly important to the economy, India is perceived fundamentally as a rural country. In this paper, we show that this received wisdom is an artefact of the definition of urbanity and the official statistics vastly undercount the level of urbanization and its importance for development policies in India.We begin by creating temporally-consistent, high-resolution population maps from sub district level population data available from the Indian Census for 2001 and 2011. The modeling framework is a two-step process that applies a Random Forest-based model to generate a prediction weighting layer subsequently used to inform a gridded dasymetric redistribution of original census counts at 100 m resolution (Stevens et al. 2015). We then apply density thresholds, contiguity conditions, distance based clustering and minimum population sizes to construct urban agglomerations for the entire country. Compared to the official estimates, we find that this approach counts 8%-30% (depending on thresholds) more urban population in 2011. We find large urban agglomerations that span large portions of Kerala and the Gangetic plain. Thus, while official estimates count more cities in the country, we delineate fewer cities but large urban regions that span jurisdictional boundaries. This has implication for urban policies.","tags":[],"title":"Missing Millions: Undercounting Urbanisation in India","type":"publication"},{"authors":["Nikhil Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"c503ac4082d08ae05482cb72c1d7cab4","permalink":"https://nkaza.github.io/publication/kaza-2019-ac/","publishdate":"2024-09-15T09:22:27.603705Z","relpermalink":"/publication/kaza-2019-ac/","section":"publication","summary":"","tags":[],"title":"Urban Form and Transportation Energy Consumption","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"0268883395adf1c9a26a8e3c673ca292","permalink":"https://nkaza.github.io/publication/kaza-vainforesight/","publishdate":"2021-02-18T16:49:57.686815Z","relpermalink":"/publication/kaza-vainforesight/","section":"publication","summary":"Why are many plans not implemented? Common explanations for this question are planners have little power, or they failed to account for political or environmental uncertainty, or they failed to include to enough voices during the planning process. Weaving different strands of implementation and strategic planning literature, I provide an alternative account by challenging the premise that plans realise their potential only when they are implemented. I argue that theoretical frameworks that we base our understanding of plans and their purposes do not allow us to explain the ways in which plans are used. Monitoring implementation of plans, presupposes that we know what plans are there to monitor. It privileges published plans and ignores all the other plans that guide urban development.  By jettisoning implementation as a key criterion by which to evaluate the effectiveness of plans, we can begin to focus on myriad of ways in which plans are used by plan makers as well as others. A better question to ask is, ``How are these plans used and when are they useful?'' In asking those questions, we can create different evaluative frameworks for different types of plans. Some unimplementable plans are worth making.","tags":[],"title":"Vain Foresight: Against the Idea of Implementation in Planning","type":"publication"},{"authors":["N. Kaza","D. Brookshire","K. Toledo","M. Perrit","S. Murphy"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"de7404c1b61979d5121296b11b63d7b8","permalink":"https://nkaza.github.io/publication/kazaetal-2018-whoseplan/","publishdate":"2021-02-18T16:49:57.563575Z","relpermalink":"/publication/kazaetal-2018-whoseplan/","section":"publication","summary":"Plan quality evaluations have used conformance of the plan to standards or their use in subsequent deliberations as metrics. In this study, we argue that plans could be evaluated using other criteria such as utility for different users. We use the energy planning of thirty-five American Indian tribes as a case study to demonstrate this evaluation methodology. American Indian tribes have used energy planning to address interdependencies of mineral rights with economic development and tribal sovereignty as well environmental stewardship under uncertainty.  Our evaluation methodology explicitly acknowledges the complexity of information presented in the plans and evaluates them holistically with an eye towards plan users, who may be different from the plan makers. We evaluate whether or not interdependencies among actions are addressed and what types of uncertainties are explicitly 15considered and in what ways. This framework helps identify how different plan elements are useful for different entities.","tags":[],"title":"Whose plan is it anyway? Energy Planning by American Indian Tribes in the United States","type":"publication"},{"authors":["K. K. Gadiraju","R. R. Vatsavai","N. Kaza","E. Wibbels","A. Krishna"],"categories":[],"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666996,"objectID":"e2f4dd51bd5807fc7218d6c7efaa99bd","permalink":"https://nkaza.github.io/publication/gadiraju-2018-aa/","publishdate":"2021-02-18T16:49:56.018733Z","relpermalink":"/publication/gadiraju-2018-aa/","section":"publication","summary":"","tags":["image classification","image segmentation","informal settlements"],"title":"Machine Learning Approaches for Slum Detection Using Very High Resolution Satellite Images","type":"publication"},{"authors":null,"categories":["equity","new-urban-analytics","regional-science"],"content":"               Introduction This post was last updated on 2025-08-13\nIn addition to obvious networks, we can also use graph theory to think though some non-obvious applications. One place this often crops up in planning is to think about adjacency/proximity as a graph. See my post on employment centers and paper on delineating areas.\nIn this post, I will demonstrate how to use networks for other spatial analysis. We can think of polygons as nodes and two nodes are connected if they have a relationship. These relationships can be spatial (e.g.¬†sharing a boundary) or non-spatial (e.g.¬†sharing a common attribute) or a combination of both.\nAcquire data Much of data acquisition follows the post on tidycensus. So I won‚Äôt repeat much of it, except to provide an uncommented source code below. In this case, I am focusing on both the Carolinas.\noptions(tigris_use_cache = TRUE) library(tidyverse) library(tidycensus) library(tigris) library(sf) library(tmap) library(RColorBrewer) library(igraph) NC_SC \u0026lt;- c(\u0026#39;37\u0026#39;, \u0026#39;45\u0026#39;) # FIPS code for NC and SC. ### Download geographies of interest ctys \u0026lt;- tigris::counties() %\u0026gt;% filter(STATEFP %in% NC_SC) %\u0026gt;% st_transform(4326) %\u0026gt;% select(GEOID, NAME) ######### Download variables of interest. ### pop \u0026lt;- map_dfr(NC_SC, function(us_state) { get_decennial(geography = \u0026#34;county\u0026#34;, variables = \u0026#34;P1_001N\u0026#34;, state = us_state, geometry = FALSE ) }) %\u0026gt;% select(GEOID, pop_census = value) # Calculate Poverty Rate by first downloading people below poverty \u0026amp; number of people for whom poverty status is determined. # https://www.socialexplorer.com/data/ACS2022_5yr/metadata/?ds=ACS22_5yr\u0026amp;table=B17001 pov \u0026lt;- map_dfr(NC_SC, function(us_state) { get_acs(geography = \u0026#34;county\u0026#34;, variables = \u0026#34;B17001_002\u0026#34;, # People below poverty summary_var = \u0026#39;B17001_001\u0026#39;, # Denominator state = us_state, geometry = FALSE, year = 2022) }) pov_rate \u0026lt;- pov %\u0026gt;% rename(pop_acs = summary_est, pov_est = estimate) %\u0026gt;% mutate(pov_rate = pov_est/pop_acs) %\u0026gt;% select(GEOID, NAME, pov_est, pop_acs, pov_rate) ### Calculate Unemployment rate #### ### https://www.socialexplorer.com/data/ACS2022_5yr/metadata/?ds=ACS22_5yr\u0026amp;table=B23001 ### To do that first need to figure out Labor Force #### # Only focusing on Female Unemployment Rate. If you want to use the male ones, please use this commented code #lf_m \u0026lt;- paste(\u0026#34;B23001_\u0026#34;, formatC(seq(4,67,7), width=3, flag=\u0026#34;0\u0026#34;), \u0026#34;E\u0026#34;, sep=\u0026#34;\u0026#34;) # You may need to add them up to get the total labor force and total unemployed persons. lf_f \u0026lt;- paste(\u0026#34;B23001_\u0026#34;, formatC(seq(90,153,7), width=3, flag=\u0026#34;0\u0026#34;), \u0026#34;E\u0026#34;, sep=\u0026#34;\u0026#34;) lf \u0026lt;- map_dfr(NC_SC, function(us_state) { get_acs(geography = \u0026#34;county\u0026#34;, variables = c(lf_f), state = us_state, geometry = FALSE, year = 2022) }) #unemp_m \u0026lt;- paste(\u0026#34;B23001_\u0026#34;, formatC(seq(8,71,7), width=3, flag=\u0026#34;0\u0026#34;), \u0026#34;E\u0026#34;, sep=\u0026#34;\u0026#34;) # See above comment unemp_f \u0026lt;- paste(\u0026#34;B23001_\u0026#34;, formatC(seq(94,157,7), width=3, flag=\u0026#34;0\u0026#34;), \u0026#34;E\u0026#34;, sep=\u0026#34;\u0026#34;) unemp \u0026lt;- map_dfr(NC_SC, function(us_state) { get_acs(geography = \u0026#34;county\u0026#34;, variables = c(unemp_f), state = us_state, geometry = FALSE, year = 2022) }) lf_t \u0026lt;- lf %\u0026gt;% group_by(GEOID) %\u0026gt;% summarize(lf_est = sum(estimate, na.rm=T)) unemp_t \u0026lt;- unemp %\u0026gt;% group_by(GEOID) %\u0026gt;% summarize(unemp_est = sum(estimate, na.rm=T)) unemp_rate \u0026lt;- left_join(lf_t, unemp_t, by=\u0026#39;GEOID\u0026#39;) %\u0026gt;% filter(lf_est \u0026gt;0) %\u0026gt;% mutate(unemp_rate = unemp_est/lf_est) df \u0026lt;- left_join(pov_rate, unemp_rate, by=\u0026#39;GEOID\u0026#39;) df \u0026lt;- left_join(df, pop, by=c(\u0026#39;GEOID\u0026#39;)) rm(pov, pov_rate, unemp, unemp_rate, lf, lf_t, unemp_t, pop, lf_f, unemp_f)  Spatial Relationships as a Graph/Network If you have a any relationship between a pair of objects, you can represent it as a graph. Spatial relationships are no different and different types of spatial relationships present different graphs. Recall that if a relationship is present, then the nodes are considered adjacent.\nQueen Contiguity In the case of spatial data, adjacency can be defined by sharing a boundary segment (Rook) or a point (Queen).There are many other types of spatial relationships (e.g.¬†overlaps, contains) that might be of interest.\nRecall that a graph can be represented as an adjacency matrix where the nodes are both rows and columns. In this case, the adjacency matrix is a binary matrix, where 1 represents adjacency and 0 represents no adjacency. We can use spdep package to construct the neighbour list for common spatial relationships such as adjacency and distances.\nlibrary(spdep) cty_nb \u0026lt;- poly2nb(ctys, queen=TRUE, row.names = ctys$GEOID) # Construct a neighborhood object coords \u0026lt;- st_centroid(ctys, of_largest_polygon = T) %\u0026gt;% st_coordinates() plot(st_geometry(ctys), border = \u0026#39;gray\u0026#39;) plot(cty_nb, coords, col=\u0026#39;red\u0026#39;, points=F, add=T) # Quickly visualise the graph  K-nearest Neighbors There is no reason to think of adjacency as just sharing a boundary. It could be based on some other criteria. For example, we could think of adjacency as being close in distance. In this case, we could specify three closest neighbours as the adjacency. Note that this is not a symmetric relationship, ‚Ä¶","date":1537747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537747200,"objectID":"3c8bf07bbc5f2d0eb2260dcf5ff5699d","permalink":"https://nkaza.github.io/post/using-network-ananlysis-to-identify-clusters-of-deprivation/","publishdate":"2018-09-24T00:00:00Z","relpermalink":"/post/using-network-ananlysis-to-identify-clusters-of-deprivation/","section":"post","summary":"Spatial clusters of deprivation and their patterns","tags":null,"title":"Regions of Deprivation","type":"post"},{"authors":null,"categories":["new-urban-analytics","regional-science"],"content":"      Introduction Networks are ubiquitous in a urban systems. The more obvious ones are street, rail and utility networks that undergird the city. However, as planners we also focus on less obvious networks; e.g.¬†social networks of people that allow for community formation, the transactional activity of firms that allow us to identify clusters of firms or industries, the mismatch between job seekers and job locations to better plan for social and physical infrastructure and information flow through organisational linkages during a disaster management process. These are but a few examples of network analysis in the urban domain. The problem definition frames the analytical tools used in the process.\nAcquire Data Let‚Äôs begin by using the bike share data from New York, we are familiar with. We already did some network analysis with it (inadvertently). Let‚Äôs formalise it. Much of the following data cleaning that you should be familiar with now.\nKey nodes in a network Networks are representation of connections between two nodes. As such bike stations, could be treated a vertex/node and each trip between any two stations could be treated as a link. In another representation, you can treat all stations connected with one another, if they belong to the same company that you can check out and return these bikes and treat the number of trips as weights on that edge; so some edges will have zero weights. In other instances, you may disallow edges that have zero weights, i.e.¬†remove the connection between two stations if there are no trips between them (or number of trips below a threshold).\nlibrary(tidyverse) library(igraph) library(here) tripdata \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;nycbikeshare\u0026#34;, \u0026#34;201806-citibike-tripdata.csv\u0026#34;) %\u0026gt;% read_csv() tripdata \u0026lt;- rename(tripdata, #rename column names to get rid of the space Slat = `start station latitude`, Slon = `start station longitude`, Elat = `end station latitude`, Elon = `end station longitude`, Sstid = `start station id`, Estid = `end station id`, Estname = `end station name`, Sstname = `start station name` ) diffdesttrips \u0026lt;- tripdata[tripdata$Estid != tripdata$Sstid, ] # to make sure there are no loops or self-connections. (trips_graph \u0026lt;- diffdesttrips %\u0026gt;% select(Sstid,Estid) %\u0026gt;% graph.data.frame(directed = T)) #We are using directed graph because the links have from and to edges. You can choose to ignore them. # IGRAPH 2b31606 DN-- 773 1909858 -- # + attr: name (v/c) # + edges from 2b31606 (vertex names): # [1] 72-\u0026gt;173 72-\u0026gt;477 72-\u0026gt;457 72-\u0026gt;379 72-\u0026gt;459 72-\u0026gt;446 72-\u0026gt;212 72-\u0026gt;458 # [9] 72-\u0026gt;212 72-\u0026gt;514 72-\u0026gt;514 72-\u0026gt;465 72-\u0026gt;173 72-\u0026gt;524 72-\u0026gt;173 72-\u0026gt;212 # [17] 72-\u0026gt;382 72-\u0026gt;462 72-\u0026gt;3141 72-\u0026gt;456 72-\u0026gt;519 72-\u0026gt;328 72-\u0026gt;426 72-\u0026gt;3659 # [25] 72-\u0026gt;359 72-\u0026gt;525 72-\u0026gt;405 72-\u0026gt;458 72-\u0026gt;457 72-\u0026gt;3163 72-\u0026gt;426 72-\u0026gt;328 # [33] 72-\u0026gt;376 72-\u0026gt;3459 72-\u0026gt;304 72-\u0026gt;457 72-\u0026gt;486 72-\u0026gt;527 72-\u0026gt;490 72-\u0026gt;459 # [41] 72-\u0026gt;459 72-\u0026gt;3178 72-\u0026gt;3178 72-\u0026gt;515 72-\u0026gt;459 72-\u0026gt;388 72-\u0026gt;500 72-\u0026gt;508 # [49] 72-\u0026gt;173 72-\u0026gt;426 72-\u0026gt;368 72-\u0026gt;3258 72-\u0026gt;532 72-\u0026gt;478 72-\u0026gt;514 72-\u0026gt;533 # [57] 72-\u0026gt;519 72-\u0026gt;423 72-\u0026gt;3457 72-\u0026gt;525 72-\u0026gt;525 72-\u0026gt;490 72-\u0026gt;529 72-\u0026gt;405 # + ... omitted several edges vcount(trips_graph) # [1] 773 ecount(trips_graph) # [1] 1909858 is.directed(trips_graph) # [1] TRUE  You can access the vertices and edges using V and E functions.\nV(trips_graph) %\u0026gt;% head() # + 6/773 vertices, named, from 2b31606: # [1] 72 79 82 83 119 120 E(trips_graph) %\u0026gt;% head() # + 6/1909858 edges from 2b31606 (vertex names): # [1] 72-\u0026gt;173 72-\u0026gt;477 72-\u0026gt;457 72-\u0026gt;379 72-\u0026gt;459 72-\u0026gt;446 tmp1 \u0026lt;- diffdesttrips %\u0026gt;% group_by(Sstid) %\u0026gt;% summarise( stname = first(Sstname), lon = first(Slon), lat = first(Slat))%\u0026gt;% rename(stid = Sstid) tmp2 \u0026lt;- diffdesttrips %\u0026gt;% group_by(Estid) %\u0026gt;% summarise( stname = first(Estname), lon = first(Elon), lat = first(Elat)) %\u0026gt;% rename(stid = Estid) station_locs \u0026lt;- rbind(tmp1, tmp2) %\u0026gt;% unique()  You can extract subgraphs for a subset of vertices\nset.seed(200) # For reproducibility because of randomisation below station_sample \u0026lt;- sample(V(trips_graph), 20) sub_trips \u0026lt;- induced_subgraph(trips_graph, station_sample) # plot using ggraph library(ggraph) ggraph(sub_trips, layout = \u0026#39;kk\u0026#39;) + geom_edge_fan(show.legend = FALSE) + geom_node_point()  Note that the graph does not respect the geographic locations. If you want to fix the positions relative to their lat/long coordinates, you should can specify them using layout parameters.\n Exercise\n Use different layouts (such as star and cricle) to visualise the network. Use different edge attributes to style the edges of the above graph Style the vertices using for e.g.¬†number of incoming trips per day, or their location in different boroughs.   Adjacency matrix One representation of a graph is an adjacency matrix. It is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. Each row and each column represents a single vertex. $ A $ is and adjacency matrix, whose elements $ a_{ij} $ is 0 when vertices/nodes $ i $ and $ j $ are not ‚Ä¶","date":1536710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536710400,"objectID":"e0f23a90fa668baf5a388db433c78b2c","permalink":"https://nkaza.github.io/post/analysing-urban-neworks/","publishdate":"2018-09-12T00:00:00Z","relpermalink":"/post/analysing-urban-neworks/","section":"post","summary":"A brief introduction to network analysis.","tags":["R","teaching"],"title":"Analysing Urban Neworks","type":"post"},{"authors":["Nikhil Kaza","Yan Chen"],"categories":["remote-sensing","machine-learning"],"content":"\r\rIntroduction\rMachine learning (ML) is currently a buzzword in urban analytics. It is a process of automated model building that generates a predictive model that can reasonably explain not just the data that it is trained on, but generalised to other data from the same data generating process. Traditional models are rules that operate on data to produce and output. Machine learning approaches, on the other hand, usually take outputs and data to figure out the appropriate rules. While traditional models have to rely upon external justification for the rules, the promise of ML is that it discovers these rules empirically, without a theoretical basis for understanding the correlations among the different variables.One important thing to note about machine learning is that the models are restricted to the hypothesis space and the search is not among the arbitrary model specifications. For example, in machine learning, that is about logistic regression model, the features are restricted to enter the model in a linear fashion, where as in a decision tree, they behave non-linearly based on the partition. While this may be too esoteric for students who are starting out on understanding ML techniques, it is useful to temper the expectations regarding what kinds of models can we expect to be generated by the various algorithms. In other words, there is no guarantee that the ML model is the best model that explains and predicts the observed data. Practical ML is as much an art as it is a science.\nIt might be beneficial to illustrate some of the salient points about ML though a practical example that interests planners. Identifying objects and land use classes from remotely sensed images of urban areas.\n\rStages of ML approach\rThere are 5 distinct stages of Machine Learning. Let‚Äôs focus on supervised learning, a subset of ML approaches. In supervised learning, target outcome is known for a vector of features and the dataset consists of a collection of the features and target. So for example, land use class is frequently the target (dependent variable) and the features (independent variables) are various bands, indices, textures, proximity etc.\nIdentifying appropriate data sources, especially labelled data. Wrangle, Clean and Assemble (Data Preprocessing)\n\rFeature Engineering. Identify the right variable combinations from the independent variables.\n\rSplitting the data into training, validation and holdout.\n\rIterating over the algorithm to fit best explain the training dataset. Use the validation data to tune the model.\n\rChoosing the best model that does well (prediction) on the holdout dataset.\n\r\rML approaches are fundamentally iterative. I cannot emphasise this enough. While there are distinct steps in the approaches, because later stages crucially depend on earlier stages, all stages, except the last one, are iterative. We usually iterate to find better fitting algorithms to the data, which necessitates changes to feature engineering and selection as shown in the figure below.\nImage credit: Goyal (2018)\n\rIn the following steps, for the sake of brevity, I do not demonstrate the iterative aspects of ML.\n\rData Acquisition and Preprocessing\rFor this exercise, I am going to use a 3m, 4-band Planetscope image from around Wuhan, China. You can download it from around here. The 4 bands are Blue, Green, Red and Near Infra Red (NIR). These are initial set of features.\nlibrary(terra)\rlibrary(here)\rlibrary(tidyverse)\rlibrary(sf)\rwuhan_raster \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;wuhanremotesensing\u0026#34;, \u0026#34;20170914_022008_0f28_3B_AnalyticMS.tif\u0026#34;) %\u0026gt;% rast()\rnames(wuhan_raster) \u0026lt;- c(\u0026#39;Blue\u0026#39;, \u0026#34;Green\u0026#34;, \u0026#34;Red\u0026#34;, \u0026#39;NIR\u0026#39;)\rwuhan_raster\r# class : SpatRaster # dimensions : 4695, 9068, 4 (nrow, ncol, nlyr)\r# resolution : 3, 3 (x, y)\r# extent : 231990, 259194, 3372027, 3386112 (xmin, xmax, ymin, ymax)\r# coord. ref. : WGS 84 / UTM zone 50N (EPSG:32650) # source : 20170914_022008_0f28_3B_AnalyticMS.tif # names : Blue, Green, Red, NIR\rplotRGB(wuhan_raster, r=3, g=2, b=1, stretch=\u0026#39;hist\u0026#39;, main=\u0026#39;True color composite\u0026#39;) #TRUE colour composite\rplotRGB(wuhan_raster, r=4,g=3,b=2, stretch=\u0026#39;hist\u0026#39;, main = \u0026#39;False color composite\u0026#39;) # FALSE colour Composite\rThe labels are vector data derived from Openstreetmap data. It is available as part of the zip file you downloaded earlier. In particular, the labels are in the ‚Äòlanduse‚Äô class.\n library(sf)\rshp \u0026lt;- here(\u0026#34;tutorials_datasets\u0026#34;, \u0026#34;wuhanremotesensing\u0026#34;, \u0026#34;landuse3.shp\u0026#34;) %\u0026gt;% st_read\r# Reading layer `landuse3\u0026#39; from data source # `/Users/kaza/Dropbox/website_new/website/tutorials_datasets/wuhanremotesensing/landuse3.shp\u0026#39; # using driver `ESRI Shapefile\u0026#39;\r# Simple feature collection with 629 features and 25 fields\r# Geometry type: MULTIPOLYGON\r# Dimension: XY\r# Bounding box: xmin: 114.21 ymin: 30.45716 xmax: 114.4891 ymax: 30.57547\r# Geodetic CRS: WGS 84\rshp \u0026lt;- st_transform(shp, crs(wuhan_raster)) ## Note that shp was not in the same projection as raster, so transform it to make the spatial operations possible. In general, it is ‚Ä¶","date":1533600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533600000,"objectID":"7b49c9a560af9d2e50189f9cb05c2098","permalink":"https://nkaza.github.io/post/machine-learning-for-remote-sensing/","publishdate":"2018-08-07T00:00:00Z","relpermalink":"/post/machine-learning-for-remote-sensing/","section":"post","summary":"Classification trees, random forests, neural networks and the like.","tags":["teaching","R"],"title":"Machine Learning for Remote Sensing","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rIntroduction\rLegality\rAn example using Baidu\rAPIs for non-point data\rUnstructured data\rConclusions\rAcknowledgements\r\r\rIntroduction\rExtracting data from un/semi/structured websites is becoming increasingly common place. Since data is collected, modified and refined continuously, it is increasingly useful to both serve them via web protocols rather than flat files that are downloaded. Furthermore, much of spatial data collection has become private, which also means that firms have stronger incentives to protect their datasets and curate what is available to the others. In other instances, the user or the analyst requires only a small section of the dataset. In these instances and others, data is served by a web-based protocol. Harvesting data usually takes the form of automating the process of sending requests to the webserver and parsing the output to extract relevant data for storage and analysis.\n\rLegality\rDifferent jurisdictions have different legal restrictions and permissions on web scraping. There are also end user agreements that prevent certain actions (storage, retrieval, replication etc.). Please make sure that you are aware of these before attempting to make a local copy of the data that might be privately owned.\nIn general, scraping requires automated and repeated requests to the server. As long as these requests are not a disruptive rate, it is unlikely that you will run afoul of legality. Local data storage and replication of services is an entirely different ball game. Please consult a lawyer.\nMany public and government websites are also now serving up data using web protocols. It is, therefore, useful to learn how to parse the outputs of these requests. In some instances, private firms such as Google, Baidu, Instagram etc. also provide application programming interfaces (API) that serve data in a structured format. In these instances, subject to end user agreements, rate limits and daily quotas, restrictions on storage and transfer, it may be possible to access datasets that are otherwise unavailable.\n\rAn example using Baidu\rBaidu is a technology service company that provides a number of services including maps and social networking. According to Wikipedia,\n\rThe name Baidu (ÁôæÂ∫¶) literally means ‚Äúa hundred times‚Äù, or alternatively, ‚Äúcountless times‚Äù. It is a quote from the last line of Xin Qiji (ËæõÂºÉÁñæ)‚Äôs classical poem ‚ÄúGreen Jade Table in The Lantern Festival‚Äù (ÈùíÁéâÊ°à¬∑ÂÖÉÂ§ï) saying: ‚ÄúHaving searched hundreds of times in the crowd, suddenly turning back, She is there in the dimmest candlelight.‚Äù (‰ºóÈáåÂØª‰ªñÂçÉÁôæÂ∫¶ÔºåËì¶ÁÑ∂ÂõûÈ¶ñÔºåÈÇ£‰∫∫Âç¥Âú®ÁÅØÁÅ´ÈòëÁèäÂ§Ñ„ÄÇ)\n\rIn this post, we are going to query Baidu for points of interest around Wuhan, China. This is similar to Google Places.\nSetting up\rIn general, all API require registration and perusal of documentation, so that queries can be structured appropriately. In the case of Baidu, there are additional steps that are required so that the IP address of the computer that you are querying from is not blacklisted for abuse. Please see the documentation\n\rAcquiring API keys.\rEvery request to API requires a key so the website can control the how much and who can access the information. To acquire a key we need to :\nHave a Baidu account. Register at https://passport.baidu.com/v2/?reg\r\rFind your computer IP address. Preferably use ifconfig or ip138\r\rLogin your Baidu account and go to http://lbsyun.baidu.com/apiconsole/key?application=key, click ‚ÄúÂàõÂª∫Â∫îÁî®‚Äù to create a new application. Use the IP address from the previous step to ‚ÄúIPÁôΩÂêçÂçï‚Äù, then submit the application.\r\rAfter getting back to the application management page, make a note of the api key. You will need to use it in your R code.\r\r\r\rScraping\rThere are There are a few steps to scrape and visualize information fro web queries. In this post, we will use Baidu API as a example to scrape the resturants around Huazhong Agricultural University (HZAU). For the moment, we will deal with structured data. Parsing unstructured data is for a different time.\nThe steps are:\nIntialise your R session\rSet the parameters of the query\rSend the query repreatedly to get all the results\rCleaning and exporting the data\rVisualize the result\r\rThe two main packages, we are going to use for scraping the web is RCurl and rjson. Install them, if necessary and intialise them into the library. We will also use devtools package to install packages that are not on Comprehensive R Archive Network (CRAN), but on places like Github.\nCurl is a command line tool that allows us to transfer data especially over the web. Rcurl is an interface for that tool. Because the result of the API query is formatted in JavaScript Object Notation (JSON), we use RJSON to parse it easily. JSON is lightweight data-interchange format.\n##################### USE YOUR OWN KEY #########################\r### aquire the key from: http://lbsyun.baidu.com/apiconsole/key?application=key\rkey = \u0026#34;_YOUR_KEY_HERE_\u0026#34;\rlibrary(rjson)\rlibrary(RCurl)\rlibrary(tidyverse) # We have seen this package before. ‚Ä¶","date":1533254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533254400,"objectID":"ebb6bea2fb0723135831d4517bfe8e60","permalink":"https://nkaza.github.io/post/scraping-web-for-data/","publishdate":"2018-08-03T00:00:00Z","relpermalink":"/post/scraping-web-for-data/","section":"post","summary":"Parsing data that is served through REST API.","tags":["new-urban-analytics","techniques-short-course"],"title":"Scraping web for data","type":"post"},{"authors":null,"categories":["R"],"content":"  Course Description \u0026amp; Objectives Textbooks Course Policies Schedule   Course Description \u0026amp; Objectives This course is about different techniques used in assembling, managing, visualising, analysing and predicting using heterogeneous and messy data sets in urban environments. These include point, polygon, raster, vector, text, image and network data; data sets with high cadence and high spatial resolution; data sets that are inherently messy and incomplete. In addition to the mechanics of urban data analytics, we will also explore the issues of ethics and politics of data generation and analysis.\nPrerequisites Much of the analytical techniques will be taught using R. A working knowledge of the R environment is useful, though the first couple of labs, we will go over the basics. However, the course moves quickly. You are advised to seek help to keep up\n  Textbooks We will discuss the topics from the following two books in class. Students ae expected to read through the material before Day 1.\nO‚ÄôNeil, Cathy (2016). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. New York: Crown.\nTownsend, Anthony M (2013). Smart cities: Big data, civic hackers, and the quest for a new utopia. WW Norton \u0026amp; Company.\nThe following books are recommended for reference.\nBivand, Roger S, Edzer Pebesma and Virgilio G√≥mez-Rubio (2013). Applied Spatial Data Analysis with R. 2nd ed. 2013 edition. New York Heidelberg Dordrecht London: Springer. ISBN: 978-1-4614-7617-7.\nBrewer, Cynthia A. (2015). Designing Better Maps: A Guide for GIS Users. 2 edition. Redlands, California: Esri Press. ISBN: 978-1-58948-440-5.\nFew, Stephen (2004). Show Me the Numbers: Designing Tables and Graphs to Enlighten. Oakland, Calif: Analytics Press. ISBN: 978-0-9706019-9-5.\nGrolemund, Garrett and Hadley Wickham (2017). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, CA: O‚Äô Reilly media,. URL: http://r4ds.had.co.nz/ (visited on May. 25, 2018).\nTufte, E. R (2001). The Visual display of Quantitative Information. Cheshire, CT: Graphics Press.\n Course Policies Equipment Every student should have a working laptop that has R and Rstudio installed. The laptops should have sufficient memory and processing capacity to deal with large data sets.\n Grading  30% lab reports to be submitted at the end of the lab (Individual)\n 30% Daily homework assignments due by 11:59 PM (Individual)\n 20% Final project (Group)\n 10% Class \u0026amp; lab participation\n  Grading of labs and homeworks will be through Canvas. Instructions will be provided on the first day of class.\n Academic Conduct I firmly believe in learning from your peers and from others. All homework and lab submissions could benefit from collaborations, however, the submissions are individual. This means that interpreting the data and the results, producing the visualisations, drawing appropriate conclusions from the data is necessarily individual even when the strategies can be discussed and developed with others in class or out of class. All help, however, should be explicitly acknowledged. Severe penalties are imposed for non-attribution.\n  Schedule Day 1 8:30 AM - 11:40 AM (Lec \u0026amp; Lab): Introduction  Lecture. slides | UCLA slides Lab Session: Introduction to R. Notes   2:30 PM - 4:40 PM (Lab): Visualising urban data  Lab Session: Exploring large urban datasets. Vector data. Visualsing using small multiples, choropleth maps etc. Notes Homework: Due Day 1 11:59 PM in Canvas    Day 2 8:30 AM - 11:40 AM (Lec \u0026amp; Lab): Raster Analysis  Lecture. slides Lab Session: Basic raster analysis in R, Urban landscape metrics Notes 1 | Notes 2 Homework: Due Day 2 11:59 PM in Canvas   2:30 PM - 4:40 PM (Seminar) : The ethics of smart cities Students are expected to read through the material and be prepared to discuss the topics in class. This is a student driven discussion. Instructor will only facilitate.\nAssigned Readings Goodspeed, Robert (2014). ‚ÄúSmart cities: moving beyond urban cybernetics to tackle wicked problems‚Äù. In: Cambridge Journal of Regions, Economy and Society 8.1, pp.¬†79-92.\nHill, Dan (2008). The street as platform. URL: http://www.cityofsound.com/blog/2008/02/the-street-as-p.html (visited on Jun. 03, 2018).\nVanolo, Alberto (2014). ‚ÄúSmartmentality: The smart city as disciplinary strategy‚Äù. In: Urban Studies 51.5, pp.¬†883-898.\nWang, Tricia (2016). Why Big Data Needs Thick Data. URL: https://medium.com/ethnography-matters/why-big-data-needs-thick-data-b4b3e75e3d7 (visited on Jun. 03, 2018).\n   Day 3 8:30 AM - 11:40 AM (Lec \u0026amp; Lab): Classification \u0026amp; Machine Learning  Lecture. slides Lab Session: Remote sensing classification, machine learning. Notes Homework: Due Day 3 11:59 PM in Canvas   2:30 PM - 4:40 PM (Lec): Predictive Blackboxes \u0026amp; Algorithmic Biases Assigned Readings Rosenblat, Alex (2016). ‚ÄúThe truth about how Uber‚Äôs app manages drivers‚Äù. In: Harvard Business Review.\nTufekci, Zeynep (2015). ‚ÄúAlgorithmic Harms beyond Facebook and Google: Emergent Challenges ‚Ä¶","date":1532563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532563200,"objectID":"823154b5de54706ed0f258fa9516c4d9","permalink":"https://nkaza.github.io/teaching/techniques-politics-short-course/","publishdate":"2018-07-26T00:00:00Z","relpermalink":"/teaching/techniques-politics-short-course/","section":"teaching","summary":"A short course on urban analytics in R. And a critical look at urban analytics.","tags":["techniques-short-course","new-urban-analytics","teaching"],"title":"Techniques \u0026 Politics of New Urban Analytics","type":"teaching"},{"authors":null,"categories":null,"content":"The wind-swept, isolated beauty of coastal barriers has always attracted people. Coastal areas in general have undergone substantial population growth: from 1970 to 2010, the population of U.S. coastal counties increased by 34.8 million (39 percent). During this time, many of these isolated outposts of sand have undergone a boom in second-home and resort development (Crawford, et al., 2013), with coastal counties issuing an average of over 1300 building permits each day between 2000 and 2010 (Crossett, et al., 2013). The increase in coastal population strains the carrying capacities of coastal land and water. The low elevation, narrowness, and shifting sands of coastal barriers make them high-risk places for development. It is not uncommon for houses on coastal barriers to be washed away, rebuilt, and then destroyed again by a subsequent storm (Salvesen 2005). Growing populations and increased development result in greater damage from natural disasters and greater difficulty in evacuating coastal residents, as past coastal storms have vividly illustrated (e.g., hurricanes Sandy and Katrina).\nFor decades, the United States government has encouraged private development on coastal barriers through financial assistance for the construction of highways and bridges, water supply and wastewater treatment facilities, and beach stabilization projects (Glavovic and Smith, 2014; Burby, et al., 1999; Beatley, et al., 2002). Federal disaster assistance and flood insurance also have facilitated coastal development by transferring much of the risks and costs of development from the private sector to the public sector (DOI, 1983; Beatley, et al, 2002; Leichenko and Thomas, 2012). Significant literature ‚Äìpertaining to coastal development as well as other hazardous areas such as floodplains ‚Äì now argues that federal subsidies have perpetuated a cycle of subsidized development, destruction, and subsidized redevelopment (Berke et al., 2012). After a major coastal storm or hurricane sweeps across a coastal barrier, damaging or destroying homes, shops, roads, and water and sewer lines, federal disaster relief helps rebuild the damaged properties and infrastructure (Gillis and Barringer, 2012; Bagstad, et al., 2007; Burby, 2006).\nThis project aims to test the long-term effects of the withdrawal of development subsidies on development in coastal areas. Land markets respond to various kinds of signals about demand, supply, cost of construction, available tax credits, and subsidies. Growth controls ‚Äì which can include development regulations or subsidy restrictions ‚Äì are common local policy tools that have been shown to generallyincrease housing prices in the community (Elliot, 1981; Dowall and Landis 1982; and Katz and Rosen, 1987). While impacts on aggregate housing affordability depend upon the details of implementation, house value impacts of growth control policies are especially strong within the areas of control, making them a good indicator of policy effectiveness (Dawkins and Nelson 2002).\nCollaborators  Dr. Todd BenDor Dr. David Salvesen Kyle Onda Jordan Branham  ","date":1532563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532563200,"objectID":"b3edb210523f4ec209d0053767e8003b","permalink":"https://nkaza.github.io/project/coastal-policy/","publishdate":"2018-07-26T00:00:00Z","relpermalink":"/project/coastal-policy/","section":"project","summary":"How does infrastructure or lack of affect urban development along the coast","tags":["urban-change","urban-patterns","urban-analytics"],"title":"Urban Development, Coastal Infrastructure and Policy ","type":"project"},{"authors":null,"categories":["R","techniques-short-course"],"content":"     Introduction In this post, I am going to demonstrate some methods that can be used to identify clusters/hotspots. Clusters of points are usually locations where there are higher than expected frequency of incidents happening. These could be clusters of disease incidences, accidents, flood insurance claims, fatalities etc. Identifying where these clusters exists and are emerging is important to take either mitigating or preventative actions.\n Acquire Data We are going to use crime data from data.police.uk for the greater Manchester Area. The data on this site is published by the Home Office, and is provided by the 43 geographic police forces in England and Wales, the British Transport Police, the Police Service of Northern Ireland and the Ministry of Justice. Most of spatial data boundaries can be acquired either from Ordinance Survey or the Census.\n After I started creating this tutorial, I found a tutorial by Dr.¬†Fabio Veronesi that is remarkably similar in the illustrative dataset, though the topics covered are slightly different. Check it out!\nSince I wrote this tutorial, many have transitioned to using sf objects instead of sp objects. Please adapt as necessary.   Crime data for Manchester clipped from January 2016 - May 2018 is here. You can also download the LSOA and the boundary file from here.\n Additional resources I strongly recommend that you read through Spatial Point Patterns: Methodology and Applications with R by by Adrian Baddeley, Ege Rubak and Rolf Turner.\nIt is also quite useful to peruse the documentation of CrimeStat and GeoDa.\n Requirements This requires R, and many libraries including spatstat,spdep, rgdal, aspace and leaflet. You should install them, as necessary, with install.packages() command.\n Caveats The crime data, especially the location data, is anonymised . This poses some problems, as the anonymisation is primarily assigning the point to the center of the street. There may be many crimes on the street that get the same location. To get around this I randomly re-jitter the points by a small \\(\\epsilon\\). The following function is a quick way to introduce noise. This introduction of noise, might be problematic for some applications. You will have to figure out how to deal with the issue of duplicate locations one way or the other.\njitter_longlat \u0026lt;- function(coords, km = 1) { n \u0026lt;- dim(coords)[1] length_at_lat \u0026lt;- rep(110.5742727, n) # in kilometers at the equator. Assuming a sphere length_at_long \u0026lt;- cos(coords[,1]* (2 * pi) / 360) * 110.5742727 randnumber \u0026lt;- coords randnumber[,2] \u0026lt;- runif(n, min=-1,max=1) * (1/length_at_lat) * km randnumber[,1] \u0026lt;- runif(n, min=-1,max=1) * (1/length_at_long) * km out \u0026lt;- coords out[,1] \u0026lt;- coords[,1] + randnumber[,1] out[,2] \u0026lt;- coords[,2] + randnumber[,2] out \u0026lt;- as.data.frame(out) return(out) } ls() # [1] \u0026#34;filename\u0026#34; \u0026#34;i\u0026#34; \u0026#34;jitter_longlat\u0026#34; \u0026#34;manchesterbnd\u0026#34; # [5] \u0026#34;manchesterlsoa\u0026#34; \u0026#34;streetcrime\u0026#34; \u0026#34;subdirs\u0026#34; streetcrime[,c(\u0026#34;Longitude\u0026#34;, \u0026#34;Latitude\u0026#34;)] \u0026lt;- jitter_longlat(streetcrime[,c(\u0026#34;Longitude\u0026#34;, \u0026#34;Latitude\u0026#34;)], km=.6) sum(duplicated(streetcrime[,c(\u0026#34;Longitude\u0026#34;, \u0026#34;Latitude\u0026#34;)])) # [1] 0 #Convert it into spatial points data frame coordinates(streetcrime) \u0026lt;- ~Longitude+Latitude streetcrime@data[,c(\u0026#34;Longitude\u0026#34;, \u0026#34;Latitude\u0026#34;)]\u0026lt;- coordinates(streetcrime) wgs84crs \u0026lt;- CRS(\u0026#34;+proj=longlat +datum=WGS84\u0026#34;) proj4string(streetcrime) \u0026lt;- wgs84crs  Visualisation \u0026amp; Explorations As with any datasets, the first and foremost thing to do is to explore the data to understand its structures, its quirks and what if anything need to be cleaned.\ntheme_set(theme_tufte()) g_street \u0026lt;- ggplot(streetcrime@data) + geom_bar(aes(x=fct_infreq(Crime_type))) + coord_flip() + facet_wrap(~year)+ labs(x=\u0026#39;Crime Type\u0026#39;, y=\u0026#39;Count\u0026#39;) #table(bicycletheft$Last_outcome_category) g_street  library(ggrepel) k \u0026lt;- streetcrime@data %\u0026gt;% count(Month, fct_infreq(Crime_type), sort = FALSE) names(k) \u0026lt;- c(\u0026#39;Month\u0026#39;, \u0026#34;Crime_type\u0026#34;, \u0026#34;n\u0026#34;) k %\u0026gt;% mutate(label = if_else(Month == max(Month), as.character(Crime_type), NA_character_))%\u0026gt;% ggplot(aes(x=Month, y=n, col= Crime_type))+ geom_smooth()+ scale_colour_discrete(guide = \u0026#39;none\u0026#39;) + geom_label_repel(aes(label = label), nudge_x = 1, na.rm = TRUE) The trends should be readily apparent. Violence and Sexual Offences, Public Order crimes have increased significantly, while Anti-social behaviour crimes have declined from 2016 to 2018 Q2. These statistics include the Manchester Arena bombing in May 2017. One thing to notice though, is how few reported crimes actually result in an outcome. I am not entirely sure, if this a Manchester issue or if it is a general criminal justice issue.\nAlso note the mixing of %\u0026gt;% and + in the following code. ggplot uses + to build its graphics, while the rest of the analysis can be done with maggittr‚Äôs %\u0026gt;%.\n g_street_outcome \u0026lt;- streetcrime@data[!is.na(streetcrime$Last_outcome_category),]%\u0026gt;% ggplot() + geom_bar(aes(x=fct_infreq(Last_outcome_category), y = (..count..)/sum(..count..) * 100)) + coord_flip() + labs(x=\u0026#39;Outcome\u0026#39;, y=\u0026#39;Percent\u0026#39;) g_street_outcome It is ‚Ä¶","date":1531094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531094400,"objectID":"7e2e5969ead9b6d7a69ce7d38b64e067","permalink":"https://nkaza.github.io/post/cluster-detection-in-point-data/","publishdate":"2018-07-09T00:00:00Z","relpermalink":"/post/cluster-detection-in-point-data/","section":"post","summary":"Spatial cluster detection.","tags":["new-urban-analytics","spatial-statistics","machine-learning","unsupervised-classification"],"title":"Identifying Clusters of Events","type":"post"},{"authors":null,"categories":["regional-science","policy-evaluation"],"content":"      Point Set Theory and the DE-9IM Matrix   Introduction The current gold standard for testing the efficacy of an intervention/program/procedure requires this randomisation of participants to treatment and control. The following video by J-PAL gives a simple explanation of why this may be the case.\n  In many cases, randomisation to some program is not feasible. There may be political issues, information leaks and other real world problems that affect the conclusions that can be drawn. There are also number of theoretical critiques for randomised controlled trials.\n Regression Discontinuity Sometimes it is possible to exploit the sharp discontinuity in program application to see what the effect of the program is at the discontinuity. The key intuition is that at the a priori selected discontinuity, in a small window on either side of the discontinuity, the observations are essentially similar except for the treatment. For example, Ellen, Horn \u0026amp; Schwartz (2016) test the effect of residential location choices of households, whose oldest child becomes eligible for kindergarten. To do this, they focus on voucher receiving households whose oldest child just met the kindergarten eligibility cut-off date and those who missed the cut-off. The rating variable in this case is age in years by Sep 1 (or some other date) and cut-point is 5 years. Because the cut-off is determined independent of the participants in the program and by looking at the effect of vouchers in a local neighborhood around the cut-off, they determined the effect of vouchers on residential mobility.\nThere are multiple ways to conceptualise this design; 1) Discontinuity at the cut-point 2) local randomization at the cut-point. See below images for visualisation. In particular, please pay attention to the boxes around the cut-off that symblolise local neighborhood.\nImages adapted from Jacob et. al (2012)\n Spatial Regression Discontinuity It is relatively straightforward to construct a regression discontinuity when the rating variable is on a real line and cut-point is a point on that line, due to fact that real numbers is a totally ordered set, in particular, the comparability property w.r.t. \\(\\lt\\) holds. However, it is often the case that the discontinuity is a spatial one.\nFor example, a subsidy for rooftop solar is implemented by a city within its jurisdiction, and we want to test if the subsidy increased the adoption of rooftop solar. The idea is parcels and households on both sides of the edge of the jurisdictions are similar and the key difference is the eligibility to the subsidy and therefore comparing the adoption rates among those households will help us identify if the subsidy has any effect.\nAnother example could be the effect of minimum wage regulation on employment levels as studied by Dube, Lester and Reich (2010). They do not employ a regression discontinuity design, but a matching design, but the intuition still applies. Counties at the state borders are likely to be similar and state policy on minimum wage is the only differentiator and differences in employment trends among these subset of counties (at the border) are an estimate of whether minimum wage policies reduce employment levels or not.\nSimilar design for testing the effect of Clean Cities Coalition (CCC) program on air quality and number of alternative fueling stations in counties within and outside the coalition boundaries can be found in Qiu and Kaza (2016). In this study, we not only use the spatial discontinuity of the boundaries but also the temporal discontinuity of when the CCC came into existence.\nIllustration of research design in Qiu \u0026amp; Kaza (2016)\n There are some key differences between standard regression discontinuity and the spatial version. See Keele and Titiunik (2015). They are 1) different measures of distance from the cutoffs may require different identification assumptions 2) Compound treatments 3) Boundary points at the cut-off have different interpretation.\nFor the most part, I am going to ignore these differences in this post for the sake of illustration. In particular, when polygonal entities (instead of points) are the observations of interest, many of the differences are not that important.\n Identifying relevant observations on either side of the spatial discontinuity. Let us assume that we have a discontinuity at the edge of the metropolitan statistical areas. For the purposes of illustration, I am going to download CBSA and county shapefiles from Census using tigris package. I am going to restrict my attention to Illinois for the sake of exposition.\n cbsa \u0026lt;- core_based_statistical_areas() cty_shp \u0026lt;- counties(state=\u0026#34;il\u0026#34;) #library(rgeos) library(sf) library(tidyverse) library(tigris) msa \u0026lt;- cbsa[cbsa$LSAD==\u0026#34;M1\u0026#34;,] # Restrict attention only to Metro areas, ignore Micro msa_IL \u0026lt;- msa[st_centroid(cty_shp, byid=T),] # Select metros that are in IL.   If we used the ‚Äú[‚Äù method using a polygon instead of points it selects MSAs that are also outside IL ‚Ä¶","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"c79f9c8799e16ad6c90ff0b599210522","permalink":"https://nkaza.github.io/post/spatial-regression-discontinuity-design-setup/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/post/spatial-regression-discontinuity-design-setup/","section":"post","summary":"Exploiting sharp edges at the border of a jurisdiction to test the effect of a program/policy.","tags":["R","research"],"title":"Spatial Regression Discontinuity Design Setup","type":"post"},{"authors":null,"categories":["regional-science","new-urban-analytics"],"content":"            In this post, I am going to show the techniques behind identifying centers. Centers are defined collections of contiguous high value (e.g.¬†employment, opportunity, activity etc.) locations. This is related to, but different from using autocorrelation statistics such as Local Indicators of Spatial Autocorrelation. The methods described in this post draw from McMillen (2001) and Giuliano \u0026amp; Small (1991).\nRequirements This requires R, and many libraries including igraph,spdep, rgdal, tigris locfit and leaflet. You should install them, as necessary, with install.packages() command.\nMost of the methods and results are discussed in Hartley, Kaza \u0026amp; Lester (2016) and Lester, Kaza \u0026amp; McAdam (in review). Please refer to those articles.\nAdditional resources I strongly recommend that you read through some of the tutorials on using R for GIS. You should have Applied Spatial Data Analysis in R by Bivand et.al in your library.\nAcquire data The data I will use in this exercise will be downloaded directly from Census. For this work, we will use the Work Area Characteristics files from Local Origin Destination Employment Statistics (LODES). LODES is a synthetic data set that provides, if not spatially accurate, distributionally consistent annual employment information as well as commuting information. We will restrict our attention to the Kentucky. Let‚Äôs download and set up the data.\nlibrary(data.table) library(rgdal) library(spdep) library(rgeos) library(igraph) library(ggplot2) library(tigris) library(tidyverse) library(sf) state \u0026lt;- \u0026#34;ky\u0026#34; baseurl \u0026lt;- \u0026#39;http://lehd.ces.census.gov/data/lodes/LODES7/\u0026#39; years \u0026lt;- as.character(2015) wac_file \u0026lt;- NULL for (year in years){ filename \u0026lt;- paste(state, \u0026#34;_wac_S000_JT00_\u0026#34;, year, \u0026#34;.csv.gz\u0026#34;, sep=\u0026#34;\u0026#34; ) #State, S000-Total Number of jobs , JT00,- All jobs url \u0026lt;- paste(baseurl, state, \u0026#34;/wac/\u0026#34;, filename, sep=\u0026#34;\u0026#34; ) if(!file.exists(filename)){download.file(url, filename, mode=\u0026#39;wb\u0026#39;)} wac_file[[year]] \u0026lt;- data.table(read.csv(filename, colClasses = c(\u0026#39;character\u0026#39;, rep(\u0026#39;integer\u0026#39;,51), \u0026#39;character\u0026#39;))) ##based on the documentation of the classes wac_file \u0026lt;- rbindlist(wac_file, idcol=\u0026#34;Year\u0026#34;, use.names = TRUE) } ky_tr \u0026lt;- tracts(\u0026#34;Kentucky\u0026#34;, year=2010, progress_bar =FALSE)   I am using read.csv only because I am using a much faster data.table package for illustration purposes. The code can be easily adapted to tibbles using read_csv.   Data preparation \u0026amp; exploration The data set is at a block level. This level of geography is far too fine (28,996 blocks) for our analyses. Lets aggregate it up to census tracts. Since blocks are nested within the tract, all we need to do is to trim the GEOID and aggregate the data.table to the trimmed GEOID.\nwac_file$w_geocode_tr \u0026lt;- substr(wac_file$w_geocode, 1,11) #trim GEOID to 11. Trim it to 12 if Blockgroups are needed setkey(wac_file, w_geocode_tr, Year) cols = sapply(wac_file, is.numeric) #identify the columns where summation can be applied. cols = names(cols)[cols] wac_file_tr \u0026lt;- wac_file[,lapply(.SD, sum), .SDcols = cols, by=list(w_geocode_tr, Year)] # Aggregate the columns to tract ids. #C000 is the column that contains the numbers for total jobs. summary(wac_file_tr$C000) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 1.0 250.5 763.5 1646.8 1950.5 57291.0  We have now a manageable number of geographic units (1,112 tracts). The summary statistics look pretty skewed. It is useful to visualise the inequality of the distribution. We can use Lorenz curve for this. We need the ineq package for this.\nlibrary(ineq) plot(Lc(wac_file_tr$C000), xlab=\u0026#34;Cumulative % of tracts\u0026#34;, ylab=\u0026#34;Cumulative % of Jobs\u0026#34;) abline(v =.5, lty=2, col=\u0026#39;blue\u0026#39;) abline(h=.09, lty=2, col=\u0026#39;blue\u0026#39;) abline(v=.9, lty=2, col=\u0026#39;red\u0026#39;) abline(h=.53, lty=2, col=\u0026#39;red\u0026#39;)  The bottom 50% of the tracts only contribute to less than 10% of the total employment, while the top 10% of the tracts contribute to more than 45% of employment. About 11% of tracts have less than 100 jobs. This shows that jobs are pretty well concentrated in particular centers. We can visualise the spatial distribution of jobs. We already used tigris to download and load the polygons, and leaflet to visualise the information. Tigris is a convenience package that automatically downloads TIGER files from Census for particular geography. Visualising complicated geography is problematic, so we will use a polygon simplifier Visvasalingam algorithm to simplify the boundaries. As long as the topology is preserved, we don‚Äôt loose much for this analysis.\n##Simplify shape for display and analysis. As long as the topology is preserved, complicated boundaries are not necessary for this purpose. library(rmapshaper) ky_tr_simple \u0026lt;- rmapshaper::ms_simplify(ky_tr) c(object.size(ky_tr), object.size(ky_tr_simple)) # [1] 20041168 2299040  Merge the spatial polygons with the WAC file.\nky_tr_simple \u0026lt;- ky_tr_simple %\u0026gt;% left_join(wac_file_tr[Year==year, .(w_geocode_tr, C000),], by = c(\u0026#34;GEOID10\u0026#34;=\u0026#34;w_geocode_tr\u0026#34;)) ky_tr_simple$C000[is.na(ky_tr_simple$C000)] \u0026lt;- 0 #set NAs to 0. ‚Ä¶","date":1530662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530662400,"objectID":"316cb7a4cf4b341f448f9fe3a6e09bb2","permalink":"https://nkaza.github.io/post/identifying-employment-centers/","publishdate":"2018-07-04T00:00:00Z","relpermalink":"/post/identifying-employment-centers/","section":"post","summary":"McMillen method of identifying outliers in local neighborhood.","tags":["R","research"],"title":"Identifying Employment Centers","type":"post"},{"authors":null,"categories":null,"content":"Collection of your personal information Personal information (email and name) is collected only when attempting to contact me via Formspree.io. Visiting this website does not require you reveal any identification information. However, Google Analytics is enabled on this site. Hugo GDPR options are enabled, so that your IP address is anonymised within Google Analytics and the ‚ÄúDo Not Track‚Äù request is respected.\nNothing on this website requires you to identify yourself. The only information collected is through Google Analytics. Only aggregated reports are availble to me, however, Google might have more information about you.\nGoogle collects this information via cookies. Most web browsers allow you to control handling of cookies. You can disable all cookies for this website without in any way reducing the functionality for you.\nSharing of your personal information I don‚Äôt collect your personal information, so there is nothing I can share.\nGoogle Analytics does collect some information about you. See the Google Analytics privacy page. .\nUse of your personal information For each visitor to reach the site, Google Analytics collects the following some identifiable information, including but not limited to browser type, version and language, operating system, pages viewed while browsing the site, page access times and referring website address. This information is presented to me as aggregated reports for the purpose of gauging visitor traffic and trends.\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://nkaza.github.io/privacy/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"Collection of your personal information Personal information (email and name) is collected only when attempting to contact me via Formspree.io. Visiting this website does not require you reveal any identification information.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"It is notoriously hard to measure and track ‚Äòurban‚Äô as there are multiple dimensions to it; social, demographic, geographic, perceputal, environmental and patterns of growth and decline. In this long standing research project, we seek to provide some novel ways of understanding our urban environments operate and change, and how those changes shape our behaviours. In doing so, we seek to understand the limitations as well as the suitability of measurements from various types of data streams (administrative to environmental sensors). By tracking the changes in these measurements, we seek characterise urban change around the world.\nFor example, in McCarty \u0026amp; Kaza(2015) we used data from different air pollution sensors in a county to calculate the number of days with bad air quality in a year. We then relate it to urban fragmentation metrics obtained from land cover data. We find that while counties with higher forest land area have higher number of polluted days. This effect is mitigated when these forests are closer to urban areas. Similarly, urban form measured as land use density or type or measured as geomorphometry have been shown to have different on energy consumption patterns in cities.\nAt the same time, urban economy is also rapidly changing. One narrative of the cities argues for understanding and applauding the renaissance of cities during the decades of 1990s and 2000s (comeback cities). Another narrative suggest that this renaissance is highly uneven and is concentrated in innovative regions, while cities in the rest of the regions (backwards) are loosing ground. To provide evidence for explanations and to disentangle the causes of these phenomena are some of the main motivations of this project.\nA natural outgrowth of this research is produce different plausible futures of urban form in different places. Since the configuration as well as the extent of urban form matter for climate as well as environmental outcomes, we seek to develop methods for projecting different futures based on various patterns, we observe in the past and learn from them.\nOur goal is to expand the repertoire of urban areas to include areas from various parts of the world, including Asia, Africa and South America.\nCollaborators  T. William Lester Josh McCarty  ","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"48901b027f5f008ed954af7a0c14d9a6","permalink":"https://nkaza.github.io/project/urban-change/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/urban-change/","section":"project","summary":"Novel measures of urban demographic, economic and morphological indicators.","tags":["urban change","landscape metrics","economy","urban morphology"],"title":"Urban Change \u0026 Its Impacts","type":"project"},{"authors":["Nikhil Kaza","Parmanand Sinha","Forrest Stevens","Andrea E. Gaughan"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"8ffbd947d75cecf9c08c84f1c642c3e7","permalink":"https://nkaza.github.io/publication/kaza-2018-aa/","publishdate":"2024-09-15T09:22:27.196847Z","relpermalink":"/publication/kaza-2018-aa/","section":"publication","summary":"","tags":[],"title":"The Missing Millions: Undercounting Urbanisation in India","type":"publication"},{"authors":["Chaosu Li","Yan Song","Nikhil Kaza"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"a730c9af85170b5c6891e74ad2f08702","permalink":"https://nkaza.github.io/publication/li-2017/","publishdate":"2021-02-18T16:49:57.812179Z","relpermalink":"/publication/li-2017/","section":"publication","summary":"Abstract While urban form affects building energy consumption, the pathways, direction and magnitude of the effect are disputed in the literature. This paper uses a unique dataset to examine the effect of urban form on residential electricity consumption in Ningbo, China. Using survey and utility bill data of 534 households in 46 neighborhoods in the city, we model the electricity use of households using a multi-level regression model. We find that neighborhood street configuration and tree shade are important in controlling residential electricity consumption and, consequently, greenhouse gas emissions. Our results suggest that seasonality and dwelling type condition the effect of neighborhood densities on electricity consumption. Neighborhood density is associated with household electricity consumption in summer months, while there is no such association in the winter months. As neighborhood density increases, households in slab and tower apartments in dense urban neighborhoods consume more electricity in summer months, which can be partly explained by exacerbated heat island effect. Interestingly, the neighborhood density is negatively associated with electricity consumption for single-family houses, suggesting that the effect of neighborhood density is different for different types of dwelling units.","tags":[],"title":"Urban form and household electricity consumption: A multilevel study","type":"publication"},{"authors":["S. Qiu","N. Kaza"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"45e5839070df2ec760e4d228eb786fce","permalink":"https://nkaza.github.io/publication/qiu-2017-jk/","publishdate":"2021-02-18T16:49:57.936915Z","relpermalink":"/publication/qiu-2017-jk/","section":"publication","summary":"The Department of Energy's Clean Cities program was created in 1993 to reduce petroleum usage in the transportation sector. The program promotes alternative fuels such as biofuels and fuel-saving strategies such as idle reduction and fleet management through coalitions of local government, non-profit, and private actors. Few studies have evaluated the impact of the program because of its complexity that include interrelated strategies of grants, education and training and diversity of participants. This paper uses a Difference-in-Differences (DiD) approach to evaluate the effectiveness of the program between 1990 and 2010. We quantify the effectiveness of the Clean Cities program by focusing on performance measures such as air quality, number of alternative fueling stations, private vehicle occupancy and transit ridership. We find that counties that participate in the program perform better on all these measures compared to counties that did not participate. Compared to the control group, counties in the Clean Cities program experienced a reduction in days with bad air quality (3.7%), a decrease in automobile commuters (2.9%), an overall increase in transit commuters (2.1%) and had greater numbers of new alternative fueling stations (12.9). The results suggest that the program is a qualified success.","tags":["\"Air quality\"","\"Alternative fuel vehicles\"","\"Policy evaluation\"","\"Vehicle miles traveled\""],"title":"Evaluating the impacts of the clean cities program","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://nkaza.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://nkaza.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"About a third of the urban population in the world (~830 million) live in irregular/informal settlements/favelas/shanty towns/slums (United Nations Development Programme 2017). Whatever names these neighbourhoods take around the world, these settlements are at the core of many urban regions around the world; they provide affordable housing for urban poor but at high social and health costs. In urban areas, these types of settlements can constitute 30%-60% of the city and in some regions around the world, these settlements are the dominant urban form surpassing the formal settlements (Busgeeth et al. 2008; Kombe 2005). While denizens of these neighbourhoods may lack access to sanitation and other urban services or security of tenure, they tend develop bottom-up institutions (e.g. informal housing markets, vote banks) in myriad of ways that are interesting for students of urban politics and systems.\nHowever, to study these phenomena, we need to know where these settlements are and how they change over time. Because official estimates grossly undercount or ignore people in these settlements, it is hard to study them. Very little is known about how the settlements change over time and how quickly they are changing in response to contested claims by different groups (Roy 2009).\nAround the world, these settlements have distinct morphological characteristics. They are often associated with poor housing quality, cramped houses, uneven building heights, lack of sanitation and drinking water infrastructure, uneven quality of construction, lack of vegetative cover, small setbacks and narrow roads. The diversity of these areas pose a challenges for coming up with consistent monitoring and for creating robust methods for detecting and characterizing settlements (Kit, L√ºdeke, and Reckien 2012; Taubenb√∂ck and Kraff 2013; Weeks et al. 2007).\nWith the availability of high quality, high resolution, high frequency satellite imagery, it is becoming increasingly possible to develop methods that will reduce the on-ground monitoring and data collection costs. The goal of this project is to map and validate the irregular settlements in entirety of metropolitan areas in India. We are focusing on Bengaluru, Delhi, Jaipur, Hyderabad and Patna as the initial set of cities.\nCollaborators  [Dr. Raju Vastavai] (https://www.csc.ncsu.edu/people/rrvatsav) [Dr. Anirudh Krishna] (https://sanford.duke.edu/people/faculty/krishna-anirudh) Dr. Erik Wibbels  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"18322385bad4b418a008beddbd476ce7","permalink":"https://nkaza.github.io/project/irregular-settlements/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/irregular-settlements/","section":"project","summary":"Irregular settlement detection using high resolution images.","tags":["Deep Learning","Remote Sensing","Urban Form"],"title":"Irregular settlements","type":"project"},{"authors":null,"categories":null,"content":"Why do we plan? And who plans?\nPlanning is usually conflated with collective action, collective choice, communication, centralisation and coordination. One of the central justifications for planning is to provide for public goods and to correct for externalities. I aim to show that these conflations and justifications are not only wrong. They provide neither a positive nor a normative framework for understanding plans and planning. Plans need account for decision situations that are fluid, uncertain, ambiguous and where decision-making authorities are fractured and have to be negotiated. The point of planning is to lay bare these complexities (to some extent) and allow us to make better decisions.\nThe professional practise of planning is not limited to planners employed in the public sector. But planning is an interaction that happens in a multi-organisational enviornment, where actors enter, leave and morph, and have different agendas and power relations. Such nuanced view of planning requires us to go beyond the conventional justificaitons of planning and to rethink the roles of planners.\nCollaborators  Dr. Lew Hopkins  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"deae7eb6caf48daec23473f160dc0045","permalink":"https://nkaza.github.io/project/planning-theory/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/planning-theory/","section":"project","summary":"Why do public and private organisations plan? When? How are the plans used?","tags":["theory","plans","property-rights","public-good"],"title":"Justifications for Planning","type":"project"},{"authors":null,"categories":null,"content":"Energy, a quintessential private good, poses a conundrum for planners. The weaknesses of the argument that public goods are proper domain of planning become quite apparent, when we consider energy production and transmission infrastructure. Production of useful energy is a highly technical process, yet the political implications of such production and transmission inevitably draws planners into the conversation.\nIn this thread of research, I seek to understand how traditional planning at the local and regional level wittingly and unwittingly intersect with energy production and consumption. In particular, I am interested in the idea that local governments can use their influence on urban form to influence energy production and consumption.\nCollaborators  Dr. Yan Song Dr. Chaosu Li Dr. Roberto Quercia  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"bbbd2aeecd6640e1bbc402ab2af3f076","permalink":"https://nkaza.github.io/project/energy-policy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/energy-policy/","section":"project","summary":"The interactions between land use, transportation, environment and energy systems","tags":["energy-policy","China","American Indian tribes","equity"],"title":"Local Energy Policy \u0026 Planning","type":"project"},{"authors":null,"categories":null,"content":"Urban areas face daunting environmental, socioecological, and infrastructure challenges. Information and communications technology (ICT) promises unprecedented capabilities to enable cities to improve the quality of life, efficiency of urban operations and services, and competitiveness, while maintaining sustainable use of resources. Ubiquitous sensing, information processing, and wireless networks are quickly becoming embedded in the fabric of contemporary cities, and the Internet of Things (IoT) connects everyday objects and devices to network technologies. The proliferation of these technologies enables the advent of ‚ÄúSmart Cities‚Äù that utilize ICT, IoT, and big data analytics to address critical urban challenges.\nWater supply and infrastructure are major areas of concern for American cities and grand challenges facing engineers. Urban areas are running out of clean reliable sources of water, and innovative solutions are needed for long-term planning [7]. The US drinking water infrastructure serves 315 million people and is in need of replacement, upgrading, and maintenance to continue to support projected population growth. The American Society of Civil Engineers rated drinking water infrastructure with a grade of D- in 2009 and D in 2013, while the American Water Works Association estimates the cost of repairing and expanding US drinking water infrastructure at over $1 trillion through 2035 or $1.7 trillion through 2050. Because of the lapse in infrastructure maintenance, 77 million people are served by more than 18,000 water systems with water quality violations (based on 2015 data), and 2.1 trillion gallons of water are lost annually due to aging and leaky pipes, broken water mains, and faulty meters.\nA promising application for smart and connected communities is the use of ICT and IoT technologies to improve urban water supply management. The IoT can connect personal smart devices with faucets and pipelines that are embedded with sensors, actuators, and network connectivity to collect and report real-time information about water consumption, quality, and losses. A water-smart city can sustainably use and reuse water resources by adapting real-time operations and planning practices in response to ubiquitous sensor networks and disparate but interconnected and heterogeneous data streams. While a survey of the water industry shows that 33% of utilities are interested in real-time control and big data system analytics, water utilities have predominantly not harnessed these technologies, due to a number of challenges associated with managing and analyzing big data. Technological gaps, workforce challenges, and community disengagement undermine the alignment of critical municipal management priorities with the analysis and application of smart water data. Installing data analytics systems can worsen data deluge, which is a serious challenge for municipalities, utilities, and their constituencies. The ubiquity of different types of sensors and data collection mechanisms obscures the issues with frequency and asynchronicity of data collection, the types of data generated, and gaps in datasets. Utilities that have installed smart meter systems need support to make sense of and apply data for decision-making, and applications are lacking that would demonstrate that the use of smart systems will support long-term sustainability and urban planning goals. The next generation smart water system should provide the analysis to guide water resources sustainability, stand as a first line of defense for communities that suffer from water quality issues, and catalyze a culture of water conservation within communities.\nCollaborators  Dr. Emily Berglund  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5d882da7d821b73d795206bf7c53b287","permalink":"https://nkaza.github.io/project/smartwater-iot/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/smartwater-iot/","section":"project","summary":"Managing urban water infrastructure with new data driven approaches and institutional change","tags":["machine-learning","new-urban-analytics","water"],"title":"Smart Water Management and Internet of Things","type":"project"},{"authors":["D. A Hartley","N. Kaza","T. W. Lester"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"fc307e4d4f7b1a4bfb694fd80f8442c0","permalink":"https://nkaza.github.io/publication/kaza-edq/","publishdate":"2021-02-18T16:50:04.416198Z","relpermalink":"/publication/kaza-edq/","section":"publication","summary":"In the years since Michael Porter's paper about the potential competitiveness of inner cities there has been growing evidence of a residential resurgence in urban neighborhoods.  Yet, there is less evidence on the competitiveness of inner cities for employment.  We document the trends in net employment growth and find that inner cities gained over 1.8 million jobs between 2002 and 2011 at a rate comparable to suburban areas.  We also find a significant number of inner cities are becoming more competitive over this period---increasing their share of metropolitan employment in 120 out of 281 MSAs.  We also describe the pattern of job growth within the inner city, finding that tracts that grew faster tended to be closer to downtown, with access to transit, and adjacent to areas with higher population growth.  However, tracts with higher poverty rates experienced less job growth, indicating that barriers still exist in the inner city. ","tags":[],"title":"Are America's Inner Cities Competitive? Evidence from the 2000s","type":"publication"},{"authors":["N. Kaza","S. Riley","R. G. Quercia","C. Tian"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"366e188c6c6815d37d601ce9d1c6cb33","permalink":"https://nkaza.github.io/publication/kaza-hpd-2016/","publishdate":"2021-02-18T16:49:58.081287Z","relpermalink":"/publication/kaza-hpd-2016/","section":"publication","summary":"Because these energy expenditures are fairly inelastic, their effects on low-income households may be significant, due to the po tential for energy consumption to displace other types of consumption when energy prices rise. This is particularly true for transportation energy consumption. We test the hypothesis that low and moderate income re sidents are less likely default, when they are located in more accessible places and find that regional accessibility has almost no effect on risk s of default, but local job diversity has moderate mitigating effect. ","tags":[],"title":"Location Efficiency \u0026 Mortgage Risks for Low-Income Households","type":"publication"},{"authors":["Chaosu Li","Yan Song","Nikhil Kaza"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"7899a4cc62f0e41933cb912ca4461e6f","permalink":"https://nkaza.github.io/publication/li-2016-aa/","publishdate":"2024-09-15T09:22:27.063252Z","relpermalink":"/publication/li-2016-aa/","section":"publication","summary":"","tags":[],"title":"Residential Energy Consumption: A Structural Analysis of Chicago","type":"publication"},{"authors":["Nikhil Kaza"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392145,"objectID":"393757730c31aa95f945dfe82422ddb3","permalink":"https://nkaza.github.io/publication/kaza-2009-review/","publishdate":"2024-09-15T09:22:25.83605Z","relpermalink":"/publication/kaza-2009-review/","section":"publication","summary":"","tags":[],"title":"bibstringReview of mkbibemphPlanning Support Systems for cities and regions, bibstringby Richard Brail","type":"publication"},{"authors":["Nikhil Kaza"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"013c270991023496077ab9e91630510e","permalink":"https://nkaza.github.io/publication/kaza-2015-review/","publishdate":"2024-09-15T09:22:25.967891Z","relpermalink":"/publication/kaza-2015-review/","section":"publication","summary":"","tags":[],"title":"bibstringReview of mkbibemphThe Exposed City: Mapping the Urban Invisibles, bibstringby Nadia Amaroso","type":"publication"},{"authors":["M. Zapata","N. Kaza"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"0efd530cc88fe65bbaf815726f5edda8","permalink":"https://nkaza.github.io/publication/zapata-dz/","publishdate":"2021-02-18T16:50:00.679537Z","relpermalink":"/publication/zapata-dz/","section":"publication","summary":"The use of scenario planning in urban and regional planning practice has grown in the last decade as one way to face uncertainty. However, in adapting scenario planning from its origins in the business sector, planners have eliminated two key components: 1) the use of multiple scenarios, and 2) the inclusion of diverse organisations, people and interests through deep deliberations. We argue that this shift limits the ability for planners to plan for multiple plausible and futures that are shaped by an increasing number of diverse actors. In this paper, we use case study research to exam how uncertainty was considered in four scenario planning processes. We analyzed and compared the cases based analytical categories related to multiple futures and diversity. We found that the processes that used multiple, structurally distinct scenarios explored a wider range of topics and issues shaping places. All four relied heavily on professional stakeholders as the scenario developers, limiting public input. Only those processes that included multiple futures captured the differential effects scenarios would have on diverse people and interests. ","tags":[],"title":"Radical Uncertainty: Scenario planning for futures","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"8cc0417e4b9961d18a3ab9edb7d6cc8e","permalink":"https://nkaza.github.io/publication/kaza-tda/","publishdate":"2021-02-18T16:50:04.294816Z","relpermalink":"/publication/kaza-tda/","section":"publication","summary":"Many place based accessibility studies ignore the time component. Relying on theoretical frameworks that treat distance between two fixed points as constant, these methods ignore the diurnal and seasonal changes in accessibility. Furthermore, network distances between two nodes are dependent on the network structure and weight distribution on the edges. These weights can change quite frequently and the network structure itself is subject to modification because of availability and unavailability of links and nodes. All these reasons, point to considering the implications of time variation in accessibility of a place. Using the case of transit, where all these feature are readily apparent simultaneously, I demonstrate the volatility in accessibility for two counties in North Carolina. Significant diurnal changes are observed in quarter of the locations and in the rest the changes are minimal mostly because of low levels of transit accessibility. I argue not for minimizing the volatility, but for acknowledging its impacts on mode choices, location choices and therefore on spatial structure of cities.","tags":[],"title":"Time Dependent Accessibility","type":"publication"},{"authors":["J. McCarty","N. Kaza"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"bd6a6e786cc47676c588c06bfae48187","permalink":"https://nkaza.github.io/publication/kaza-wp-yo/","publishdate":"2021-02-18T16:49:58.328421Z","relpermalink":"/publication/kaza-wp-yo/","section":"publication","summary":"In metropolitan regions, urban form is usually associated with pollutant levels. However, empirical analyses have relied on small samples and demographic variables of urban form. In this paper, we study the relationships between urban form indicators derived from land cover data and emissions for all of the conterminous United States. We use a spatial error model to identify to test if the urban form indicators, and in particular mixing of urban and forest land cover has effect on emissions. We find that fragmentary patterns are associated with higher emissions but these emissions are mitigated when there is a mixing of forest and urban land cover. This points to a nuanced understanding of the implications of sprawling city development on air quality.","tags":[],"title":"Urban Form and Air Quality","type":"publication"},{"authors":["N. Kaza","C. Tian","R. G. Quercia"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"25e5941815d2532368f1e44d7937f407","permalink":"https://nkaza.github.io/publication/kaza-2013-tg/","publishdate":"2021-02-18T16:50:01.055436Z","relpermalink":"/publication/kaza-2013-tg/","section":"publication","summary":"Recently, as part of his Climate Action Plan, the Obama administration proposed that energy efficiency should be factored into mortgage underwriting upon the sale or refinancing of new and existing homes. Unfortunately, there is little empirical evidence on the relationship between energy efficiency and mortgage risks. Utilizing a unique dataset, we examine this relationship. More narrowly, we examine the performance of mortgages backed by Energy Star certified homes. We find that default and prepayment risks are significantly lower in certified homes. Even for Energy Star certified homes, more energy efficiency is associated with even lower loan risks.  These results offer support for taking energy efficiency into consideration in the mortgage underwriting process. Being the first of its kind, further research needs to replicate the present study with other data sets, over different time periods, and with alternative methodologies.","tags":[],"title":"Home Energy Efficiency and Mortgage Risks","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"e21a05998233e93981a75dd8c47cd9b5","permalink":"https://nkaza.github.io/publication/kaza-2013-fk/","publishdate":"2021-02-18T16:50:01.924892Z","relpermalink":"/publication/kaza-2013-fk/","section":"publication","summary":"Deliberative democracy often presumes that the deliberators are members of a political community that often share common understanding about their values, even when they disagree about them. Participatory planning processes building upon these ideas argue that planning itself has to be communicative among a variety of interest groups and should, usually, result in a common consensus. However, the boundaries of these groups rarely get attention. These boundaries shape not only the discursive practices within groups but also among them, and therefore need to be examined more thoroughly. Furthermore, the relationship of membership to substantive issues of planning is important yet underexplored. Political membership in a diverse, mobile, transient and multi-cultural world is a contested subject and should be given deserving attention for its implications for planning practice.","tags":["\"Cosmopolitanism\"","\"Communitarianism\"","\"Justifications for Planning\"","\"Commons\"","\"property rights\""],"title":"Persons, Polities and Planning","type":"publication"},{"authors":["N. Kaza","M. Patane"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"1f902d78548aedf8acf160685dafd8b7","permalink":"https://nkaza.github.io/publication/kaza-2013-fv/","publishdate":"2021-02-18T16:50:00.925374Z","relpermalink":"/publication/kaza-2013-fv/","section":"publication","summary":"To date, most planners have focussed on energy consumption. They argue that compact housing and urban patterns reduce both household and transportation energy use and should be promoted to combat a variety of ills, including energy import dependency and climate change. However, planners also have a strong role to play in energy production, particularly with the increasing adoption of renewable forms of energy. Planners will play an integral part in harmonising local land use regulations and policies that will either promote or hinder these technologies. In this paper, we survey the literature and point to environmental and other land use issues that are pertinent to different types of centralised, distributed, conventional and renewable energy production.","tags":[],"title":"The Land Use Energy Connection","type":"publication"},{"authors":["Nikhil Kaza","Gerrit Knaap","Lewis D. Hopkins"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392147,"objectID":"1724e15d21bc721fefd700f69ca219fd","permalink":"https://nkaza.github.io/publication/kaza-2014-aa/","publishdate":"2024-09-15T09:22:26.919811Z","relpermalink":"/publication/kaza-2014-aa/","section":"publication","summary":"","tags":[],"title":"Vain Foresight: Against Implementation","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"6f9c746bacfeebc72766349c2f1856b2","permalink":"https://nkaza.github.io/publication/kaza-tepeval/","publishdate":"2021-02-18T16:49:58.565768Z","relpermalink":"/publication/kaza-tepeval/","section":"publication","summary":"","tags":[],"title":"Whose Plan Is It Anyway? Energy Planning By American Indian Tribes In The United States","type":"publication"},{"authors":["N. Kaza","C. Tian","R. Quercia"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"d4391f2a0f3bd3ea8387bd09d3fb5cd6","permalink":"https://nkaza.github.io/publication/kaza-fe/","publishdate":"2021-02-18T16:50:00.210383Z","relpermalink":"/publication/kaza-fe/","section":"publication","summary":"","tags":[],"title":"Home Energy Efficiency and Mortgage Risks","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"7a725dbb998055fb84cf8a4e00cd9ee6","permalink":"https://nkaza.github.io/publication/kaza-ec/","publishdate":"2021-02-18T16:50:00.093874Z","relpermalink":"/publication/kaza-ec/","section":"publication","summary":"","tags":[],"title":"Industrial Land Transformation in Cities","type":"publication"},{"authors":["N. Kaza","T. K. BenDor"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"7ec5a2dc5db9bf8f223f3a9ba12d0296","permalink":"https://nkaza.github.io/publication/kaza-2013-kx/","publishdate":"2021-02-18T16:50:01.795317Z","relpermalink":"/publication/kaza-2013-kx/","section":"publication","summary":"U.S. regulations require offsets for aquatic ecosystems damaged during land development, often through restoration of alternative resources. What effect does large scale wetland and stream restoration have on surrounding land values? Restoration effects on real estate values have substantial implications for protecting resources, increasing tax base, and improving environmental policies. Our analysis focuses on the three county Raleigh---Durham--Chapel Hill, North Carolina region, which has experienced rapid development and extensive aquatic ecological restoration (through the state's Ecosystem Enhancement Program [EEP]). Since restoration sites are not randomly distributed across space, we used a genetic algorithm to match parcels near restoration sites with comparable control parcels. Similar to propensity score analysis, this technique facilitates statistical comparison and isolates the effects of restoration sites on surrounding real estate values. Compared to parcels not proximate to any aquatic resources, we found that, 1) natural aquatic systems steadily and significantly increase parcel values up to 0.75 miles away, and 2) parcels 0.5 mi from EEP sites gain substantial amenity value. When we control for intervening water bodies (e.g. unrestored streams and wetlands), we find a similar inflection point whereby parcels ","tags":["\"Matching\"","\"Land values\"","\"Restoration\""],"title":"Land Value Impacts of Aquatic Ecosystem Restoration","type":"publication"},{"authors":["T.W. Lester","N. Kaza","S. Kirk"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"33b023e3c9ab99423aeae2e0448b4234","permalink":"https://nkaza.github.io/publication/lester-wp-jo/","publishdate":"2021-02-18T16:49:58.206118Z","relpermalink":"/publication/lester-wp-jo/","section":"publication","summary":"Problem: Given the extent of deindustrialization since the late 1970s, many cities are forced to deal with a legacy of vacant and environmentally damaged industrial land.  Urban economic development strategies have also turned away from manufacturing, towards attracting high-skill service sector jobs. Thus, it is of no surprise that planners, to date, have been focused on converting vacant industrial parcels within the city into more `residentiary' uses and deliberately directing public infrastructure investment elsewhere.  However, the recent resurgence in manufacturing in the aftermath of the Great Recession and the transformation of many goods-producing sectors offers an opportunity that may bypass urban areas that do not preserve industrial land or view it as complementary to other uses. Research Strategy and Findings: To this end, we develop an index of vulnerability of industrial land that is based on location factors, neighborhood dynamics, detailed industrial trends, environmental hazards, and local regulations.  We show for the cases of Cook County, IL and Mecklenburg County, NC how these factors can explain the conversion probability and how various industrial preservation strategies counteract these vulnerabilities. We use a logit model formulation with detailed parcel data and establishment time series data to derive the index of vulnerability. Takeaway for Practice: This index can be used to strategically plan for which industries to target and which sites to preserve as industrial uses. The statistical models suggest that traditional planning and regulatory tools such as industrial zone designations do reduce conversion risk and factors such as transit accessibility increase the vulnerability of conversion. We argue that local governments should be strategic about which manufacturing industries can be preserved at what locations using these tools. We also develop and demonstrate an open source interactive web-based tool that demonstrates some of the key concepts that can be replicated in other places.","tags":[],"title":"Making Room for Manufacturing: Understanding Industrial Land Conversion in Cities","type":"publication"},{"authors":["D. Brookshire","N. Kaza"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"33f8792e369890fd2804016d838cb4c0","permalink":"https://nkaza.github.io/publication/brookshire-2013-bs/","publishdate":"2021-02-18T16:50:00.796003Z","relpermalink":"/publication/brookshire-2013-bs/","section":"publication","summary":"The prevalence of energy resources on American Indian lands, the links between energy management and tribal sovereignty, and recent federal government incentives make tribal energy planning an interesting case study for community energy planning in the US. This paper studies the strategic energy planning efforts, energy resource development, and energy efficiency policies established by tribes within the continental US. The paper analyzes the results of a survey of various tribes and energy resource development and planning efforts and supplements the responses with publicly available information on resources, economics, and demographics. We find that incentives and advisory services from the federal government are key to developing the capacity of the tribes to pursue energy planning and energy resource development. These incentives largely avoid the misdeeds of past federal policy by promoting tribal control over energy planning and energy resource development efforts. Tribes with formal energy plans or visions are more likely to develop energy resources than tribes without them and are engaged in a more comprehensive and sustainable approach to energy resource development and energy efficiency.","tags":[],"title":"Planning for Seven Generations: Energy Planning of American Indian Tribes","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"6b772593704d74439491c4967699d512","permalink":"https://nkaza.github.io/publication/kaza-changing-2013/","publishdate":"2021-02-18T16:50:04.022869Z","relpermalink":"/publication/kaza-changing-2013/","section":"publication","summary":"Sprawling urbanisation has been the hallmark of development in United States. This study intends to provide a comprehensive overview of the changing urban landscape pat- terns between 2001 and 2006. Using the land cover data from US Geological Survey for these two years, I characterise the landscape metrics for each county in the continental United States. The changes in these metrics are correlated with the drivers of urbani- sation including socioeconomic variables. Uneven and heterogeneous patterns of growth in a country this large are not surprising. However, the metrics reveal that while urban counties are becoming less fragmented, some rural counties in South and Western United States are experiencing significant leapfrog development.","tags":["\"Landscape metrics\"","\"urban patterns\"","\"drivers of urbanisation\""],"title":"The changing urban landscape of continental United States","type":"publication"},{"authors":["N. Kaza","T.W. Lester","D. Rodriguez"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"b9aee69757bbbbd5c0b3806dc5eb4a1a","permalink":"https://nkaza.github.io/publication/kaza-2013-uq/","publishdate":"2021-02-18T16:50:04.162988Z","relpermalink":"/publication/kaza-2013-uq/","section":"publication","summary":"This paper explores the spatial and temporal patterns of green building in the commercial and institutional sector in the U.S. While these buildings are becoming more common place, they have yet to reach a critical mass to affect the entire construction industry. Given the potential for green building practices to reduce energy consumption and carbon emissions, we seek to understand the geography of green building. Using multiple metrics, we explain the patterning of geography of LEED and Energy Star certified buildings in the United States. We find strong evidence of clustering at the metropolitan and sub-metropolitan scales. This exploratory research serves as a foundation for future research aimed at specifying the nature of agglomerative processes in green buildings.","tags":["\"Green buildings\"","\"Clustering\"","\"Economic Development\""],"title":"The spatio-temporal clustering of green buildings in the US","type":"publication"},{"authors":["T. K. BenDor","N. Kaza"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"2cf4306805616a01891f8891b91eb24b","permalink":"https://nkaza.github.io/publication/ben-dor-2028-vn/","publishdate":"2021-02-18T16:50:01.672762Z","relpermalink":"/publication/ben-dor-2028-vn/","section":"publication","summary":"Historic reference behavior and archetypes of system structure are key tools for creating rigorous system dynamics (SD) models. Modelers often delineate causal relationships by employing common archetypes of dynamic system structure, which produce behaviors such as growth and decline, oscillation, and complex combinations thereof. We extend archetypes to spatial-dynamic models, focusing on structural archetypes that exhibit changing spatial patterns in two-dimensional landscapes. Although many fields employ spatial modeling techniques, analogy-based, causally focused system archetypes remain confined to non-spatial SD models. We draw on spatial analysis literature to explore the influence of space on dynamic relationships and archetypes, including methods for articulating √íspace√ì and expressing feedback. We offer simple examples of spatial system archetypes and explore network structures for spatially extending SD models. By doing this, we argue for spatial modeling techniques that parallel the learn-by-analogy environment that archetypes have promoted in aspatial SD research.","tags":["\"archetypes\"","\"network space\"","\"intensive process\"","\"extensive process\""],"title":"A Theory of Spatial Systems Archetypes","type":"publication"},{"authors":["Nikhil Kaza"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726392146,"objectID":"949d77f189896a8c9242feff36300174","permalink":"https://nkaza.github.io/publication/kaza-2012-review/","publishdate":"2024-09-15T09:22:26.101457Z","relpermalink":"/publication/kaza-2012-review/","section":"publication","summary":"","tags":[],"title":"bibstringReview of mkbibemphLand Policy: Planning and the Spatial Consequences of Property, bibstringby Benjamin Davy","type":"publication"},{"authors":["N. Kaza","L. D. Hopkins"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"823cb3afe9c64d4d932cfd217e379c8c","permalink":"https://nkaza.github.io/publication/kaza-2012-zr/","publishdate":"2021-02-18T16:50:01.548543Z","relpermalink":"/publication/kaza-2012-zr/","section":"publication","summary":"If urban development plans were just target patterns to be achieved, conventional data structures in GIS would be sufficient. Urban development plans have a strong spatial component, but recent literature in planning emphasizes that plans are about actions and relationships among them. These relationships include interdependence, substitutability, priority, and parthood. In order to support planning, GIScience should devise data structures and queries to support reasoning with these relationships. This paper shows how relationships encoded within each of a set of plans, using a recently developed data model, can be used to infer the relationships of actions among these plans. Simple databases and use cases based on real situations in McHenry County, Illinois are used to demonstrate that these relationships can be encoded and queried. The results demonstrate that previously dis- covered semantic relationships can be used to discover additional relationships across plans, thereby enriching the decision making. The approach provides a systematic way of structuring the information in plans to support making and using plans.","tags":["\"Plans\"","\"Decision Support Systems\"","\"Substitutability\"","\"Interdependence\""],"title":"Intentions, Urban Plans and Information Systems","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"adb43ffd0c198ddd6d4e7412cda29f59","permalink":"https://nkaza.github.io/publication/kaza-rw/","publishdate":"2021-02-18T16:50:00.444267Z","relpermalink":"/publication/kaza-rw/","section":"publication","summary":"","tags":[],"title":"Persons, Polities and Planning","type":"publication"},{"authors":["D. Brookshire","N. Kaza"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"8b7d4e477678703d0383d4b170633668","permalink":"https://nkaza.github.io/publication/brookshire-dk/","publishdate":"2021-02-18T16:50:00.556383Z","relpermalink":"/publication/brookshire-dk/","section":"publication","summary":"","tags":[],"title":"Sustaining the Seventh Generation: Strategic Energy Planning of American Indian Tribes","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"ef6ba90209396bb5b501e0d26b3f9cd2","permalink":"https://nkaza.github.io/publication/kaza-mi/","publishdate":"2021-02-18T16:50:00.32594Z","relpermalink":"/publication/kaza-mi/","section":"publication","summary":"","tags":[],"title":"The Changing Urban Landscape of United States","type":"publication"},{"authors":["N. Kaza","C. Towe","X. Ye"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"4871143f93300a346fceb883cf879abb","permalink":"https://nkaza.github.io/publication/kaza-2011-qf/","publishdate":"2021-02-18T16:50:01.429302Z","relpermalink":"/publication/kaza-2011-qf/","section":"publication","summary":"The need for models that forecast land use change spans many disciplines. Pattern-based models were the first in which projections of change at specific locations in actual landscapes could be predicted. In contrast, recent economic models have modeled the underlying behavioral process that produces land use change. This paper combines attributes from each approach into a hybrid model using Multiple Discrete Continuous Extreme Value formulation that allows for multiple conversion types while estimating the intensity of each type of conversion. We demonstrate the simulation routine for three county region in Maryland, which successfully predicts a majority of growth by type, time, and location at a disaggregate scale.","tags":["\"MDCEV\"","\"Land Use Change\"","\"Urban Growth\""],"title":"A Hybrid Land Conversion Model Incorporating Multiple End Uses","type":"publication"},{"authors":["T. BenDor","N. Kaza"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"092cc175e7e5dd0c167349faf19ce8b3","permalink":"https://nkaza.github.io/publication/ben-dor-dw/","publishdate":"2021-02-18T16:49:58.689527Z","relpermalink":"/publication/ben-dor-dw/","section":"publication","summary":"","tags":[],"title":"Land Value Impacts of Aquatic Ecosystem Restoration: An Analysis of the Research Triangle Region of North Carolina","type":"publication"},{"authors":["N. Kaza","I. Knaap","G. J. Knaap","R. Lewis"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"90123ac7119fe2293db6f1dbd36fa833","permalink":"https://nkaza.github.io/publication/kaza-2011-dq/","publishdate":"2021-02-18T16:50:01.313009Z","relpermalink":"/publication/kaza-2011-dq/","section":"publication","summary":"We assessed the relationships between peak oil and urban form, travel behavior, and public health. Peak oil will affect the general economy, travel behavior, and urban form through income and substitution effects; however, because of the wide range of substitution possibilities, the impacts are likely to be gradual and relatively small. Furthermore, we suggest that changes in travel behavior and increases in urban density will have both favorable and unfavorable effects on public health. To mitigate the adverse impacts and to maximize the positive effects of peak oil, we recommend that careful attention should be paid to urban design and public health responses for a range of urbanization patterns.","tags":["\"urban form\"","\"public health\"","\"substitution effects\"","\"local energy policy\""],"title":"Peak Oil, Urban Form and Public Health: Exploring the Connections","type":"publication"},{"authors":["N. Kaza","G. J. Knaap"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"ca4d60365d6f251a03a612e95ecc3c46","permalink":"https://nkaza.github.io/publication/kaza-knaap-2011/","publishdate":"2021-02-18T16:50:03.761811Z","relpermalink":"/publication/kaza-knaap-2011/","section":"publication","summary":"The literature in economics and planning contains advice that seems to hinge upon the distinctions between planning and market approaches to public policy. In these distinctions, the planning approach is often characterized as stiffly regulatory while the market approach is characterized as more flexible and thus usually preferable. This distinction, we argue, is false both in its positive formulation and in its normative implication that the market approach is always superior. Instead, we assert planning is not about market failures and regulatory remedies but can be better understood by several other economic principles. These include the Coasian notion of transaction costs, the problem of optimization over time, and the problem of non-cooperative games in which the public sector is but one of many players. We argue plans and planning make sense in both situations where single or multiple actors think before acting, using limited information. When viewed in these frameworks, we argue that research by planners and economists are much stronger complements than they are substitutes.","tags":[],"title":"Principles of Planning for Economists","type":"publication"},{"authors":["A. Chakraborty","N. Kaza","G. J. Knaap","B. Deal"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667004,"objectID":"69663caab9af1b32bc2542c5abc0d9fe","permalink":"https://nkaza.github.io/publication/chakraborty-2011-lr/","publishdate":"2021-02-18T16:50:03.894005Z","relpermalink":"/publication/chakraborty-2011-lr/","section":"publication","summary":"Problem: The practice of scenario plan- ning is often too focused on developing a single preferred scenario and fails to adequately consider multiple uncertain futures. The U.S. Department of Housing and Urban Development recently awarded grants for scenario planning at regional and metropolitan scales that further promote this practice. However, a lack of systematic analysis of uncertainty limits the role of scenario planning.  Purpose: The purpose of this article is to demonstrate how to incorporate uncertainty into large-scale scenario analysis and then use that framework to identify contingent and robust plans.  Methods: We adapt the concepts of controllable internal options and uncontrol- lable external forces and consider their interactions in order to develop future scenarios and identify contingent and robust decisions. We then apply this technique using advanced econometric, land use, and transportation models developed for the Baltimore--Washington metropolitan region and its vicinity. Finally, based on the results of a hypothetical, yet plausible, exercise, we show how contingent and robust decisions can help local and regional governments develop contingent and robust plans.  Results and conclusions: Scenarios developed as a combination of internal options and external forces allow us to identify a wider range of future impacts than in traditional metropolitan scenario planning. Robust plans support choices that offer benefits across scenarios. Contingent plans can be tailored to specific futures.  Takeaway for practice: By providing a way to think systematically about uncer- tainty, scenario analysis promises to improve the efficacy of large-scale planning. ","tags":["\"robust and contingent plans\"","\"internal options\"","\"external forces\"","\"land use and transportation models\"","\"uncertainty\"","\"strategic decision making\""],"title":"Robust Plans and Contingent Plans: Scenario Planning for an Uncertain World","type":"publication"},{"authors":["M. Zapata","N. Kaza"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"d1a0ee3138846b7fbc9e3342c1a3675b","permalink":"https://nkaza.github.io/publication/zapata-ay/","publishdate":"2021-02-18T16:49:58.450459Z","relpermalink":"/publication/zapata-ay/","section":"publication","summary":"","tags":[],"title":"Scenario Planning for Uncertain Futures: Incorporating Multiple Futures and Multiple Publics","type":"publication"},{"authors":["G. Knaap","N. Kaza","R. Lewis"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"59375413e146c206cdd48dca8f62875e","permalink":"https://nkaza.github.io/publication/knaap-pt/","publishdate":"2021-02-18T16:49:58.97177Z","relpermalink":"/publication/knaap-pt/","section":"publication","summary":"","tags":[],"title":"Density and Public Health: It is not that simple","type":"publication"},{"authors":["A. Blohm","J. Becker","N. Kaza","G. Knaap","G. Moglen","M. Ruth"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"eb3e2a2ef5592f9975420994e9d03100","permalink":"https://nkaza.github.io/publication/blohm-xq/","publishdate":"2021-02-18T16:49:59.098477Z","relpermalink":"/publication/blohm-xq/","section":"publication","summary":"","tags":[],"title":"Envisioning a sustainable Maryland: Comparing alternative development scenarios considering energy con- sumption and water quality","type":"publication"},{"authors":["N. Kaza","C. Towe","X Ye"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666998,"objectID":"3b0b83f18da5fe211dcd7de5ab8c8844","permalink":"https://nkaza.github.io/publication/kaza-hq/","publishdate":"2021-02-18T16:49:58.846123Z","relpermalink":"/publication/kaza-hq/","section":"publication","summary":"","tags":[],"title":"Scenarios Using Land Conversion Model for Multiple Land uses","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"3fb6c7a43a0afadcc415dd33de836693","permalink":"https://nkaza.github.io/publication/kaza-2010-lr/","publishdate":"2021-02-18T16:50:03.630059Z","relpermalink":"/publication/kaza-2010-lr/","section":"publication","summary":"Residential energy consumption accounts for 22% of the total energy consumption in the US. However, the impacts of local planning policies, such as increasing density and changing the housing type mix, on residential energy consumption are not well understood. Using Residential Energy Consumption Survey Data from the Energy Information Administration, quantile regression analysis was used to tease out the effects of various factors on entire distribution on the energy consumption spectrum instead of focusing on the conditional average. Results show that while housing size matters for space conditioning, housing type has a more nuanced impact. Self-reported neighborhood density does not seem to have any impact on energy use. Furthermore, the effects of these factors at the tails of the energy use distribution are substantially different than the average, in some cases differing by a factor of six. Some, not all, types of multifamily housing offer almost as much savings as reduction in housing area by 100 m2, compared to single family houses.","tags":["\"Quantile\"","\"regression\""],"title":"Understanding the spectrum of residential energy consumption: A quantile regression approach","type":"publication"},{"authors":["N. Kaza","C. Towe","X. Ye","B Thapa"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"4fca8d7ca18bd922959519650a8ab6e4","permalink":"https://nkaza.github.io/publication/kaza-xw/","publishdate":"2021-02-18T16:49:59.345868Z","relpermalink":"/publication/kaza-xw/","section":"publication","summary":"","tags":[],"title":"An Economic model of Land Conversion Incorporating Multiple Land uses","type":"publication"},{"authors":["N. Kaza","G.J. Knaap","K. Clfiton"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"5cc03f6d563c1c937b1aa4d9eb8a5316","permalink":"https://nkaza.github.io/publication/kaza-kn/","publishdate":"2021-02-18T16:49:59.223763Z","relpermalink":"/publication/kaza-kn/","section":"publication","summary":"","tags":[],"title":"Economic Scenarios and Development Patterns in the Baltimore-Washington Region","type":"publication"},{"authors":["N. Kaza","L. D. Hopkins"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"3208de08037f2fa11ddd348c7dc7b896","permalink":"https://nkaza.github.io/publication/kaza-2009-fk/","publishdate":"2021-02-18T16:50:03.502967Z","relpermalink":"/publication/kaza-2009-fk/","section":"publication","summary":"The literature on planning assumes that plans, to be useful, should be public and then implemented. The participatory paradigm, with its recent resurgence, assumes that planning should be performed with stakeholders in public forums. This paper challenges the notion that plans and planning processes should be public in general or even within a group whose mandate it is to plan. It considers the inherent strategic reality of planning and interactions of multiple plans, existing and being made and being discarded, to argue for cases in which plans are and ought to be private and planning necessarily strategic and idiosyncratic. This paper addresses questions such as: In what circumstances will a plan maker choose to make plans public, to whom, and when? What should we expect to be public from the plans of others and what not? This paper will posit that plans are subsumed in plans about plans and that plans are strategically made explicit in public. The absence of public documents in particular situations should not be taken to imply that plans do not exist or that plan led behaviour is not occurring, inferable, or observable. Based in part on examples from New Orleans recovery planning, we then provide explanations of why and in what circumstances individuals, voluntary groups, and governments are likely to plan in public and make plans public. And we consider related but distinct justifications for what aspects of plans should be made public and in what circumstances.","tags":["\"plans\"","\"planning\"","\"strategic\"","\"information\""],"title":"In What Circumstances Should Plans Be Public?","type":"publication"},{"authors":["N. Kaza","D. Finn","L. D. Hopkins"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667001,"objectID":"b673fd9c005264a2b30ba2af73f608cc","permalink":"https://nkaza.github.io/publication/kaza-2009-nx/","publishdate":"2021-02-18T16:50:01.180357Z","relpermalink":"/publication/kaza-2009-nx/","section":"publication","summary":"Plans provide information about how multiple decisions are structured over time, and what the intentions of a particular actor are. As and when these decisions get made, or not made, some parts of plans become irrelevant, while some other new relationships are discovered and considered. Recognising this provides a useful way to interpret the changes in plans by tracking the decisions and vice versa. In this paper, we illustrate the complexities of an ontology of urban systems which are needed to ensure the currency of plans, so that they could be effectively used in urban decision making. Especially, when actors are numerous, jurisdictions overlap, actions are interdependent and interests are unstable, this framework enables us to think about plans in a complex and changing urban environment and make them so that they remain useful.","tags":["\"plans\"","\"information systems\""],"title":"Updating Plans: A Historiography of Decisions Over Time","type":"publication"},{"authors":["G. Knaap","A. Chakraborty","N. Kaza"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"e243618d5e45b2d64bbbdbeba0d58311","permalink":"https://nkaza.github.io/publication/knaap-ee/","publishdate":"2021-02-18T16:49:59.476851Z","relpermalink":"/publication/knaap-ee/","section":"publication","summary":"","tags":[],"title":"Planning a new era in the smart growth state: A primer on state development plans","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"78d714083a788edbbed729d8fdf70944","permalink":"https://nkaza.github.io/publication/kaza-reasoning-2008/","publishdate":"2021-02-18T16:50:03.377063Z","relpermalink":"/publication/kaza-reasoning-2008/","section":"publication","summary":"","tags":[],"title":"Reasoning with plans: Inference of semantic relationships among plans about urban development","type":"publication"},{"authors":["N. Kaza","L. D. Hopkins"],"categories":[],"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"a1bb1b3cf574fb7a2f1b21f182df5be8","permalink":"https://nkaza.github.io/publication/kaza-ontology-2007/","publishdate":"2021-02-18T16:50:03.135615Z","relpermalink":"/publication/kaza-ontology-2007/","section":"publication","summary":"The focus of geographic and other ontologies of urban devel- opment has been to represent locations with object attributes or objects with locational attributes. Urban information systems should also repre- sent decisions, which have or could have locational attributes. Develop- ment processes are critically influenced by expectations about declared intentions manifest through plans and records of decisions. This paper provides an ontology of decision situations characterized by actors par- ticipating, intentions expressed, and alternatives considered. We argue that these elements are closely tied to and interdependent with other aspects of urban ontologies, which typically focus on physical objects of development. An ontology of plans and decisions will enable sharing of information among actors and consideration of disparate and distributed information.","tags":["\"Plans\"","\"Decisions\"","\"Urban Ontology\""],"title":"Ontology for Land Development Decisions and Plans","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"caa567983c464044121ecfd1a5607137","permalink":"https://nkaza.github.io/publication/kaza-os/","publishdate":"2021-02-18T16:49:59.600266Z","relpermalink":"/publication/kaza-os/","section":"publication","summary":"","tags":[],"title":"Plans in Decision Support Systems and Decisions in Planning Support System","type":"publication"},{"authors":["N. Kaza","D. Finn","L. D. Hopkins"],"categories":[],"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"33b4f25a58c76ea26a73fc1891b197a6","permalink":"https://nkaza.github.io/publication/kaza-updating-2007/","publishdate":"2021-02-18T16:50:03.258244Z","relpermalink":"/publication/kaza-updating-2007/","section":"publication","summary":"","tags":[],"title":"Updating Plans: A historiography of decision over time","type":"publication"},{"authors":["J. Calder√≥n","A. de Froment","M. Donaldson","G. McInery","N. Kaza"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"9e021bc80162cd487b8539124efa8009","permalink":"https://nkaza.github.io/publication/calderon-gamesevolving-2006/","publishdate":"2021-02-18T16:50:02.779582Z","relpermalink":"/publication/calderon-gamesevolving-2006/","section":"publication","summary":"","tags":[],"title":"Games on Evolving Networks","type":"publication"},{"authors":["N. Kaza","L. D Hopkins"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667000,"objectID":"c89ecf367eef6e67ec91498815125a7c","permalink":"https://nkaza.github.io/publication/kaza-kb/","publishdate":"2021-02-18T16:49:59.962545Z","relpermalink":"/publication/kaza-kb/","section":"publication","summary":"","tags":[],"title":"In What Circumstances Should We Plan in Public?,","type":"publication"},{"authors":["N. Kaza","L. D. Hopkins"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"dab09dfb36ca0919cbd532895d7fa38b","permalink":"https://nkaza.github.io/publication/kaza-qf/","publishdate":"2021-02-18T16:49:59.838711Z","relpermalink":"/publication/kaza-qf/","section":"publication","summary":"","tags":[],"title":"Ontology for Land Development Decisions and Plans","type":"publication"},{"authors":["N. Kaza","L. D. Hopkins"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666999,"objectID":"80c079251f48203a524a4877f490d923","permalink":"https://nkaza.github.io/publication/kaza-qq/","publishdate":"2021-02-18T16:49:59.717186Z","relpermalink":"/publication/kaza-qq/","section":"publication","summary":"","tags":[],"title":"Reasoning with Plans: Semantic Relationships among Interde- pendent and Contingent Landuse Plan","type":"publication"},{"authors":["K. P. Donaghy","N. Kaza"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"d869b0e3eed5bfd91bead12e3660981b","permalink":"https://nkaza.github.io/publication/donaghy-value-2006/","publishdate":"2021-02-18T16:50:02.901812Z","relpermalink":"/publication/donaghy-value-2006/","section":"publication","summary":"","tags":[],"title":"The Value of Waiting: A primer on Option Value for Planners","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667003,"objectID":"c869761e610ab4de583dfbe906894436","permalink":"https://nkaza.github.io/publication/kaza-2006-oq/","publishdate":"2021-02-18T16:50:03.016001Z","relpermalink":"/publication/kaza-2006-oq/","section":"publication","summary":"Wide participation, in the urban planning context, is justified as the means of balancing multiple interests outside the traditional decision-making setup. However, this article argues that the participatory paradigm provides at best inadequate justification to the planning process. Particularly if consensus building is the aim of the participatory process, it suffers from a number of impossibility results well documented in the political economics literature. `Lazy deliberators' will arrive at the acceptance of a priori median prefer- ence, and participatory processes necessarily exclude some groups, even under equitable capability and power distribution. This article intends to contribute to the debate on the nature of participatory planning by critically analyzing the motivations of participation and limitation of the participatory planning paradigms, and advocates a temperate view on their efficacy.","tags":["\"consensus\"","\"consent\"","\"participation\"","\"persuasion\"","\"political economy\""],"title":"Tyranny of the Median: A reflection on the participatory urban processes","type":"publication"},{"authors":["L. D. Hopkins","N. Kaza","V. G. Pallathucheril"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"1942b6dcb06fac1ab88f0f35f4610346","permalink":"https://nkaza.github.io/publication/hopkins-data-2005/","publishdate":"2021-02-18T16:50:02.405195Z","relpermalink":"/publication/hopkins-data-2005/","section":"publication","summary":"","tags":[],"title":"A Data Model to Incorporate Plans and Regulations in Urban Simulation Models","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"b85d6ed16a4a9e8e7e2f143023f00a09","permalink":"https://nkaza.github.io/publication/kaza-deliberations-2005/","publishdate":"2021-02-18T16:50:02.526447Z","relpermalink":"/publication/kaza-deliberations-2005/","section":"publication","summary":"","tags":[],"title":"Deliberations between experts and laity: An approach to building planning information systems","type":"publication"},{"authors":["L. D. Hopkins","N. Kaza","V. G. Pallathucheril"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"48a8f17da8834251145bdb5157df6d4c","permalink":"https://nkaza.github.io/publication/hopkins-representing-2005/","publishdate":"2021-02-18T16:50:02.659952Z","relpermalink":"/publication/hopkins-representing-2005/","section":"publication","summary":"Using plans and regulations when making decisions about urban development requires access to the many plans and regulations of many different organizations, both private and public. Current information technologies, the Internet and the World Wide Web in particular, make access to data from such disparate sources feasible. Using these technologies to implement a `system of plans' approach to urban development planning will, however, require a widely shared data model for structuring the content and meanings of plans and regulations and a widely shared language, based on this data model. This paper presents an initial version of a planning data model (PDM), several use cases that set the scope of such a data model, and illustrations of how the current version of the PDM supports these use cases. Further work can build on this data model, including an XML (eXtensible Markup Language) implementation for sharing the information. These tools will enable urban development decision makers to conceive of planning as involving many actors whose many plans can provide useful and usable information.","tags":[],"title":"Representing Urban Development Plans and Regulations as Data: A Planning Data Model","type":"publication"},{"authors":["N. Kaza"],"categories":[],"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"9664ee316c9756c3af99f3986152de12","permalink":"https://nkaza.github.io/publication/kaza-towardsdata-2004/","publishdate":"2021-02-18T16:50:02.282987Z","relpermalink":"/publication/kaza-towardsdata-2004/","section":"publication","summary":"","tags":[],"title":"Towards a Data Model for Urban Planning: Ontological Constructs for Representing Regulations and Guidelines with Geography","type":"publication"},{"authors":["L. D Hopkins","N. Kaza","V. G. Pallathucheril"],"categories":[],"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"837ab720ae8f31ceaf1cfa83bc37f5c4","permalink":"https://nkaza.github.io/publication/hopkins-planning-2003-1/","publishdate":"2021-02-18T16:50:02.04524Z","relpermalink":"/publication/hopkins-planning-2003-1/","section":"publication","summary":"","tags":[],"title":"Planning Markup Language: Representing the Meanings of Plans and Regulations","type":"publication"},{"authors":["L. D. Hopkins","N. Kaza","V. G. Pallathucheril"],"categories":[],"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613667002,"objectID":"c4cefe7876c88088392d23b3ab379d53","permalink":"https://nkaza.github.io/publication/hopkins-shared-2003-1/","publishdate":"2021-02-18T16:50:02.160809Z","relpermalink":"/publication/hopkins-shared-2003-1/","section":"publication","summary":"","tags":[],"title":"Shared Data Model for Describing Plans and Regulations as Inputs to Simulation Models of Urban Development","type":"publication"},{"authors":["J. Branham","D. Salvesen","N. Kaza","T. K. BenDor"],"categories":[],"content":"","date":2024,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2024,"objectID":"8e350ca3a979dd366258811bc4e8b125","permalink":"https://nkaza.github.io/publication/branham-2022-te/","publishdate":"1969-12-31T19:33:44-05:00","relpermalink":"/publication/branham-2022-te/","section":"publication","summary":"\"Federal, state, and local government funding helps stimulate urban development, with growth machine politics playing an important role in determining where subsidies are allocated. The U.S. Coastal Barrier Resources Act (CBRA) was enacted to curb the role of federal subsidies in fostering development along hazardous coastal barriers, providing an opportunity to explore how local growth politics are influenced by the removal of one source of government funding. In this study, we used a series of interview-based case studies to investigate why certain areas in the CBRA developed while most did not. In most cases, the CBRA obstructed local growth coalitions, isolating landowners from the resources necessary to improve the growth potential of their land interests. However, in cases where development occurred within the CBRA, we often found evidence that powerful growth machines were able to acquire replacement subsidies from state and local governments, suggesting these actions are a key driver in overcoming the financial barriers posed by the CBRA.\nTakeaway for practice: This study revealed how growth machines could be hampered by removing access to the financial resources of one level of government, despite the potential to be undermined by intervention at other levels. In an era of increasing coastal risks, subsidy removal can be an effective tool for managing coastal growth, even when authority over land use decisions is limited.\"","tags":null,"title":"A Wrench in the Machine: How subsidy removal alters the politics of coastal development","type":"publication"},{"authors":["T. W. Lester","N. Kaza","T. McAdam"],"categories":[],"content":"","date":2018,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613666997,"objectID":"0d0b0e00664f62b4899775de6bb19dc4","permalink":"https://nkaza.github.io/publication/lester-kaza-mc-adam-2018/","publishdate":"2021-02-18T16:49:57.327373Z","relpermalink":"/publication/lester-kaza-mc-adam-2018/","section":"publication","summary":"","tags":[],"title":"Splintered Metropolitan Opportunity in the United States? Re-Examining the 'Mismatch' between Emerging Employment Centers and Distressed Neighborhoods","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://nkaza.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"125a326aa24b9a9b362c545f872ef9f1","permalink":"https://nkaza.github.io/slides/advancedspatial_with_sf/advanced_spatial_with_sf_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/advancedspatial_with_sf/advanced_spatial_with_sf_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb1551c9c976181e367c0241a8722eb9","permalink":"https://nkaza.github.io/slides/advancedspatial_with_sf/advanced_spatial_with_sf_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/advancedspatial_with_sf/advanced_spatial_with_sf_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e42d163b81a8a09aba02fb808868da61","permalink":"https://nkaza.github.io/slides/bootcamp_day1_1/intro2r_day1_1_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_1/intro2r_day1_1_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a5bce91553fd55e552a48616e86ca95a","permalink":"https://nkaza.github.io/slides/bootcamp_day1_1/intro2r_day1_1_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_1/intro2r_day1_1_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"824ff85732c0aa58bda688c9cc66f942","permalink":"https://nkaza.github.io/slides/bootcamp_day1_2/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_2/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0820d7fa41171991e20ef47e0a6f9d63","permalink":"https://nkaza.github.io/slides/bootcamp_day1_2/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_2/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d8a6ae5baed766230ab32b8ba62c99","permalink":"https://nkaza.github.io/slides/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"98e7390de82f78427df7925fc7c8bed7","permalink":"https://nkaza.github.io/slides/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day1_2/intro2r_day1_2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f59d6ee58230737f57275a5a9f0bc0c","permalink":"https://nkaza.github.io/slides/bootcamp_day2_1/visual_ggplot_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day2_1/visual_ggplot_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ad60ddad921f15c1425b82d413086d2d","permalink":"https://nkaza.github.io/slides/bootcamp_day2_1/visual_ggplot_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day2_1/visual_ggplot_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5fdb4ad60b682bb5ff90c2fc5c8a7243","permalink":"https://nkaza.github.io/slides/bootcamp_day2_2/programminginr_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day2_2/programminginr_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ba66f8f35e30963efdaa18522394081","permalink":"https://nkaza.github.io/slides/bootcamp_day2_2/programminginr_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/bootcamp_day2_2/programminginr_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f9eaeec462101523b3042fdbb80fef0d","permalink":"https://nkaza.github.io/slides/census_geo/census_geo_v2_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/census_geo/census_geo_v2_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8406a90878b0367aa5834bbd40e20a2e","permalink":"https://nkaza.github.io/slides/ggplot2/visual_ggplot_tmap_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/ggplot2/visual_ggplot_tmap_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2394020ffb20b2ea29966702318e5f45","permalink":"https://nkaza.github.io/slides/ggplot2/visual_ggplot_tmap_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/ggplot2/visual_ggplot_tmap_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7bb9159c63b4ee1850228056f5c53972","permalink":"https://nkaza.github.io/slides/intro2r/intro2r_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/intro2r/intro2r_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e43fea80a4e86a4d4aee0e640492088d","permalink":"https://nkaza.github.io/slides/intro2r/intro2r_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/intro2r/intro2r_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eff374c2cc0e292637e5aeb11878dcc4","permalink":"https://nkaza.github.io/slides/spatial_with_sf/spatial_with_sf_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial_with_sf/spatial_with_sf_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b6b78171844f5be6884e358c399fcd7","permalink":"https://nkaza.github.io/slides/spatial_with_sf/spatial_with_sf_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial_with_sf/spatial_with_sf_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6ca922eebc533291b7bd13bdf10d5857","permalink":"https://nkaza.github.io/slides/techniques_introduction/introduction_slides_v2_files-cas-cr-mj377z5s-conflicted-copy-2024-08-19/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/techniques_introduction/introduction_slides_v2_files-cas-cr-mj377z5s-conflicted-copy-2024-08-19/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"  reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .label, #upcoming-slide .label { position: absolute; top: 10px; left: 10px; z-index: 2; } #connection-status { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; padding: 30% 20% 20% 20%; font-size: 18px; color: #222; background: #fff; text-align: center; box-sizing: border-box; line-height: 1.4; } .overlay-element { height: 34px; line-height: 34px; padding: 0 10px; text-shadow: none; background: rgba( 220, 220, 220, 0.8 ); color: #222; font-size: 14px; } .overlay-element.interactive:hover { background: rgba( 220, 220, 220, 1 ); } #current-slide { position: absolute; width: 60%; height: 100%; top: 0; left: 0; padding-right: 0; } #upcoming-slide { position: absolute; width: 40%; height: 40%; right: 0; top: 0; } /* Speaker controls */ #speaker-controls { position: absolute; top: 40%; right: 0; width: 40%; height: 60%; overflow: auto; font-size: 18px; } .speaker-controls-time.hidden, .speaker-controls-notes.hidden { display: none; } .speaker-controls-time .label, .speaker-controls-pace .label, .speaker-controls-notes .label { text-transform: uppercase; font-weight: normal; font-size: 0.66em; color: #666; margin: 0; } .speaker-controls-time, .speaker-controls-pace { border-bottom: 1px solid rgba( 200, 200, 200, 0.5 ); margin-bottom: 10px; padding: 10px 16px; padding-bottom: 20px; cursor: pointer; } .speaker-controls-time .reset-button { opacity: 0; float: right; color: #666; text-decoration: none; } .speaker-controls-time:hover .reset-button { opacity: 1; } .speaker-controls-time .timer, .speaker-controls-time .clock { width: 50%; } .speaker-controls-time .timer, .speaker-controls-time .clock, .speaker-controls-time .pacing .hours-value, .speaker-controls-time .pacing .minutes-value, .speaker-controls-time .pacing .seconds-value { font-size: 1.9em; } .speaker-controls-time .timer { float: left; } .speaker-controls-time .clock { float: right; text-align: right; } .speaker-controls-time span.mute { opacity: 0.3; } .speaker-controls-time .pacing-title { margin-top: 5px; } .speaker-controls-time .pacing.ahead { color: blue; } .speaker-controls-time .pacing.on-track { color: green; } .speaker-controls-time .pacing.behind { color: red; } .speaker-controls-notes { padding: 10px 16px; } .speaker-controls-notes .value { margin-top: 5px; line-height: 1.4; font-size: 1.2em; } /* Layout selector¬†*/ #speaker-layout { position: absolute; top: 10px; right: 10px; color: #222; z-index: 10; } #speaker-layout select { position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: 0; box-shadow: 0; cursor: pointer; opacity: 0; font-size: 1em; background-color: transparent; -moz-appearance: none; -webkit-appearance: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } #speaker-layout select:focus { outline: none; box-shadow: none; } .clear { clear: both; } /* Speaker layout: Wide */ body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { width: 50%; height: 45%; padding: 6px; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #upcoming-slide { top: 0; left: 50%; } body[data-speaker-layout=\u0026#34;wide\u0026#34;] #speaker-controls { top: 45%; left: 0; width: 100%; height: 50%; font-size: 1.25em; } /* Speaker layout: Tall */ body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { width: 45%; height: 50%; padding: 6px; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #current-slide { top: 0; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #upcoming-slide { top: 50%; left: 0; } body[data-speaker-layout=\u0026#34;tall\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 45%; width: 55%; height: 100%; font-size: 1.25em; } /* Speaker layout: Notes only */ body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #current-slide, body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #upcoming-slide { display: none; } body[data-speaker-layout=\u0026#34;notes-only\u0026#34;] #speaker-controls { padding-top: 40px; top: 0; left: 0; width: 100%; height: 100%; font-size: 1.25em; } @media screen and (max-width: 1080px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 16px; } } @media screen and (max-width: 900px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 14px; } } @media screen and (max-width: 800px) { body[data-speaker-layout=\u0026#34;default\u0026#34;] #speaker-controls { font-size: 12px; } }    Loading speaker view...  Upcoming Time Click to Reset 0:00 AM  00:00:00   Pacing ‚Äì Time to finish current slide 00:00:00   Notes        (function() { var notes, notesValue, currentState, currentSlide, upcomingSlide, layoutLabel, layoutDropdown, pendingCalls = {}, lastRevealApiCallId = 0, connected = false, whitelistedWindows = ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f047edd0ab998d465af3ba7e25213180","permalink":"https://nkaza.github.io/slides/techniques_introduction/introduction_slides_v2_files/libs/revealjs/plugin/notes/speaker-view/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/techniques_introduction/introduction_slides_v2_files/libs/revealjs/plugin/notes/speaker-view/","section":"slides","summary":"reveal.js - Speaker View  body { font-family: Helvetica; font-size: 18px; } #current-slide, #upcoming-slide, #speaker-controls { padding: 6px; box-sizing: border-box; -moz-box-sizing: border-box; } #current-slide iframe, #upcoming-slide iframe { width: 100%; height: 100%; border: 1px solid #ddd; } #current-slide .","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.g. to comment on certain aspects, you can open a chalkboard or whiteboard on which you can make notes.  The main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation Copy the file plugin.js and the img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n\u0026lt;script src=\u0026#34;plugin/chalkboard/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;plugin/customcontrols/plugin.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Reveal.initialize({ // ... plugins: [ RevealChalkboard, RevealCustomControls ], // ... }); \u0026lt;/script\u0026gt;  The following stylesheet\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/chalkboard/style.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;plugin/customcontrols/style.css\u0026#34;\u0026gt;  has to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css\u0026#34;\u0026gt;  to the head section of you HTML-file.\nUsage Mouse or touch  Click on the pen symbols at the bottom left to toggle the notes canvas or chalkboard Click on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active) Click on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard) Click the left mouse button and drag to write on notes canvas or chalkboard Click the right mouse button and drag to wipe away previous drawings Touch and move to write on notes canvas or chalkboard Touch and hold for half a second, then move to wipe away previous drawings  Keyboard  Press the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings Press the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard Press the ‚Äòc‚Äô key to toggle the notes canvas Press the ‚Äòb‚Äô key to toggle the chalkboard Press the ‚Äôd\u0026#39; key to download drawings Press the ‚Äòx‚Äô key to cycle colors forward Press the ‚Äòy‚Äô key to cycle colors backward  Playback If the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing The plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export If the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration The plugin has several configuration options:\n boardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines. chalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines. chalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0. storage: Optional variable name for session storage of drawings. src: Optional filename for pre-recorded drawings. readOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function). transition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed. theme: Can be set to either \u0026#34;chalkboard\u0026#34; or \u0026#34;whiteboard\u0026#34;.  The following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n background: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae880302a8601a7a472c7be4e5793a4d","permalink":"https://nkaza.github.io/slides/techniques_introduction/introduction_slides_v2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/techniques_introduction/introduction_slides_v2_files/libs/revealjs/plugin/reveal-chalkboard/readme/","section":"slides","summary":"Chalkboard With this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n you can make notes directly on the slides, e.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Location   from: github.com/schloerke/leaflet-providers@urlProtocol\n  Inspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‚Äòold‚Äô relative url protocols and to ‚Äòupgrade‚Äô them at js runtime.\n  Notes‚Ä¶  Copy/paste provider information into providers.json  var providers = L.TileLayer.Provider.providers; JSON.stringify(providers, null, \u0026#34; \u0026#34;);    ./data-raw/providerNames.R was re-ran to update to the latest providers\n  Some providers had their protocols turned into ‚Äò//‚Äô.\n This allows browsers to pick the protocol To stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7f469e6c25f1658e18ca5d477cb29a78","permalink":"https://nkaza.github.io/slides/urbanform_energyconsumption/figs/widgets/leaflet_libs/leaflet-providers/rstudio_install/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/urbanform_energyconsumption/figs/widgets/leaflet_libs/leaflet-providers/rstudio_install/","section":"slides","summary":"Location   from: github.com/schloerke/leaflet-providers@urlProtocol\n  Inspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‚Äòold‚Äô relative url protocols and to ‚Äòupgrade‚Äô them at js runtime.\n  Notes‚Ä¶  Copy/paste provider information into providers.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Location   from: github.com/schloerke/leaflet-providers@urlProtocol\n  Inspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‚Äòold‚Äô relative url protocols and to ‚Äòupgrade‚Äô them at js runtime.\n  Notes‚Ä¶  Copy/paste provider information into providers.json  var providers = L.TileLayer.Provider.providers; JSON.stringify(providers, null, \u0026#34; \u0026#34;);    ./data-raw/providerNames.R was re-ran to update to the latest providers\n  Some providers had their protocols turned into ‚Äò//‚Äô.\n This allows browsers to pick the protocol To stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"66ff30e7c61aab7dad99e9023c121a6b","permalink":"https://nkaza.github.io/slides/urbanform_energyconsumption/libs/leaflet-providers/rstudio_install/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/urbanform_energyconsumption/libs/leaflet-providers/rstudio_install/","section":"slides","summary":"Location   from: github.com/schloerke/leaflet-providers@urlProtocol\n  Inspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‚Äòold‚Äô relative url protocols and to ‚Äòupgrade‚Äô them at js runtime.\n  Notes‚Ä¶  Copy/paste provider information into providers.","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"Data \u0026amp; Cities ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9bbe626cf158e1583bc3ef2161728146","permalink":"https://nkaza.github.io/slides/techniques_introduction/introduction_slides_v2.knit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/techniques_introduction/introduction_slides_v2.knit/","section":"slides","summary":"Data \u0026 Cities ","tags":null,"title":"Data \u0026 Cities","type":"slides"},{"authors":null,"categories":null,"content":"Data \u0026amp; Cities ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff9bcba9b577c9d2f9178499b2dbe27d","permalink":"https://nkaza.github.io/slides/techniques_introduction/introduction_slides_v2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/techniques_introduction/introduction_slides_v2/","section":"slides","summary":"Data \u0026 Cities ","tags":null,"title":"Data \u0026 Cities","type":"slides"},{"authors":null,"categories":null,"content":" Exploratory Data Analysis Use Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask 1 Instead of O\\(_3\\) as used in the tutorial, perform the analysis for NO\\(_2\\) for 2017 data. For this you will need to download the data from the EPA website and clean it. You need to tell a story about this pollutant and its distribution (in various senses). Explore some problematic features of the data collection and discuss their impacts on your analyses. You may consider some of the following questions in directing your analyses.\n How many sensors are measuring NO\\(_2\\)? Of what percentage of the year are the NO\\(_2\\) sensors active? Where are the sensors that are not active? Map them. How does the geographical distribution of NO\\(_2\\) AQI differ from that of O\\(_3\\) Which cities (CBSA, not counties) are worst affected by NO\\(_2\\)? Is there a correlation between NO\\(_2\\) and O\\(_3\\)? Do different correlations matter? (i.e.¬†correlations among AQIs of NO\\(_2\\) and O\\(_3\\) at site level, vs correlation of days AQI\u0026gt;100 at CBSA level, vs correlation of days AQI\u0026gt;100 at CBSA level etc..) What does the scatter plot look like? What does faceting the scatter plot by state tell us (pick 5 or so states)? Link this with other data (temperature, population etc..)? Where to acquires these datasets? How to link them?  These are by no means, exhaustive. Feel free to engage with your interests and the interesting bits about the datasets.\n Task 2 Download the motor vehicle traffic collisions data from NYC Open data portal. Answer the following questions\n Which locations have high incidences of traffic collisions? How are these high traffic collisions locations different at different times of the day? Visualise the correlation between home values in a block group and traffic collisions and tell a story.    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7c98bc2bbe542821813193f4832ec94","permalink":"https://nkaza.github.io/teaching/techniques_hw/hw1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/techniques_hw/hw1/","section":"teaching","summary":"Exploratory Data Analysis Use Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask 1 Instead of O\\(_3\\) as used in the tutorial, perform the analysis for NO\\(_2\\) for 2017 data.","tags":null,"title":"HW 1: Due Day 1 11:59 PM","type":"teaching"},{"authors":null,"categories":null,"content":"\rRaster Analysis\rUse Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask\rThe National Oceanic and Atmospheric Administration‚Äôs Coastal Change Analysis program produces land cover on the coast of the US. Download the 1996,2001,2006 and 2010 landcover data for the state of North Carolina from here. Answer the following questions and write a report summarising the findings.\n\rDownload the county vector files using tigris or the Census website. How much urban land exists in each county on the NC coast? How has it changed over time?\rHow have different types of wetlands changed in quantity over time?\rWhere have the wetlands become more fragmented and which indices are good for measuring fragmentation of the wetlands in NC coast? _ How is the fragmentation of wetland related to fragmentary urban development in these counties?\rRelate the population growth in each county to the wetland fragmentation. Annual population estimates at the county level can be downladed from the National Cancer Institute\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"883f246fd6269235ebad969b6a7af7dd","permalink":"https://nkaza.github.io/teaching/techniques_hw/hw2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/techniques_hw/hw2/","section":"teaching","summary":"Raster Analysis\rUse Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask\rThe National Oceanic and Atmospheric Administration‚Äôs Coastal Change Analysis program produces land cover on the coast of the US.","tags":null,"title":"HW 2: Due Day 2 11:59 PM","type":"teaching"},{"authors":null,"categories":null,"content":" Machine Learning Use Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask Improve the classification performance on the same dataset you used in the lab and write up a report on the significance of your feature engineering, model selection choices.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bbb20c0ce3588518168949c12a88913","permalink":"https://nkaza.github.io/teaching/techniques_hw/hw3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/techniques_hw/hw3/","section":"teaching","summary":"Machine Learning Use Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask Improve the classification performance on the same dataset you used in the lab and write up a report on the significance of your feature engineering, model selection choices.","tags":null,"title":"HW 3: Due Day 3 11:59 PM","type":"teaching"},{"authors":null,"categories":null,"content":"\rScraping web for data\rUse Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask\rMake the script from the lab more robust to errors and get a listing of all day care centers, hospitals, elder care centers, health clinics etc. in Wuhan. For this you may need to chunk the city into parts and search within each chunk.\nFor each transit stop in Wuhan (Bus and Metro) find the shortest distance/travel time to each of these points of interests. Are there certain points that are not particularly well-served by transit? Write up a report summarising your findings.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b5d337128823b7d106c7455d1a837ed3","permalink":"https://nkaza.github.io/teaching/techniques_hw/hw4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/techniques_hw/hw4/","section":"teaching","summary":"Scraping web for data\rUse Canvas to submit the HW. Both RMarkdown and the knitted html file is required for full credit.\nTask\rMake the script from the lab more robust to errors and get a listing of all day care centers, hospitals, elder care centers, health clinics etc.","tags":null,"title":"HW 4: Due Day 4 11:59 PM","type":"teaching"},{"authors":null,"categories":null,"content":"  class: right, bottom\nMachine Learning for Urban Analytics Nikhil Kaza  Department of City \u0026amp; Regional Planning University of North Carolina at Chapel Hill updated: 2022-01-22    # What is Machine Learning ?         The purpose of Machine Learning  Mostly for Prediction‚Ä¶  Classification (Categories of objects e.g.¬†spam/not spam; median strip /side walk/road, default/ prepayment / Current) Regression (Continous variables, e.g.¬†volume of water consumption/ energy use )  Not the same as statistical inference such as linear regression.    ### OK. What kinds of prediction?\n Local governments: Traffic congestion Google: What ads to show Amazon: What products to buy Insurance: Risk based on prior claims UNC: Sakai use to identify students in need of intervention.   Different Terms  Prediction Projection Forecast Scenarios  What do you think the differences are?   The central dogma of prediction  Components of a predictor \n question -\u0026gt; input data -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation   SPAM Example \n question -\u0026gt; input data -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation  \nStart with a general question\nCan I automatically detect emails that are SPAM that are not?\nMake it concrete\nCan I use quantitative characteristics of the emails to classify them as SPAM/HAM?\n SPAM Example \n question -\u0026gt; input data  -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation  http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html\n SPAM Example \n question -\u0026gt; input data -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation  \n Dear Jeff,\nCan you send me your address so I can send you the invitation?\nThanks,\nBen \n SPAM Example \n question -\u0026gt; input data -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation  \n\nDear Jeff,\nCan you send me your address so I can send you the invitation?\nThanks,\nBen \n\nFrequency of you \\(= 2/17 = 0.118\\)\n SPAM Example \n question -\u0026gt; input data -\u0026gt; features -\u0026gt; algorithm -\u0026gt; parameters -\u0026gt; evaluation  library(kernlab) data(spam) str(spam) ## \u0026#39;data.frame\u0026#39;: 4601 obs. of 58 variables: ## $ make : num 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ... ## $ address : num 0.64 0.28 0 0 0 0 0 0 0 0.12 ... ## $ all : num 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ... ## $ num3d : num 0 0 0 0 0 0 0 0 0 0 ... ## $ our : num 0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ... ## $ over : num 0 0.28 0.19 0 0 0 0 0 0 0.32 ... ## $ remove : num 0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ... ## $ internet : num 0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ... ## $ order : num 0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ... ## $ mail : num 0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ... ## $ receive : num 0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ... ## $ will : num 0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ... ## $ people : num 0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ... ## $ report : num 0 0.21 0 0 0 0 0 0 0 0 ... ## $ addresses : num 0 0.14 1.75 0 0 0 0 0 0 0.12 ... ## $ free : num 0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ... ## $ business : num 0 0.07 0.06 0 0 0 0 0 0 0 ... ## $ email : num 1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ... ## $ you : num 1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ... ## $ credit : num 0 0 0.32 0 0 0 0 0 3.53 0.06 ... ## $ your : num 0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ... ## $ font : num 0 0 0 0 0 0 0 0 0 0 ... ## $ num000 : num 0 0.43 1.16 0 0 0 0 0 0 0.19 ... ## $ money : num 0 0.43 0.06 0 0 0 0 0 0.15 0 ... ## $ hp : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hpl : num 0 0 0 0 0 0 0 0 0 0 ... ## $ george : num 0 0 0 0 0 0 0 0 0 0 ... ## $ num650 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ lab : num 0 0 0 0 0 0 0 0 0 0 ... ## $ labs : num 0 0 0 0 0 0 0 0 0 0 ... ## $ telnet : num 0 0 0 0 0 0 0 0 0 0 ... ## $ num857 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ data : num 0 0 0 0 0 0 0 0 0.15 0 ... ## $ num415 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ num85 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ technology : num 0 0 0 0 0 0 0 0 0 0 ... ## $ num1999 : num 0 0.07 0 0 0 0 0 0 0 0 ... ## $ parts : num 0 0 0 0 0 0 0 0 0 0 ... ## $ pm : num 0 0 0 0 0 0 0 0 0 0 ... ## $ direct : num 0 0 0.06 0 0 0 0 0 0 0 ... ## $ cs : num 0 0 0 0 0 0 0 0 0 0 ... ## $ meeting : num 0 0 0 0 0 0 0 0 0 0 ... ## $ original : num 0 0 0.12 0 0 0 0 0 0.3 0 ... ## $ project : num 0 0 0 0 0 0 0 0 0 0.06 ... ## $ re : num 0 0 0.06 0 0 0 0 0 0 0 ... ## $ edu : num 0 0 0.06 0 0 0 0 0 0 0 ... ## $ table : num 0 0 0 0 0 0 0 0 0 0 ... ## $ conference : num 0 0 0 0 0 0 0 0 0 0 ... ## $ charSemicolon : num 0 0 0.01 0 0 0 0 0 0 0.04 ... ## $ charRoundbracket : num 0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ... ## $ charSquarebracket: num 0 0 0 0 0 0 0 0 0 0 ... ## $ charExclamation : num 0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ... ## $ charDollar : num 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ... ## $ charHash : num 0 0.048 0.01 0 0 0 0 0 0.022 0 ... ## $ capitalAve : num 3.76 5.11 9.82 3.54 3.54 ... ## $ capitalLong : num 61 101 485 40 40 15 4 11 445 43 ... ## $ capitalTotal : num 278 1028 2259 191 191 ... ## $ type : Factor w/ 2 levels \u0026#34;nonspam\u0026#34;,\u0026#34;spam\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ...  SPAM Example ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"27797dfde94153d935b8c5d36d5c3ecb","permalink":"https://nkaza.github.io/slides/machinelearning/ml_slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/machinelearning/ml_slides/","section":"slides","summary":"class: right, bottom\nMachine Learning for Urban Analytics Nikhil Kaza  Department of City \u0026 Regional Planning University of North Carolina at Chapel Hill updated: 2022-01-22    # What is Machine Learning ?","tags":["techniques-short-course","new-urban-analytics","teaching","slides"],"title":"Machine Learning for Urban Analytics","type":"slides"}]